{"docstore/metadata": {"f0b98c510df971d3ce3cba2df5454a8ddd109fc5": {"doc_hash": "dbd6fd3d84edfa4389132fdb01bf36c19029ddd65d66d956b33aa79405543b4d"}, "bfcabb23c961bc8f4b16cb62d6345edfce1c6ba8": {"doc_hash": "5ef5bf4591b9a0e6706a9117761d609f3a518565c15d07107dbc99b14a0d9705"}, "edc18535956f4832534c0eb7f12118f393382241": {"doc_hash": "154c98679f76c18bfd99631f49eb8e7e0ac30f7028a82c41867f554dd506983a"}, "aebf7bb9c1405f04479ebe20a4c05eecada7e126": {"doc_hash": "7b45648c4ea5cfddf62695813d91aa5975740f87c4b4e097f9849b14f9f167aa"}, "6f7b64b74e52d83f0c64de07cde0e3d49b26acaa": {"doc_hash": "4dca7461ee31ed8fe0ae058c9f84172f2d14d68bce3ed05ac5467333ba27523e"}, "fcab491dca76f6cbed6abef8c60c3955d641fd52": {"doc_hash": "6640949cc4ce39fcf740021e39c851364273cd93dca40a6bf4475cb48809a401"}, "164ca94a0451118caab80ef9f9fe59d9b70e1ac7": {"doc_hash": "a3a18127b3c8b2dfe4351fbe845315df87f47e169d6540fbf9bde55b11cbd872"}, "b4f05092efb2fb768e7b064ec7f3c0bbe80f2688": {"doc_hash": "697f7401ad8990992f82bffa275503bd0523e34a89c4eadb5645572896c4ad34"}, "5f43dd876f21e8b1a4c175d44180bc9a99b46e3d": {"doc_hash": "8847079eb331e85ef73b38aefeee43dae1d4f4e09a7d20e4f059d4a72a73f105"}, "5f43db1a3196c695750e930895ca2c989594a5d9": {"doc_hash": "22b5f27b1b9d7b1576687eadfa92ecb87e83de57dc5be6b59eaa41685ee6e053"}, "036c5d8ecf903500763d02d18d3f5317bfed1923": {"doc_hash": "4ab7ce336e1659ddde8c196b60a3e586c5cbc582d106bfccc3978c30fad4ac69"}, "4adc9c0d92b6708aee44a652137dbf24403c7944": {"doc_hash": "9860c25624910502bd3060b99103d914c1662a23298f102c2ff46182f8e5dfa2"}, "6e16ac103b09cb71be3f33bbb68b2ef6bc7d3fcc": {"doc_hash": "1ae08974cb80653de0b3a7f66f3339e324ad61faa6e86152b0776a50a4007a2a"}, "1aeaeda99d3d4d605e1e5a50c24221a24ec14545": {"doc_hash": "9ce7c1d4a80b92c917e8d695a450d9fbf6b51aadd2e0a4a87a59cb803a067300"}, "d2ef3a5aebad2315dfabfe610a95a2148843b2f3": {"doc_hash": "52049d4dedb5af267c1d8058ab6409f1a1eee0799bbd6857baab529a2731e9df"}, "36a88465987820e159f5c5c85e7ef67bf6fcee5f": {"doc_hash": "39e3a1ae1f1177a2e03e0e37405ab4c8a1fa701a9edd130ee288a1d6a7952a90"}, "4c3aa6bae911907618067c75cd83a8585070dfb1": {"doc_hash": "dbc03fec25907ada9df8b46032560d2846e80d310d7ff635d8d853853f99c484"}, "95ce5773add72664c480be8dc558573dfd0daa1a": {"doc_hash": "1eed00862dcbb971f5cdd1c4cb57d687b5461020bb0d78c99908792cddd543ee"}, "06c954d55f081965a46e87f087da2d13a4d78a54": {"doc_hash": "a042409209e76a618397be1df009eda2162825171ed85218d1a7ffc98289a1f6"}, "5396077a51c81e98f78ee259c51b9d568b48e656": {"doc_hash": "6c418e0765c05453369024cf377969a4a19bea7d62f3323f5cf53ffc15fe1a47"}, "7d353751852345c3464fab7b1c2160fd7e745d75": {"doc_hash": "f981f32bcffd1bb02aa8dcc72f74402d7ad1cdcf2b83327f6e3c3f414105837e"}, "694d68cd05211c977fd232cf1e8166579eef2d90": {"doc_hash": "a6eba2869da6ab3285e7b018cf8d243c2acff34c9f63f9fdbf3a1457786fd8e9"}, "636d82cbe736f0e4b87074e2c4bac77becae2a28": {"doc_hash": "088b8b3573d8ad47eb5406f5c3ce360a661b9661e95b0a4c8ff24504e0be2726"}, "5f68cc26ac6f6f085c287888a51130f9e6d399a1": {"doc_hash": "5c2e90eec66c45c3ed69e5f3cd5e48ab3579818cde116662386ddf0efd8c4c86"}, "db7a594acaaabbb6b84352a6719c7045b36a31c3": {"doc_hash": "50cb2af6615db266b968212168f623fa82945be6d88a86b90d19c62bf36f4b94"}, "3778126c3f8d6c114a4804ff3e62747973115392": {"doc_hash": "fda433296dd0f686911ec7c1134cf06b71cc668b7a3655785b3647709f1b8920"}, "1f6253a69f7dc9d6db384a333c1e40582b2d4cb7": {"doc_hash": "fa518e8e4b84a2f678e4593144bb4f1247dabe9bbd2a7f6fc7808dac394d5730"}, "aac7341d37e177f35e582c98b61ee22a8c002f39": {"doc_hash": "596c2beb2596af8ae546a165e699511a4bd6da5bb4d6827632cd34c7880048eb"}, "636b0952f819caa55fb9bafa992266d6e9ba4045": {"doc_hash": "7d8a786d99536f543c14004a43e55b4d18938b2c16f1dc0cbdb63add4b67742e"}, "b53cbf19b410703f4d5108405c739c7cce41cf35": {"doc_hash": "e17e248f6b86a2071215f8daae1a979bf3b3f798d60d5f0b49145ed1730fd20b"}, "a33bb9aa3007e487161c4bbb30fe17f2a39bf993": {"doc_hash": "e048fe608e959d9fb9f2fe881f6619f59eab3b88ccf723b9d18d6544d0030cd1"}, "1ec77f04-7b50-4419-a1e5-63eef72a8579": {"doc_hash": "f9df7dcf7e6215563b36c1c88d9cd1f3e645908afb5f980c62d75325e21afe21", "ref_doc_id": "f0b98c510df971d3ce3cba2df5454a8ddd109fc5"}, "05ce8a6b-3f28-4366-9bbf-8361455a8b27": {"doc_hash": "0f3a7cb083ca79b3539d5c7b36269fe561a8be938b237f3223e7836a8cb13ad1", "ref_doc_id": "bfcabb23c961bc8f4b16cb62d6345edfce1c6ba8"}, "c7abbe2c-8cd8-42d1-9600-64ff0faf3063": {"doc_hash": "9d55f382c2fb7996cbcd89b6101f9d41aca2690581850c4e905da3f0ba5fbc53", "ref_doc_id": "bfcabb23c961bc8f4b16cb62d6345edfce1c6ba8"}, "1b1c0d36-3430-4f38-b3c1-3c02cd7c555a": {"doc_hash": "6f6caf7e6324648fefde633a3621a1e130bd824d911d9145ec0db08d2be68d94", "ref_doc_id": "edc18535956f4832534c0eb7f12118f393382241"}, "a0f026f5-95a0-4489-9a27-e8fb6aa0b87d": {"doc_hash": "622dcbdd30b841e0096ff3aebe21d3a774e6a4e65c0525926f7ab273c1e010fb", "ref_doc_id": "edc18535956f4832534c0eb7f12118f393382241"}, "79916c62-fc44-45c7-8fe0-3a1f992df87d": {"doc_hash": "3383a3f1faed43f070d30aeeb92a20996001282a71a3a1a8ced13f0a0a3563fb", "ref_doc_id": "edc18535956f4832534c0eb7f12118f393382241"}, "445a9234-9058-4838-91cf-f5dc12a42ee6": {"doc_hash": "b2fcfbc58d3caa9e8ab06f5730815e0a5b26f23853977e72056af4eeb0a7abf6", "ref_doc_id": "edc18535956f4832534c0eb7f12118f393382241"}, "ef3ef38f-542a-401b-aa20-fbc728ae3b62": {"doc_hash": "22a37c5d91883a1ac7e4c84603432c077c987e986c0c27ad52e184f7694759cc", "ref_doc_id": "edc18535956f4832534c0eb7f12118f393382241"}, "ec2dc4a0-8008-4498-8774-2afa4893a22e": {"doc_hash": "29b47e9ff48bcd7b3792a02ba465a2d385a21c21a84b0d681f96722ae93f6461", "ref_doc_id": "edc18535956f4832534c0eb7f12118f393382241"}, "9d86b46f-6ff1-41d4-a772-df4fe3b270c6": {"doc_hash": "8194b55eb12b0850b55c1872f9f9993e9c1a4ec019fd1fb728616e2c563ba253", "ref_doc_id": "edc18535956f4832534c0eb7f12118f393382241"}, "7a52b72f-a01d-4b63-943f-c796a7798925": {"doc_hash": "71a6c6e85186cbfaba68af70d7360b30d934453517f3598d557ec7401e9ea0fd", "ref_doc_id": "edc18535956f4832534c0eb7f12118f393382241"}, "e364f60e-08ed-418e-921c-ed51d94b3b24": {"doc_hash": "99e9e9b53c60ef38b893499451d1b3f0efe999ef13dd229f42c6cee4e35b39a9", "ref_doc_id": "aebf7bb9c1405f04479ebe20a4c05eecada7e126"}, "a764c2d7-5329-45ca-92ba-1ba9e99490c7": {"doc_hash": "3f3201040277b2171d1e541f2030b885023b6ca950eb1b732e3b807454f4ffaf", "ref_doc_id": "aebf7bb9c1405f04479ebe20a4c05eecada7e126"}, "81ecf67e-0411-4eba-8cab-8c6732b5d388": {"doc_hash": "8fbcbb8180f0252cc4d28b794080393080a5e78ccb5f085c2b71187d40deb1c6", "ref_doc_id": "aebf7bb9c1405f04479ebe20a4c05eecada7e126"}, "36792f6e-b573-4812-aeb9-341790196e85": {"doc_hash": "1d822463bbb06788d51d312f36ed62c6558b037e0ab121dd754a4f4b2ed7e6ff", "ref_doc_id": "aebf7bb9c1405f04479ebe20a4c05eecada7e126"}, "ab3cd4e3-2b68-4e33-9bd2-8075e6b99f51": {"doc_hash": "3bbef188dba29f8b8ca938fad5c3d57c8ae6ce1ab9f2253c0b5d27c66c1a0997", "ref_doc_id": "aebf7bb9c1405f04479ebe20a4c05eecada7e126"}, "68f5f974-7162-47a5-b198-ba401306bee4": {"doc_hash": "6de7831a68901e0a5580b6c605ff1b2c20c8d1beaab2fc6be84818093f3f383d", "ref_doc_id": "aebf7bb9c1405f04479ebe20a4c05eecada7e126"}, "865f459a-485e-43fb-ad14-e3b09686f266": {"doc_hash": "f7d405118804904311020fdce589f90996d65b62d602cc8f1299eae701c564ef", "ref_doc_id": "6f7b64b74e52d83f0c64de07cde0e3d49b26acaa"}, "32e4722a-009e-4f49-86d1-f4e68f38a76c": {"doc_hash": "4c6d742a1786be7a0f8f9030cb8be94be1de4f855541abdc7d96212ec1d83563", "ref_doc_id": "6f7b64b74e52d83f0c64de07cde0e3d49b26acaa"}, "7bdafbdc-652c-4fad-acc4-dbe3db44623f": {"doc_hash": "8931151ac951d6e13dbdfeb0de7d32cba7439be06c50fc2ddd3f190f6ad02fc2", "ref_doc_id": "fcab491dca76f6cbed6abef8c60c3955d641fd52"}, "cf99a2be-2c1c-489c-a1fa-852c9982b88a": {"doc_hash": "d0264a0a8123a5556d6fdb69ba182a8f43b6408840063c0e749e3609554d6755", "ref_doc_id": "fcab491dca76f6cbed6abef8c60c3955d641fd52"}, "944c48e5-f036-4200-a31e-71d697017d5d": {"doc_hash": "e88a0ba318a6815380e58a07eba241171ae58e09270237b3d961c6cb915edc91", "ref_doc_id": "fcab491dca76f6cbed6abef8c60c3955d641fd52"}, "1ede687e-b6b9-4190-9182-2111f6a1c8e6": {"doc_hash": "ac2e37afea77db640bf6ae9e72623dd319b82e65f8e062208bc2915e2cab12fd", "ref_doc_id": "fcab491dca76f6cbed6abef8c60c3955d641fd52"}, "a2f1a300-5ed5-49e7-ba30-3e09b09849f0": {"doc_hash": "10c12122c6a6740ca77bc9dfb17d6a3dfbfdeb4f220977de65304d7ba88f1b72", "ref_doc_id": "fcab491dca76f6cbed6abef8c60c3955d641fd52"}, "487e6953-b868-421d-a102-4c50e24c5e39": {"doc_hash": "47da5a22edb2422350c9debba1a4e5600b706e8d463bc8f64d2dd80a66d6ec03", "ref_doc_id": "fcab491dca76f6cbed6abef8c60c3955d641fd52"}, "125170b8-9579-4228-aaef-bf6a18ad2b84": {"doc_hash": "50ea7d5e69a44d7cac1114c255db670289075dc6d96cdb83705839d8bc8a6c0a", "ref_doc_id": "164ca94a0451118caab80ef9f9fe59d9b70e1ac7"}, "2298a7b1-911b-490c-a2ed-1877652579ee": {"doc_hash": "c43d841afc99370c869c5637fc39dd49f8e733357b8b6ea4ecc331d7ed0909f8", "ref_doc_id": "164ca94a0451118caab80ef9f9fe59d9b70e1ac7"}, "a4f6a240-67cc-4018-9ba1-33ac586fd589": {"doc_hash": "eab843354cd697c2ea73123f8745ec95dce17a2bd09c1162cad0ba285a7d665b", "ref_doc_id": "164ca94a0451118caab80ef9f9fe59d9b70e1ac7"}, "c25a9759-eb01-484a-87af-7787accef6c9": {"doc_hash": "1fdaf9836310bee99f3ac0720955ff5dc76ee0277cbeefe7b17b611bab0d198d", "ref_doc_id": "164ca94a0451118caab80ef9f9fe59d9b70e1ac7"}, "d3327c84-8696-4709-a1cb-f9cf4f353e60": {"doc_hash": "08e5978e60f7c607115051c0cc69e7d8ee3021a70c2b86d32112a5a2261d26a1", "ref_doc_id": "164ca94a0451118caab80ef9f9fe59d9b70e1ac7"}, "bb05c387-9ce5-459f-b950-31dfa2df0a26": {"doc_hash": "0b24ae3f7b37f14403116a40aaec4d6b0161da29a5fc5d8156beea0b92af1881", "ref_doc_id": "164ca94a0451118caab80ef9f9fe59d9b70e1ac7"}, "21f20236-84b5-40a0-a78f-956cefb72524": {"doc_hash": "1564d1c8d9f88f6949c12a81cafdce2c84cca8ac362aedbe2471768d3175a935", "ref_doc_id": "164ca94a0451118caab80ef9f9fe59d9b70e1ac7"}, "c1000cdd-019c-41b7-8646-4c2dcb2f6b62": {"doc_hash": "991d2ff9c578d1c5136fc6831f048af73a52b197cc1f1fd30a2af992b03ede32", "ref_doc_id": "164ca94a0451118caab80ef9f9fe59d9b70e1ac7"}, "17cb78f4-7d8d-4113-8e88-cde6b42ad9b9": {"doc_hash": "548308320216139aec82299a1f3bf43666291f90953aeaa9c68fd2bc167c86c2", "ref_doc_id": "164ca94a0451118caab80ef9f9fe59d9b70e1ac7"}, "075a46ac-b470-4883-8720-389a85aa84ec": {"doc_hash": "96ddebc906b5a19724b2480318f8b0b3df4e66824f8b01df12cc07462515fd76", "ref_doc_id": "164ca94a0451118caab80ef9f9fe59d9b70e1ac7"}, "0a92b93f-276b-4593-acb2-06b141803419": {"doc_hash": "f3ee0f9088654b3e6c81ce32b7dae1770d0eb60574001f6071c593073faeb343", "ref_doc_id": "164ca94a0451118caab80ef9f9fe59d9b70e1ac7"}, "0013b21a-8efc-4460-8593-1d48294c47df": {"doc_hash": "30460460d2ccbe8f7c82e2bad5299980a7f7dfe71cc58fa67ecd27cd54a695f4", "ref_doc_id": "164ca94a0451118caab80ef9f9fe59d9b70e1ac7"}, "82bce502-5b2d-40bf-a61b-69dec53a6a8b": {"doc_hash": "a6698392ec6b339cf6fdfb6f58cb57c0bc85b03f9c6faf3c0c32b518036f9b92", "ref_doc_id": "b4f05092efb2fb768e7b064ec7f3c0bbe80f2688"}, "b9abe666-6658-45ca-bf79-1dcc42eeef00": {"doc_hash": "55cbcd322374174872f21dc086b8c3e794dc66033bb647d6dc077c2ba1ff05ad", "ref_doc_id": "b4f05092efb2fb768e7b064ec7f3c0bbe80f2688"}, "5902745f-43f5-4e02-8ffe-4c32bb240d27": {"doc_hash": "750282f02fdacc4ce893528154a28638734120287a4ba0803fa90cd3646edee4", "ref_doc_id": "5f43dd876f21e8b1a4c175d44180bc9a99b46e3d"}, "13e2b4fd-a55e-40f4-aea0-0a6a21ce5adc": {"doc_hash": "2e7a41837043cf48289d21ed4db61156e8c505e7e5302810b90529fac3bb1d99", "ref_doc_id": "5f43dd876f21e8b1a4c175d44180bc9a99b46e3d"}, "4416d839-f94b-45e7-9dbf-c1516926babf": {"doc_hash": "2e50afeb81e8744586cd628c0231e590782fb08b661b566c5a2b1e568e33a5ca", "ref_doc_id": "5f43dd876f21e8b1a4c175d44180bc9a99b46e3d"}, "4c5bfd13-0e96-4acb-b7b9-27afecb12995": {"doc_hash": "a9546d5795dbad8ed1e17b97ba4d5e818aed273b1c6af1b8f45adb719c404bed", "ref_doc_id": "5f43dd876f21e8b1a4c175d44180bc9a99b46e3d"}, "eb6668ea-4b84-4d25-a38b-351a718b8eed": {"doc_hash": "5e8c86c2e0043e4882956fb460023442a76e3e77b79ef25f7da1197e858126f5", "ref_doc_id": "5f43dd876f21e8b1a4c175d44180bc9a99b46e3d"}, "1966337f-81ce-468d-80b3-1714fea6e49d": {"doc_hash": "f981516c86abbbc9b806793fadabdc748dfc7edb0ead81cbb1d83c6a335fda3b", "ref_doc_id": "5f43dd876f21e8b1a4c175d44180bc9a99b46e3d"}, "82b36099-7ef0-4cb3-afe6-6d391f4b5c27": {"doc_hash": "ddc199a88ab0f71705b68504689933f2cafa21d1577df1c0aae91b577590e6b3", "ref_doc_id": "5f43dd876f21e8b1a4c175d44180bc9a99b46e3d"}, "ebafea1a-b940-4cbb-b1d4-d2350ad77905": {"doc_hash": "30b71723e9ca6911898fbf8f505739716b6e2e340cc123efda1666f8516b05c4", "ref_doc_id": "5f43dd876f21e8b1a4c175d44180bc9a99b46e3d"}, "599dbdfb-3049-4a7c-afa1-88d0372625ab": {"doc_hash": "9660a6ca008fb66d7f804d1dc0cb08bb9a21b62873883580b99ca0b7b75abe0b", "ref_doc_id": "5f43dd876f21e8b1a4c175d44180bc9a99b46e3d"}, "40da5828-2dba-4cbb-acda-83c02bdd8b63": {"doc_hash": "5c32a37e1a1742e2ef32b4097cd2af28edbed6667d60867a22e7fdc748c13484", "ref_doc_id": "5f43dd876f21e8b1a4c175d44180bc9a99b46e3d"}, "32b1f477-b3f8-4542-9321-b617259b3a13": {"doc_hash": "4532ffe558e0fef0f6dc844885ba29a93a85a293c9864606faf7f1b0fc9597b2", "ref_doc_id": "5f43dd876f21e8b1a4c175d44180bc9a99b46e3d"}, "ec8cbbc1-3390-4794-a203-10cce7f1113f": {"doc_hash": "f5c9c2749bdf7f4334aec1845f988d3c12a6d20a2959098984fe9968cd6fc10a", "ref_doc_id": "5f43dd876f21e8b1a4c175d44180bc9a99b46e3d"}, "0a39cbf1-a298-4f1e-9d18-bb82cf940edc": {"doc_hash": "df323c988df1e518bfd973ca3949bc016fe10c7cb856485ee9883f5587c6071c", "ref_doc_id": "5f43dd876f21e8b1a4c175d44180bc9a99b46e3d"}, "fa74fa51-e7eb-41d5-afab-6c96c0e0033c": {"doc_hash": "444cc817a54d660db71ec0b628c164a5940a99494cee0c2ac588142890f2059d", "ref_doc_id": "5f43dd876f21e8b1a4c175d44180bc9a99b46e3d"}, "885ba2dc-e05b-4055-b45a-841a7adf9bc9": {"doc_hash": "ff220ace9f44fed9033348bc56066dbc47b6a5b6a3df171dcd95f7ef3eb6fd10", "ref_doc_id": "5f43dd876f21e8b1a4c175d44180bc9a99b46e3d"}, "6066287c-e78e-46b4-9747-13d1ac9291ca": {"doc_hash": "e9c4bc6c0186d91d3453ebc2ec106d71a8dbd268045b62f39ba834662ba05009", "ref_doc_id": "5f43dd876f21e8b1a4c175d44180bc9a99b46e3d"}, "fd81bc34-c92f-414c-9813-a0ce486af0bd": {"doc_hash": "562631c832b885e3e58b31db95b52199971b33cda6996245d275c8ec4f13189e", "ref_doc_id": "5f43dd876f21e8b1a4c175d44180bc9a99b46e3d"}, "89329e42-8078-42e9-85d4-002d0aa6cbbd": {"doc_hash": "39d479766f12d635b2476e1cdddefc0482a7b8a8f52b3b16b859dc86017a2edf", "ref_doc_id": "5f43dd876f21e8b1a4c175d44180bc9a99b46e3d"}, "c1a3b427-4784-48b4-ae4d-655dd4e3ea08": {"doc_hash": "c6d140c9b3d2d5646c4c1c1518012432e88939bde15835989f9830b28b4db0b8", "ref_doc_id": "5f43dd876f21e8b1a4c175d44180bc9a99b46e3d"}, "018f7e7c-995c-4587-bc2b-7c4ad1d6a62b": {"doc_hash": "9112ef855430c436538cb94332e2e2af2e7c1f169d6eb47aaf0df510ac7dc6b5", "ref_doc_id": "5f43dd876f21e8b1a4c175d44180bc9a99b46e3d"}, "b416f012-a8c7-44f1-b5ed-1cf906e8a631": {"doc_hash": "5344b34126295092e8713b6a7a6e610d6b6d71441e88ae68ce2eb05097de54d8", "ref_doc_id": "5f43dd876f21e8b1a4c175d44180bc9a99b46e3d"}, "7832a54c-1260-4b79-a2fc-2633e6c02f48": {"doc_hash": "c7ca9ce5052f7a8fb66747bec7c9685ed48036543bfe0c0bd579720b08df7cc0", "ref_doc_id": "5f43db1a3196c695750e930895ca2c989594a5d9"}, "cb545bda-e589-4c66-9fc5-ad49f436e370": {"doc_hash": "386df0313fdb837ba705fa929c9bbc0d65c78418bdca6bd7ece4e5c4716f11ed", "ref_doc_id": "5f43db1a3196c695750e930895ca2c989594a5d9"}, "c04ef79d-dd51-402e-9087-9c4534b341ed": {"doc_hash": "900751bc5ca56f2d6a3962c4f809490045dede93546c432daebafa36aad53d24", "ref_doc_id": "5f43db1a3196c695750e930895ca2c989594a5d9"}, "51cebf8b-6391-4eb6-950f-9178e8a441ec": {"doc_hash": "7e0a9fa3308da655dd288c6d4f5e9ff6c6904dc35b155f48ff706a7bd85ee115", "ref_doc_id": "5f43db1a3196c695750e930895ca2c989594a5d9"}, "6cfda5e0-7219-423e-9372-b61d44ed1b87": {"doc_hash": "faa87b31b494ec036fe6d0136a456a6e1ab7ac0ea02a44204b4086c65a1e25e8", "ref_doc_id": "5f43db1a3196c695750e930895ca2c989594a5d9"}, "354d5665-b2a8-4bc4-b831-ea2e21a34727": {"doc_hash": "ee9562f1f6721a355552add1cf43955a328e712a2d6d2e940c74cc88578e2c0b", "ref_doc_id": "5f43db1a3196c695750e930895ca2c989594a5d9"}, "94e0c2a4-9d84-4aaf-b1e9-d35b8bf5285b": {"doc_hash": "4a806d064f7d44d924697165ad7682d9a0dca0f86884ef1eef0348b1eb05ca8b", "ref_doc_id": "5f43db1a3196c695750e930895ca2c989594a5d9"}, "f81d8bc5-ff42-4de1-b8eb-508b8112b90c": {"doc_hash": "3bb59f8b7b401e5a44a4625d47f573b744ac554a6d805a2d9d9e259f54648540", "ref_doc_id": "5f43db1a3196c695750e930895ca2c989594a5d9"}, "6bece696-695d-4d03-8972-baf9d18a4319": {"doc_hash": "6bee223137644827c3d0b78074f79543f3de4578df5e5432d31325ef1644f139", "ref_doc_id": "5f43db1a3196c695750e930895ca2c989594a5d9"}, "e84dfd76-1081-4a09-bd5b-24de0f2ecb54": {"doc_hash": "447eaaa63f6d3304defa815e53cec32ef8b7f6279206f5205020b343801bda31", "ref_doc_id": "036c5d8ecf903500763d02d18d3f5317bfed1923"}, "a8016ff5-8f66-42f7-b4da-fefc410bbf11": {"doc_hash": "368a085cf737a5be65f244f77a106ac39b475e6584d6f300bd9a8213e3359680", "ref_doc_id": "036c5d8ecf903500763d02d18d3f5317bfed1923"}, "0a5284a0-9f00-401d-bfd8-f3a1e54ba28c": {"doc_hash": "1689e3468d66bbf3288e0d716c9117b316b2b87a3ed2c5f09b11f7e45755d968", "ref_doc_id": "036c5d8ecf903500763d02d18d3f5317bfed1923"}, "94b62fd8-02be-4783-92dd-78ea1b7d99d4": {"doc_hash": "319c89cd9c236b813b390bff12c323ece7702db96496d56b09923a35e4a8300e", "ref_doc_id": "036c5d8ecf903500763d02d18d3f5317bfed1923"}, "6920ea8f-5c14-44f2-9ea4-addbc435c4ba": {"doc_hash": "a01dbb8b5d8f62767dacf7948e56cec5c0afc4e280670460969cd6a14cd1caf7", "ref_doc_id": "036c5d8ecf903500763d02d18d3f5317bfed1923"}, "dd9c496f-c281-4cec-8c23-233a046ad400": {"doc_hash": "52327e9df20254c6e9244735c93bc66b327ef5644c3242ee6e8728a6e4f748fb", "ref_doc_id": "036c5d8ecf903500763d02d18d3f5317bfed1923"}, "16e76ad7-03e5-433f-a8f3-a891665536a3": {"doc_hash": "c460a640b5aa03b313c723b49eca12a0ddffb9b8321df0462b9f190533fe125d", "ref_doc_id": "036c5d8ecf903500763d02d18d3f5317bfed1923"}, "6629031a-a105-4430-8302-a16a24a7e7fc": {"doc_hash": "954257115ff52ae2c5c978c4e61194909a203b6c0670376ffdf07726ace38b52", "ref_doc_id": "4adc9c0d92b6708aee44a652137dbf24403c7944"}, "6d3586ab-8145-4b14-9ed8-6e3cd8b5596f": {"doc_hash": "6d4961310c3084ba5a4187b5ec37f1c1cf139cf8e537e5f89ad4808c3c2c8ae8", "ref_doc_id": "4adc9c0d92b6708aee44a652137dbf24403c7944"}, "205e4873-6821-4621-a7c1-5741886f1e91": {"doc_hash": "c11c6531ec9ff19d6b3277bd981e1a6a246e34f0fceb34bda8bc72e00c17ccbc", "ref_doc_id": "4adc9c0d92b6708aee44a652137dbf24403c7944"}, "8788ca26-5277-4dd0-bacc-8a14aad5cfc5": {"doc_hash": "25af124efd3ff711eda69f98326ebc45d54b2160b6b097dce3535e37c86b40d6", "ref_doc_id": "4adc9c0d92b6708aee44a652137dbf24403c7944"}, "6b4a08a1-e6e7-4bab-8e55-e27203b74bbd": {"doc_hash": "9bb96e0f560d8079a9087b48c7a7e035976a2b0b7045c60660536d29604693a0", "ref_doc_id": "4adc9c0d92b6708aee44a652137dbf24403c7944"}, "7e0361c3-0921-4dbb-bab8-bebc722c3a68": {"doc_hash": "0246acd16b107281c672314b9f28ad921876cc8d453b86e4333a94e91ed57b9b", "ref_doc_id": "4adc9c0d92b6708aee44a652137dbf24403c7944"}, "e0bba44c-21fb-4c6b-89a2-17bed896ac35": {"doc_hash": "c582ef307f5d32698709ee7cbd1298dec8edfed3703ab5ec462dc335e1bd3f86", "ref_doc_id": "4adc9c0d92b6708aee44a652137dbf24403c7944"}, "5acbedd4-f8d3-4b50-b12b-1929fefd31dd": {"doc_hash": "1b792a18fc77ef94e0742d825a7a1fba0afe75a484d98c8040a013202e329fea", "ref_doc_id": "6e16ac103b09cb71be3f33bbb68b2ef6bc7d3fcc"}, "f474ca00-0583-43aa-8266-ce97fc931ce4": {"doc_hash": "552721246bdc649a20dd06d4c5c704bb4209f64e539ecaaadb9a91cabca33e6f", "ref_doc_id": "6e16ac103b09cb71be3f33bbb68b2ef6bc7d3fcc"}, "f26eb4d3-0dcf-4a8f-8cac-9e45f11fb3fc": {"doc_hash": "62c95678b35f23b34e6594fcebd121a3200c536b873a7f344175ab6a6fc5754b", "ref_doc_id": "1aeaeda99d3d4d605e1e5a50c24221a24ec14545"}, "d003939d-e855-4f7c-aea9-3657ed918f27": {"doc_hash": "1bb5d5b20777625e54d13110072034cb30bf59fc7163576a6e472bae3045fc84", "ref_doc_id": "1aeaeda99d3d4d605e1e5a50c24221a24ec14545"}, "00f70469-9b34-4893-9a87-8f010cefe2a3": {"doc_hash": "8e0da0edbbe1833fbc4112c88e3c8d28ab4c91d589339dcbe18cdcca2da025d3", "ref_doc_id": "1aeaeda99d3d4d605e1e5a50c24221a24ec14545"}, "c1bf8ab0-3b4a-4c6e-a4ae-386430078a30": {"doc_hash": "69132994e28c45b15071363b5463e6e2c7cc044418a06351585c0bcc777c1bea", "ref_doc_id": "d2ef3a5aebad2315dfabfe610a95a2148843b2f3"}, "50ec52c6-eb13-47d9-9789-198df925f274": {"doc_hash": "47d1fdf85f0b0530f94f9adc756c660a85379649ed8c7c6ebe768b9b23aa7ca9", "ref_doc_id": "d2ef3a5aebad2315dfabfe610a95a2148843b2f3"}, "392de5e3-6ee2-443c-a63d-0fd4b3e16985": {"doc_hash": "c0147b8560f22c4586db1107ddfa48705a5a773230bf604372a2722c47372902", "ref_doc_id": "d2ef3a5aebad2315dfabfe610a95a2148843b2f3"}, "5a0f079e-c8f8-453c-b868-f7678a5c89ea": {"doc_hash": "81329f64cc822544dc88cdda502849faa5e670f201942800ebeaaa56fa941742", "ref_doc_id": "36a88465987820e159f5c5c85e7ef67bf6fcee5f"}, "90f94734-6c75-4bc2-b66f-8bedcaf69a26": {"doc_hash": "a1e8361ed11ae6114726c04d9fbfd5c11b98464e20c0964c9c0e92a5b7cd6a94", "ref_doc_id": "36a88465987820e159f5c5c85e7ef67bf6fcee5f"}, "8fdc6edc-7354-446d-b226-25a7cfe06b39": {"doc_hash": "1ba1b35a7032883bf80592b4c09cd59cbebdc902125a924f23221677812e5b67", "ref_doc_id": "36a88465987820e159f5c5c85e7ef67bf6fcee5f"}, "6aeffd42-ab0c-41c4-acbb-694beac9a0a4": {"doc_hash": "a498198b739d265d487b10ba62f86719b21e49fc74eac729570d985f4e64868e", "ref_doc_id": "36a88465987820e159f5c5c85e7ef67bf6fcee5f"}, "b0a9a076-eaed-4258-86a0-51da739d8798": {"doc_hash": "e1858e31b06ddcf9d1c72f36ea66eec110c355cf89776d494efde6aef9cf98a8", "ref_doc_id": "36a88465987820e159f5c5c85e7ef67bf6fcee5f"}, "5be61f5e-83c2-4b85-a269-3abf9837eb1e": {"doc_hash": "bb821b0690e11be1e00f11ba6685e648b445f5a8e5dd0b3cae7433ff5fedbd0b", "ref_doc_id": "36a88465987820e159f5c5c85e7ef67bf6fcee5f"}, "3ac5d80d-d57e-495b-92b2-963f3a83d847": {"doc_hash": "6b4ed7990495163df7fb1a7bde3c0fb61e702a7bfb796a06123f3696554f602d", "ref_doc_id": "4c3aa6bae911907618067c75cd83a8585070dfb1"}, "021840f5-53ac-49dc-8804-813553c72162": {"doc_hash": "0460e41ccfc337186d10937b4c9dcf7d9393b6f2a8a9d4b74a299399232a4abc", "ref_doc_id": "4c3aa6bae911907618067c75cd83a8585070dfb1"}, "0fd1bdd5-9695-4177-ae1a-0c5adef838f8": {"doc_hash": "45eb6a20585ff98f95abfe68631e8481fce304b9a5a8cfd9a1fef05a288f5b3c", "ref_doc_id": "4c3aa6bae911907618067c75cd83a8585070dfb1"}, "3ddc18b6-7593-4e54-b150-f9249eb2f9e0": {"doc_hash": "2b845e08d60f7e8b58e073fca5325a0c9ef3c4239dbab8eefc4127677ba7192e", "ref_doc_id": "4c3aa6bae911907618067c75cd83a8585070dfb1"}, "5d4dc916-6a40-44c4-94d4-799b499573b0": {"doc_hash": "1926f9a59a863f59169a3306c15d62857c56998b9b01c071764e8e41f111f90f", "ref_doc_id": "4c3aa6bae911907618067c75cd83a8585070dfb1"}, "21d436f7-2402-45d8-b5af-8b5822c28dac": {"doc_hash": "98b2df5eaf0f3988fb5fe06d09f982fc249577951d4954d15619e4de78564a0e", "ref_doc_id": "4c3aa6bae911907618067c75cd83a8585070dfb1"}, "f0809c93-14fc-44f7-b228-d78dc4a9028c": {"doc_hash": "b29b80358bc5e53ba09476331a14c3bf7d8cfd7c52d5957ae8d25ba55298b817", "ref_doc_id": "4c3aa6bae911907618067c75cd83a8585070dfb1"}, "bf38b9e5-0243-436e-9683-94696aca428e": {"doc_hash": "7d65b16b9b2e85fd1e39ff8ee65dfadade23e4841fdd240c15e9c0cfc5ec1c05", "ref_doc_id": "95ce5773add72664c480be8dc558573dfd0daa1a"}, "5679e811-ced1-405d-b111-8832c2bd191b": {"doc_hash": "a328cc2ed6557048ac0fa8810e55d0583f3e39a43dd84d08b1388c9ce0dd9873", "ref_doc_id": "06c954d55f081965a46e87f087da2d13a4d78a54"}, "e325d2b6-1b9c-4e4f-899a-931e94194617": {"doc_hash": "a304f169cd0dfa21dd27863b93941ed075c29630eb66d1d70055167568d39dd5", "ref_doc_id": "06c954d55f081965a46e87f087da2d13a4d78a54"}, "f9108746-9470-4159-abf0-6f1dc870148f": {"doc_hash": "c39ab15734e4a9d1ee47db4780db2cf61fc8b4925567388658438e79e79968cc", "ref_doc_id": "06c954d55f081965a46e87f087da2d13a4d78a54"}, "650167e0-b75b-423e-9c86-95d1e6628dc8": {"doc_hash": "06d6d5c3eca6e0d38f5889d8d2a0cb9e61567bb1f74214cfa508b9b2bed7299e", "ref_doc_id": "5396077a51c81e98f78ee259c51b9d568b48e656"}, "55490239-e8fc-406c-bff6-89cbb86ce742": {"doc_hash": "3952239b80473a33667a05307b45640eb19bbec315060f0028595ea05b0c7434", "ref_doc_id": "5396077a51c81e98f78ee259c51b9d568b48e656"}, "146d6f8a-443f-473b-a5a2-637b3ce26e86": {"doc_hash": "961f2a95848bc8fd730ac95238ae1b250c386d68c1c74e0df5b26e8cd5f2ce14", "ref_doc_id": "5396077a51c81e98f78ee259c51b9d568b48e656"}, "7b7d0930-3f63-44f4-acdc-cb59b02848d8": {"doc_hash": "1b727d51ad4d1236b144477361af4f2662817e8f69dfff9d89605c442cd35e9e", "ref_doc_id": "7d353751852345c3464fab7b1c2160fd7e745d75"}, "ba656205-9446-431c-84c9-e5e336774d5a": {"doc_hash": "bde7b828c04164df6c6f213339159208e70545dbee9223b3ade197cb867ea3ce", "ref_doc_id": "7d353751852345c3464fab7b1c2160fd7e745d75"}, "76ac2da5-c3f4-448b-a853-1215816a7da8": {"doc_hash": "68b560968d3e8ecec1f289307b33d4744a6eb992347ed49cc5275a6804ae2c49", "ref_doc_id": "694d68cd05211c977fd232cf1e8166579eef2d90"}, "36a2215c-9a6d-45ac-bfa1-dd0349e75cf5": {"doc_hash": "798a425bc7769eeae2deebb62d70c7ec4cc120112d96e7295853bddeb60c4eb3", "ref_doc_id": "694d68cd05211c977fd232cf1e8166579eef2d90"}, "78eeaf75-8b78-460e-b074-7f868bbbe16f": {"doc_hash": "9050bd9ae728df2f876713c5466bf9c69b29ba354318115645b0e12e1ae5e7da", "ref_doc_id": "694d68cd05211c977fd232cf1e8166579eef2d90"}, "24085bab-197a-40a6-92bd-31637ca55de8": {"doc_hash": "8b08734473a4537d77f67307441d5a6df7f05b124145bb5c3a588897206171cd", "ref_doc_id": "694d68cd05211c977fd232cf1e8166579eef2d90"}, "e279fcd0-f738-464e-bfc1-64bf088b3bc1": {"doc_hash": "7b94cd8fa3f6e358c74bf8abe97daccb49fe3268ecff86a3f28e4b67478d71b4", "ref_doc_id": "636d82cbe736f0e4b87074e2c4bac77becae2a28"}, "51b774b9-2723-42b6-88e8-a121bc275146": {"doc_hash": "25c727b44473dfc4300518a1d5a5e945ebc3069037c6c15dcb11e11d8f036d07", "ref_doc_id": "636d82cbe736f0e4b87074e2c4bac77becae2a28"}, "1fd51f4e-62cd-4a67-a4cb-96f822030abe": {"doc_hash": "cdbf31755dc5524adccd58009d11248c3afcfd242426d1e34e3296eced102001", "ref_doc_id": "636d82cbe736f0e4b87074e2c4bac77becae2a28"}, "aa7b7f87-d852-426b-bd8e-7fbbe178a655": {"doc_hash": "c3d97554a4bebef2f3cb594215ce6f3adfa83a68b78d3d071f4bf03db7771f1a", "ref_doc_id": "636d82cbe736f0e4b87074e2c4bac77becae2a28"}, "e260f3c4-051e-4076-b03c-37a78b8189b2": {"doc_hash": "d2f7f1e39d099453336a09bd24b695e938640f9b54d07196edae92dfcf5378e6", "ref_doc_id": "636d82cbe736f0e4b87074e2c4bac77becae2a28"}, "91eaade7-7478-4723-8fb6-3e34868a4859": {"doc_hash": "b68b01bf951143c5512dee3ab377ec0cce7b5b22655b0af27fa54baa1d31a605", "ref_doc_id": "636d82cbe736f0e4b87074e2c4bac77becae2a28"}, "c71dbe1d-3141-4b8c-a3a9-ebdf501354c5": {"doc_hash": "10ca2f84f79b3d50d22a875969a3a3565de25805d1abbcfc0edf2d6af8f632fb", "ref_doc_id": "636d82cbe736f0e4b87074e2c4bac77becae2a28"}, "d88f337e-736e-4b55-bc11-3bdbd99a2641": {"doc_hash": "eb691d340e9f1b44cf402985e123289c044ffde622446bcd7b69bb79d661f0b0", "ref_doc_id": "636d82cbe736f0e4b87074e2c4bac77becae2a28"}, "5c02ba1a-6514-48fe-bffb-8d9085d8d849": {"doc_hash": "0b79ff2ad11944f9791e7e9923ba3bbca7f439e66fa17fd7ca878ca2790b1f97", "ref_doc_id": "636d82cbe736f0e4b87074e2c4bac77becae2a28"}, "510939b7-eda6-4953-bfe8-678dffdafac7": {"doc_hash": "5da208af0a1cf2a294f0adf024f1889202b08625dafecd8a97dc9c902853dbbe", "ref_doc_id": "636d82cbe736f0e4b87074e2c4bac77becae2a28"}, "3e7c21df-7e8b-473c-90f7-fed1cb0d2511": {"doc_hash": "f32a198d059a2a8053450e0e3829613867a42f07c2ddcc1c5d6cc579d095acc0", "ref_doc_id": "636d82cbe736f0e4b87074e2c4bac77becae2a28"}, "c2785032-514c-4f06-b894-c367e44a06cd": {"doc_hash": "a44caef1dcad3a5f9e07a2fe1696d6bdc69e5bb324933ecd319ec4926000f9a2", "ref_doc_id": "636d82cbe736f0e4b87074e2c4bac77becae2a28"}, "86a518cd-a160-4b6e-a600-3e8e04e7bbf6": {"doc_hash": "7f679fee5141a5c95d43893549e64667ea6dc2a4cbd58457910b27d8cc9c8e8e", "ref_doc_id": "636d82cbe736f0e4b87074e2c4bac77becae2a28"}, "251985eb-6011-43d3-ad55-53a3d4c7ee30": {"doc_hash": "04aa6c495b9b15985cb26255292e1687419679db4c7d66715c730a106da84ac0", "ref_doc_id": "636d82cbe736f0e4b87074e2c4bac77becae2a28"}, "60b68069-0cc2-495f-a9f5-add32930ef46": {"doc_hash": "38f066063646ff7fc82b687dc9ebc7fa812fbf855e72c1d712c558229c3d802d", "ref_doc_id": "636d82cbe736f0e4b87074e2c4bac77becae2a28"}, "3df84d5b-5623-4ffb-bc05-0bbdf371eefd": {"doc_hash": "fdfe9f07907c3140c9160a778b409befa4f3a4ce1d841caac9f4ad9111dba251", "ref_doc_id": "636d82cbe736f0e4b87074e2c4bac77becae2a28"}, "65ce763f-c431-435e-9657-0b6f2d5fc9c4": {"doc_hash": "48d6a25b39e6f92b3b5956c9d459780e69f3c92c57da741d80935b86353dad3d", "ref_doc_id": "636d82cbe736f0e4b87074e2c4bac77becae2a28"}, "df5ea1f3-e42e-4bb5-a8b3-e3a101ead26b": {"doc_hash": "0b8d8186d65e2b1b16fec14c7d5307ce5400a77fc9da470e56fd13a77f4971d4", "ref_doc_id": "636d82cbe736f0e4b87074e2c4bac77becae2a28"}, "8b5ef574-ebaa-4afd-8777-78f7b986a5a1": {"doc_hash": "cc405d6c98f25aa4de7db21a9a4cafa6e7eaccf417d091cca17be27d59498201", "ref_doc_id": "636d82cbe736f0e4b87074e2c4bac77becae2a28"}, "342261b5-48ee-4425-8897-2a96bd90006c": {"doc_hash": "5a56cde521a22d381f69d37154f1de184d4e2d691a19aee5d9d194fee919539c", "ref_doc_id": "636d82cbe736f0e4b87074e2c4bac77becae2a28"}, "6bf31471-1b03-4690-a43e-c8c71d8bd884": {"doc_hash": "ab537e6f20d59387fc1b1db89be69618d5f94da2f53151381cfcde665d1cde19", "ref_doc_id": "636d82cbe736f0e4b87074e2c4bac77becae2a28"}, "bba317d1-c5eb-4944-9ba2-0912a09d1720": {"doc_hash": "debbdb4a04f88c59a5a4e1bc19796a4150f8aa96ebc99e5de779e402870d45d3", "ref_doc_id": "5f68cc26ac6f6f085c287888a51130f9e6d399a1"}, "849314f2-9d75-4761-a629-218cd4f2bc8f": {"doc_hash": "8b400d9fe1b74cea49d1eed844b5d5919855f61e1099bdfc3751ed3d5eceeeab", "ref_doc_id": "5f68cc26ac6f6f085c287888a51130f9e6d399a1"}, "b1f0978e-8242-448a-85b3-6bb61f702256": {"doc_hash": "8f188946c1803f6d10cea4473539b42b3d5d1e5a7f09ac874fd757275d5cc822", "ref_doc_id": "db7a594acaaabbb6b84352a6719c7045b36a31c3"}, "987e8618-c2ad-403d-8e4a-e478f0ab5260": {"doc_hash": "1a733e30695a71974ac65a9f1f0d752a9bcc08f95c6da89ff5d8e123e59731a4", "ref_doc_id": "db7a594acaaabbb6b84352a6719c7045b36a31c3"}, "d245941a-e0af-4e45-a04b-e23ff5fbfdef": {"doc_hash": "a92f6282c45cbe66489bfdddd6973e2b8fa38d81b2c2c600e345acfaa0a8a805", "ref_doc_id": "db7a594acaaabbb6b84352a6719c7045b36a31c3"}, "2ea9be27-a1af-461b-ab04-71f4ac6fca73": {"doc_hash": "a158d97f641fadea8a7b5e9945c3cbc16a4bb63d1a66e20d7b135912ab3e82f1", "ref_doc_id": "db7a594acaaabbb6b84352a6719c7045b36a31c3"}, "21251270-23dc-4a6f-b29f-b9c13a48c226": {"doc_hash": "724a826940b825ac87b695cb9b59747a0f8d63b579d403134446134d5f8b18bc", "ref_doc_id": "db7a594acaaabbb6b84352a6719c7045b36a31c3"}, "964776da-1750-475e-9ff5-efe227f3cb04": {"doc_hash": "9cd49e8d94a095c38ddfd897716757bfa58d7aaee32362b26b11e274f6cbb78b", "ref_doc_id": "db7a594acaaabbb6b84352a6719c7045b36a31c3"}, "44945d41-5a12-491a-924a-b97e239af1ba": {"doc_hash": "c03d98e35c956443404c27d03b02627b7ccfc6d3960515997b10e86e393df375", "ref_doc_id": "db7a594acaaabbb6b84352a6719c7045b36a31c3"}, "1ed85396-1fe1-4c6b-ace0-dab134705495": {"doc_hash": "c5a67f87512b4c6821bcb43746c25a21eba07c1ac6fd934f7aa5024cb3fdf2b5", "ref_doc_id": "db7a594acaaabbb6b84352a6719c7045b36a31c3"}, "e23489cd-f252-48fb-ace5-bfce5c2f3251": {"doc_hash": "d12d7346ed74796017d1e57fcbae87c6df32fd0fcdfb54e4cb9e2147a5e96fb1", "ref_doc_id": "db7a594acaaabbb6b84352a6719c7045b36a31c3"}, "8e6ad538-2e89-48a3-854c-f7a8abe351ee": {"doc_hash": "b7e05bbf3c406063477f5621f3610776e62d3c17a3761d90f3edeca1bf2c4ac0", "ref_doc_id": "db7a594acaaabbb6b84352a6719c7045b36a31c3"}, "3a4bc64a-71be-4851-aa46-f9163e424db4": {"doc_hash": "1838bb5ddf818ae8554a704b7e6ea96b4f92fc0cc23c0e793e5bec4fceca7098", "ref_doc_id": "db7a594acaaabbb6b84352a6719c7045b36a31c3"}, "2670d1e3-f2c4-4e6a-a234-545c9ca4f909": {"doc_hash": "80ee066930a3d04a31c854653f0236cbf71d13cfe38d65889fadba60cb5dc0cb", "ref_doc_id": "db7a594acaaabbb6b84352a6719c7045b36a31c3"}, "c907ebdd-ec57-495c-b258-3bbae9ff432f": {"doc_hash": "cb73c5c6ba93630f049c2c01eddeb0385d9074c357e2e3b856df03a5d5c1909d", "ref_doc_id": "db7a594acaaabbb6b84352a6719c7045b36a31c3"}, "7dd06df2-2281-4ee7-bffa-637628a09c0c": {"doc_hash": "d4f79c0735088a4573055d0e6a2e2a3ebded6cde082b29495db20115a7083910", "ref_doc_id": "db7a594acaaabbb6b84352a6719c7045b36a31c3"}, "6b96c19c-e8a7-4e6f-bbd4-e7691ddf6117": {"doc_hash": "2e1412a0acebfd9a76b680e4247a48dac59bb168309ef8682cc4134e5e0a99b3", "ref_doc_id": "db7a594acaaabbb6b84352a6719c7045b36a31c3"}, "91d12dfc-dd9f-4c13-abf6-eeacb0925fa4": {"doc_hash": "85e6cc4f1b87c5684baa2d8ed3b4e5dd0043f61866f73b690b0c7f689ac7b434", "ref_doc_id": "db7a594acaaabbb6b84352a6719c7045b36a31c3"}, "023b4e6a-1f76-4f8b-b548-863c6dc27d3b": {"doc_hash": "31c85af89dc63bd19e2a577c600c842b8e4c4b52cc42cd1b8c56166403ddfa06", "ref_doc_id": "db7a594acaaabbb6b84352a6719c7045b36a31c3"}, "767b9b4f-6546-4376-8122-1aec03371b68": {"doc_hash": "8210489c05ab74f6bcecdcc080c394de47565ab6f0f20755753ba93c92d89553", "ref_doc_id": "db7a594acaaabbb6b84352a6719c7045b36a31c3"}, "0ed1fe50-3b49-4022-b72f-f684715aa191": {"doc_hash": "afa9c1cd66ef069b8f26da976ae9ea7ef6983399ea6a1e6e77c7b6b2db989a2b", "ref_doc_id": "db7a594acaaabbb6b84352a6719c7045b36a31c3"}, "6da8aa39-9c5d-4a3b-8a63-9157b7c1a241": {"doc_hash": "9e50a1cd5e0b41cab855ab67bf855cd7f40512a08ecfe9cae43e4143a04b293a", "ref_doc_id": "db7a594acaaabbb6b84352a6719c7045b36a31c3"}, "adf110de-6ee7-40eb-a6e4-656d82d04f05": {"doc_hash": "ad0a64dfbe0ddc229a5159e46f7f481b113911cb6692da72d6b1676c6ff6c3bd", "ref_doc_id": "db7a594acaaabbb6b84352a6719c7045b36a31c3"}, "3ef9a297-63d2-4050-bfb4-3b8a46d1aea4": {"doc_hash": "5eb2068caa6d9ae9878dfad03f3263988d13df13bfb8561a7f177938ded91170", "ref_doc_id": "db7a594acaaabbb6b84352a6719c7045b36a31c3"}, "cad50b16-721e-4045-949f-2ec998dc9b3b": {"doc_hash": "a181be733d03156e1a9bf241df24fccdd7635d8424bec7d3f51b1f4a275f520d", "ref_doc_id": "db7a594acaaabbb6b84352a6719c7045b36a31c3"}, "1958a5c9-7550-484c-96a3-942056c58345": {"doc_hash": "68e380dd0600a7c6e09ce8f4ec8aa6a4f4150a3e839e0a18a6a326755e6c7c8a", "ref_doc_id": "db7a594acaaabbb6b84352a6719c7045b36a31c3"}, "28409c52-9258-4335-941d-ac167bcaa839": {"doc_hash": "d442943f51c77f80cbf380fb3941693d60327a1df337de6ee67738c699699615", "ref_doc_id": "db7a594acaaabbb6b84352a6719c7045b36a31c3"}, "a3f69744-aecd-4d85-86aa-dc85a3654336": {"doc_hash": "ec20c2ec767152b77fa204cfc7029c632c6660f46b244e076c307341aec3e347", "ref_doc_id": "db7a594acaaabbb6b84352a6719c7045b36a31c3"}, "565938cc-1933-4079-bdfc-35ef73169e62": {"doc_hash": "c76c246ffeda757f42c0cd4a48201cc8ca549259ee7fd0f60c25874a9efb469d", "ref_doc_id": "db7a594acaaabbb6b84352a6719c7045b36a31c3"}, "92a3ac49-2d11-4c91-8adb-2f2a3de43477": {"doc_hash": "f192bb0e7c569df0052a7c3994e27b16cf43e033cf8a6969f8d2b33c8e2fd092", "ref_doc_id": "db7a594acaaabbb6b84352a6719c7045b36a31c3"}, "c5c76d6d-dc87-42f6-a4f4-f60200c7089a": {"doc_hash": "abcab50dc176c94019e8004f26e997bc3a157d73e42c78d5390e76e132937066", "ref_doc_id": "3778126c3f8d6c114a4804ff3e62747973115392"}, "5f38ced3-f532-4e1e-8e2e-a157da62bd55": {"doc_hash": "69fc0e4bceeab64b631d145a2ac2d632781e6630eb49cc07427f52f0fe34fb26", "ref_doc_id": "3778126c3f8d6c114a4804ff3e62747973115392"}, "191609f6-dcb7-46b3-8759-31889a56726b": {"doc_hash": "8e614fc462be1e0c3575934522ab43d6c11465f4aa1d43f2a3e5bd36380f4549", "ref_doc_id": "3778126c3f8d6c114a4804ff3e62747973115392"}, "21988fc6-30c1-4e66-8fd0-3bdaa70ee9ad": {"doc_hash": "05f7b9bfcafd9df532a22abb0100a8cb66684ba06fabfeba163b53af35f7b07c", "ref_doc_id": "3778126c3f8d6c114a4804ff3e62747973115392"}, "5bdedd96-7a5b-4418-8c1d-ad10f33d2b70": {"doc_hash": "8cd97738b366848766633e9652c1e3c7458be329900e1e32a2cf8227a0a6ade4", "ref_doc_id": "3778126c3f8d6c114a4804ff3e62747973115392"}, "bf8d2480-0ae0-4cf0-adf6-7abf6215d9aa": {"doc_hash": "03920edeeac1a302eb1ddbff58dc6cc4125b05ad27684bd751d4bec4c0d1a39b", "ref_doc_id": "3778126c3f8d6c114a4804ff3e62747973115392"}, "9f9fdf29-987f-484d-9f71-984d46de8d01": {"doc_hash": "132bfd3f8146466fe732cb753e688e056b2183ccc07d7262a3fafb4f00e77fcd", "ref_doc_id": "3778126c3f8d6c114a4804ff3e62747973115392"}, "0e1ff258-fcaf-4962-92af-fbd7a8475a7a": {"doc_hash": "21145a4d1efdee493a6b7fa8d1b958ab11ec85f43d5df4dfe60dc2a7fb259526", "ref_doc_id": "3778126c3f8d6c114a4804ff3e62747973115392"}, "21bca994-6699-410b-a1a9-b732721bd33b": {"doc_hash": "23218ecee789a641a8bcbfbcc321a2f86fa4190a3d36c440a3808e3ea13ca03c", "ref_doc_id": "3778126c3f8d6c114a4804ff3e62747973115392"}, "92288b6a-332b-4c21-804a-68e1301d538f": {"doc_hash": "424ad83bb050f512193668bd71cccfc3168e4cbc424685ca9fba773727440319", "ref_doc_id": "3778126c3f8d6c114a4804ff3e62747973115392"}, "35041c69-67b4-46b1-9093-01f260631494": {"doc_hash": "b9a461dbcdd1761b287adb08d14e95348ab24aceb4afd5398f57b1db1236d9cf", "ref_doc_id": "3778126c3f8d6c114a4804ff3e62747973115392"}, "0cecd8ba-77b1-4051-98a7-01627b961b02": {"doc_hash": "eb3ece60962a2d6c380b6c43fe1549fc3ef471a41539657879908d5fde55a626", "ref_doc_id": "3778126c3f8d6c114a4804ff3e62747973115392"}, "d0141c92-fc27-4d55-b4d1-94076acfdce8": {"doc_hash": "4cc12e1fb57c037daa9314c6c055de138b5ece7700c078b8b9b6f11a342dee15", "ref_doc_id": "3778126c3f8d6c114a4804ff3e62747973115392"}, "baad20ff-c3fb-424a-9810-140d2ab4696a": {"doc_hash": "3ea20813cab9ab1272788a1971ecb51f43b08c0295144aff6e210a79cfcaf307", "ref_doc_id": "3778126c3f8d6c114a4804ff3e62747973115392"}, "95baefbe-e992-4876-82a3-a2e7dff9aee6": {"doc_hash": "882ab05bfcc8a5faba6cf52c1030e9a8e0873619f2f917e16147036e6265674f", "ref_doc_id": "1f6253a69f7dc9d6db384a333c1e40582b2d4cb7"}, "c7c3fb4b-734c-4a9f-8a50-77f6c805f858": {"doc_hash": "c6c529fee97db848b02a4c911553427fb799acfc1b9241adba95cd83d0d42324", "ref_doc_id": "aac7341d37e177f35e582c98b61ee22a8c002f39"}, "32cd74a7-d777-4148-91f3-59212a5e7db2": {"doc_hash": "57232693f60918be055d5369090ebda09a2706d65bec4a70db5cd82f2475c474", "ref_doc_id": "aac7341d37e177f35e582c98b61ee22a8c002f39"}, "f24222d1-22a0-4213-8821-0bea3b01faca": {"doc_hash": "baecb23221caea8037e6f35f430e6d44e4ca09923ff1b9e489a9cfc94a0208b0", "ref_doc_id": "aac7341d37e177f35e582c98b61ee22a8c002f39"}, "162daf80-f111-47e5-9363-d502d3e827c4": {"doc_hash": "96b4ccacce22f4acd61892ff84d10d66a375243312f459903d99103e3da4c8f2", "ref_doc_id": "aac7341d37e177f35e582c98b61ee22a8c002f39"}, "5ff7a0d0-1e3e-4c78-8506-2bc7eb2a58f0": {"doc_hash": "878ff2ee7b0fa5a8dc34c8249afa261e96931d96a2a495f69f18535f9fa4ecb8", "ref_doc_id": "aac7341d37e177f35e582c98b61ee22a8c002f39"}, "38003815-e0f2-4dd1-9af3-0d93d869a3f2": {"doc_hash": "781e1a2dcf3f92bf300356977b7ea1504da4a31950dc7e1d5a0c763287ef8389", "ref_doc_id": "aac7341d37e177f35e582c98b61ee22a8c002f39"}, "e28da2fd-46b1-4067-b0a6-664354fd3566": {"doc_hash": "4f653e63b4056984d488a95e81870641c95e1aafd516c5e3cf03d0ae75827bf2", "ref_doc_id": "aac7341d37e177f35e582c98b61ee22a8c002f39"}, "3038151e-cec1-4531-a55a-943885881fcf": {"doc_hash": "24540141f71e88ed0dd3142833e76da2fef1af52dc75489cac2ddf8fe73c1367", "ref_doc_id": "636b0952f819caa55fb9bafa992266d6e9ba4045"}, "cdc4fccb-2879-4e38-b6ce-c9eb068f92a4": {"doc_hash": "894adf8ef4bb1d51765246e1dcfc2f82ee2bb0af6bfaefd8baeb14bd046affde", "ref_doc_id": "636b0952f819caa55fb9bafa992266d6e9ba4045"}, "55081324-bdaf-4f4f-83b1-ef63c409f632": {"doc_hash": "5d6db4b8450fd8fde2043329398f4ea110bbf0d09d7bbf9d795a38805b6b442e", "ref_doc_id": "636b0952f819caa55fb9bafa992266d6e9ba4045"}, "27774c1b-5464-4b2d-9ce3-98c622896d64": {"doc_hash": "3fb7357f9bd0fd1a585f472997715ff8f751442c795149a04608289c7ba044a6", "ref_doc_id": "b53cbf19b410703f4d5108405c739c7cce41cf35"}, "f7a91cab-6829-4f8b-a8d1-ab23466363f1": {"doc_hash": "f23af7db56e7f14ec283fd2666808a2f1f83b894838e2713be67643cde81be66", "ref_doc_id": "b53cbf19b410703f4d5108405c739c7cce41cf35"}, "47b441db-a090-45d4-8a71-b5b5130d3c3d": {"doc_hash": "ed94e0f83100bcd325b097271d61c3a8b07c5a6609b6895b57e4b0813ccd9b0e", "ref_doc_id": "b53cbf19b410703f4d5108405c739c7cce41cf35"}, "7502534f-4fc0-47cf-a2e1-376d24608c28": {"doc_hash": "dab2277a5e396baa31b9d5d9eb2222c98fabd958ea579edccee04ae368a5495d", "ref_doc_id": "b53cbf19b410703f4d5108405c739c7cce41cf35"}, "b6a9a829-f63e-40f5-a777-385c8e1ac15f": {"doc_hash": "94483832c4d78e1ad98354913495ad1f09536d7ab6eb7fe826fa789788ca0e62", "ref_doc_id": "b53cbf19b410703f4d5108405c739c7cce41cf35"}, "631c1fc8-e333-4113-b6de-541d0f6699a6": {"doc_hash": "7a556aa2343dd89ba69fbfee67be849c5c1709c1f5976398909bdb04f3a2ac68", "ref_doc_id": "b53cbf19b410703f4d5108405c739c7cce41cf35"}, "eb83dd1a-d5c1-42b6-901b-d0d6c201a0e4": {"doc_hash": "55cc86fe58ce8b17c8692cb408e57252abce3a27a7b596f2145355136f78e4a0", "ref_doc_id": "b53cbf19b410703f4d5108405c739c7cce41cf35"}, "2cdbca24-3ddc-4512-b8fb-6a4096afc86a": {"doc_hash": "ce56f6586639cf91984f7591d1c7d17df7f615d1162066b3060ec6f42d4c7d4e", "ref_doc_id": "b53cbf19b410703f4d5108405c739c7cce41cf35"}, "46a61c50-ef38-4639-a4f2-f123b1869a18": {"doc_hash": "a861f8b20c72114855bbd71b82511381a20251093324b669b1dc02afc1b26c2f", "ref_doc_id": "b53cbf19b410703f4d5108405c739c7cce41cf35"}, "4ec64556-b6c0-4ffa-a53f-54b5ec9d9d4e": {"doc_hash": "8a313c97b7a19f8ed983a1bf5e97ff6b9ba0847044a052002b6a24581bd4ac77", "ref_doc_id": "b53cbf19b410703f4d5108405c739c7cce41cf35"}, "c034246f-6e6f-4396-adcf-07d1f0518ec5": {"doc_hash": "1382f5d6a2670deaa5251b477074b6b80bb66bd455780c1521178e1b04b69cc0", "ref_doc_id": "a33bb9aa3007e487161c4bbb30fe17f2a39bf993"}, "079822ac-1560-4bec-bf7c-5ca2b446fb6d": {"doc_hash": "39009fc9ce7e8a4d6b1a434adcab315e4f947590be066cd7dc2497f0062028d3", "ref_doc_id": "a33bb9aa3007e487161c4bbb30fe17f2a39bf993"}, "0f353ea5-010f-45b3-ad99-c0f6cc2ef073": {"doc_hash": "895c076933d069cb9d41c8562b64d4c45439471265db6be097fbebd8e58d6379", "ref_doc_id": "a33bb9aa3007e487161c4bbb30fe17f2a39bf993"}}, "docstore/data": {"1ec77f04-7b50-4419-a1e5-63eef72a8579": {"__data__": {"id_": "1ec77f04-7b50-4419-a1e5-63eef72a8579", "embedding": null, "metadata": {"file_path": "docs/README.md", "file_name": "README.md"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "f0b98c510df971d3ce3cba2df5454a8ddd109fc5", "node_type": null, "metadata": {"file_path": "docs/README.md", "file_name": "README.md"}, "hash": "dbd6fd3d84edfa4389132fdb01bf36c19029ddd65d66d956b33aa79405543b4d"}}, "hash": "f9df7dcf7e6215563b36c1c88d9cd1f3e645908afb5f980c62d75325e21afe21", "text": "# Nextflow Documentation\n\nNextflow documentation is written using [Sphinx](http://www.sphinx-doc.org/), [MyST](https://myst-parser.readthedocs.io/en/latest/) which is an extended version of Markdown for Sphinx, and the [Read The Docs theme for Sphinx](https://github.com/readthedocs/sphinx_rtd_theme).\n\n\n## Dependencies\n\nThe most convenient approach is to create a Conda environment with Python 3.7 (other versions may work but haven't been tested).\n\nThe build dependencies can be installed with `pip`:\n\n```bash\ncd docs\npip install -r requirements.txt\n```\n\nAlternatively, you can use the Dockerfile to build the docs in a container (see below).\n\n\n## Contributing\n\nTo edit and contribute to the documentation, you only need a text editor to change the appropriate `.md` files in this directory.\n\nOnce you have made your changes, run the following command to build the HTML files:\n\n```bash\nmake clean html\n```\n\nAlternatively, you can use the Dockerfile to build the docs in a container:\n\n```bash\ndocker build -t nextflow/sphinx:5.3.0 .\ndocker run -v $(pwd):/tmp nextflow/sphinx:5.3.0 -- make html\n```\n\nThen start up a local http server and open `localhost:8080` in your browser to verify the changes:\n\n```bash\npython -m http.server 8080 --directory _build/html/\n```\n\n\n## License\n\nNextflow documentation is distributed under\n[Creative Commons Attribution-ShareAlike 4.0 International (CC BY-SA 4.0) license](https://creativecommons.org/licenses/by-sa/4.0/).", "start_char_idx": 0, "end_char_idx": 1454, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "05ce8a6b-3f28-4366-9bbf-8361455a8b27": {"__data__": {"id_": "05ce8a6b-3f28-4366-9bbf-8361455a8b27", "embedding": null, "metadata": {"file_path": "docs/amazons3.md", "file_name": "amazons3.md"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "bfcabb23c961bc8f4b16cb62d6345edfce1c6ba8", "node_type": null, "metadata": {"file_path": "docs/amazons3.md", "file_name": "amazons3.md"}, "hash": "5ef5bf4591b9a0e6706a9117761d609f3a518565c15d07107dbc99b14a0d9705"}, "3": {"node_id": "c7abbe2c-8cd8-42d1-9600-64ff0faf3063", "node_type": null, "metadata": {"file_path": "docs/amazons3.md", "file_name": "amazons3.md"}, "hash": "9d55f382c2fb7996cbcd89b6101f9d41aca2690581850c4e905da3f0ba5fbc53"}}, "hash": "0f3a7cb083ca79b3539d5c7b36269fe561a8be938b237f3223e7836a8cb13ad1", "text": "(amazons3-page)=\n\n# Amazon S3 storage\n\nNextflow includes support for Amazon S3 storage. Files stored in an S3 bucket can be accessed transparently in your pipeline script like any other file in the local file system.\n\n## S3 path\n\nIn order to access an S3 file, you only need to prefix the file path with the `s3` schema and the `bucket` name where it is stored.\n\nFor example, if you need to access the file `/data/sequences.fa` stored in a bucket named `my-bucket`, that file can be accessed using the following fully qualified path:\n\n```\ns3://my-bucket/data/sequences.fa\n```\n\nThe usual file operations can be applied to a path handle with the above notation. For example, the content of an S3 file can be printed as follows:\n\n```groovy\nprintln file('s3://my-bucket/data/sequences.fa').text\n```\n\nSee the {ref}`script-file-io` section to learn more about available file operations.\n\n## Security credentials\n\nAmazon access credentials can be provided in two ways:\n\n1. Using AWS access and secret keys in your pipeline configuration.\n2. Using IAM roles to grant access to S3 storage on Amazon EC2 instances.\n\n### AWS access and secret keys\n\nThe AWS access and secret keys can be specified by using the `aws` section in the `nextflow.config` configuration file as shown below:\n\n```groovy\naws {\n    accessKey = '<Your AWS access key>'\n    secretKey = '<Your AWS secret key>'\n    region = '<AWS region identifier>'\n}\n```\n\nIf the access credentials are not found in the above file, Nextflow looks for AWS credentials in the following order:\n\n1. The `nextflow.config` file in the pipeline execution directory\n2. The environment variables `AWS_ACCESS_KEY_ID` and `AWS_SECRET_ACCESS_KEY`\n3. The environment variables `AWS_ACCESS_KEY` and `AWS_SECRET_KEY`\n4. The `default` profile in the AWS credentials file located at `~/.aws/credentials`\n5. The `default` profile in the AWS client configuration file located at `~/.aws/config`\n6. The temporary AWS credentials provided by an IAM instance role. See [IAM Roles](http://docs.aws.amazon.com/AWSEC2/latest/UserGuide/iam-roles-for-amazon-ec2.html) documentation for details.\n\nMore information regarding [AWS Security Credentials](http://docs.aws.amazon.com/general/latest/gr/aws-security-credentials.html) are available in the AWS documentation.\n\n### IAM roles with Amazon EC2 instances\n\nWhen running your pipeline in an EC2 instance, IAM roles can be used to grant access to AWS resources.\n\nIn this scenario, you only need to launch the EC2 instance with an IAM role which includes the `AmazonS3FullAccess` policy. Nextflow will detect and automatically acquire the permission to access S3 storage, without any further configuration.\n\nLearn more about [Using IAM Roles to Delegate Permissions to Applications that Run on Amazon EC2](http://docs.aws.amazon.com/IAM/latest/UserGuide/roles-usingrole-ec2instance.html) in the Amazon documentation.\n\n## China regions\n\nTo use an AWS China region, make sure to specify the corresponding AWS API S3 endpoint in the Nextflow configuration file as shown below:\n\n```groovy\naws { \n    client {\n        endpoint = \"https://s3.cn-north-1.amazonaws.com.cn\"        \n    }\n}\n```\n\nRead more about AWS API endpoints in the [AWS", "start_char_idx": 0, "end_char_idx": 3195, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "c7abbe2c-8cd8-42d1-9600-64ff0faf3063": {"__data__": {"id_": "c7abbe2c-8cd8-42d1-9600-64ff0faf3063", "embedding": null, "metadata": {"file_path": "docs/amazons3.md", "file_name": "amazons3.md"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "bfcabb23c961bc8f4b16cb62d6345edfce1c6ba8", "node_type": null, "metadata": {"file_path": "docs/amazons3.md", "file_name": "amazons3.md"}, "hash": "5ef5bf4591b9a0e6706a9117761d609f3a518565c15d07107dbc99b14a0d9705"}, "2": {"node_id": "05ce8a6b-3f28-4366-9bbf-8361455a8b27", "node_type": null, "metadata": {"file_path": "docs/amazons3.md", "file_name": "amazons3.md"}, "hash": "0f3a7cb083ca79b3539d5c7b36269fe561a8be938b237f3223e7836a8cb13ad1"}}, "hash": "9d55f382c2fb7996cbcd89b6101f9d41aca2690581850c4e905da3f0ba5fbc53", "text": "more about AWS API endpoints in the [AWS documentation](https://docs.aws.amazon.com/general/latest/gr/s3.html)\n\n## S3-compatible storage\n\nTo use S3-compatible object storage such as [Ceph](https://ceph.io) or [Minio](https://min.io) specify the endpoint of \nyour storage provider and enable the [S3 path style access](https://docs.aws.amazon.com/AmazonS3/latest/userguide/VirtualHosting.html#path-style-access) \nin your Nextflow configuration as shown below:\n\n\n```groovy\naws {\n    accessKey = '<Your access key>'\n    secretKey = '<Your secret key>'\n    client {\n        endpoint = '<Your storage endpoint URL>'\n        s3PathStyleAccess = true\n    }\n}\n```\n\n## Advanced configuration\n\nRead {ref}`AWS configuration<config-aws>` section to learn more about advanced S3 client configuration options.", "start_char_idx": 3155, "end_char_idx": 3950, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "1b1c0d36-3430-4f38-b3c1-3c02cd7c555a": {"__data__": {"id_": "1b1c0d36-3430-4f38-b3c1-3c02cd7c555a", "embedding": null, "metadata": {"file_path": "docs/aws.md", "file_name": "aws.md"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "edc18535956f4832534c0eb7f12118f393382241", "node_type": null, "metadata": {"file_path": "docs/aws.md", "file_name": "aws.md"}, "hash": "154c98679f76c18bfd99631f49eb8e7e0ac30f7028a82c41867f554dd506983a"}, "3": {"node_id": "a0f026f5-95a0-4489-9a27-e8fb6aa0b87d", "node_type": null, "metadata": {"file_path": "docs/aws.md", "file_name": "aws.md"}, "hash": "622dcbdd30b841e0096ff3aebe21d3a774e6a4e65c0525926f7ab273c1e010fb"}}, "hash": "6f6caf7e6324648fefde633a3621a1e130bd824d911d9145ec0db08d2be68d94", "text": "(aws-page)=\n\n# Amazon Web Services\n\n## AWS security credentials\n\nNextflow uses the [AWS security credentials](https://docs.aws.amazon.com/general/latest/gr/aws-sec-cred-types.html) to make programmatic calls to AWS services.\n\nYou can provide your AWS access keys using the standard AWS variables shown below:\n\n- `AWS_ACCESS_KEY_ID`\n- `AWS_SECRET_ACCESS_KEY`\n- `AWS_DEFAULT_REGION`\n\nIf `AWS_ACCESS_KEY_ID` and `AWS_SECRET_ACCESS_KEY` are not defined in the environment, Nextflow will attempt to\nretrieve credentials from your `~/.aws/credentials` and `~/.aws/config` files. The `default` profile can be\noverridden via the environmental variable `AWS_PROFILE` (or `AWS_DEFAULT_PROFILE`).\n\nAlternatively AWS credentials and profile can be specified in the Nextflow configuration file. See {ref}`AWS configuration<config-aws>` for more details.\n\n:::{note}\nCredentials can also be provided by using an IAM Instance Role. The benefit of this approach is that it spares you from managing/distributing AWS keys explicitly. Read the [IAM Roles](http://docs.aws.amazon.com/AWSEC2/latest/UserGuide/iam-roles-for-amazon-ec2.html) documentation and [this blog post](https://aws.amazon.com/blogs/security/granting-permission-to-launch-ec2-instances-with-iam-roles-passrole-permission/) for more details.\n:::\n\n## AWS IAM policies\n\n[IAM policies](https://docs.aws.amazon.com/IAM/latest/UserGuide/access_policies.html) are the mechanism used by AWS to defines permissions for IAM identities. In order to access certain AWS services, the proper policies must be attached to the identity associated to the AWS credentials.\n\nMinimal permissions policies to be attached to the AWS account used by Nextflow are:\n\n- To use AWS Batch:\n\n  ```\n  \"batch:DescribeJobQueues\"\n  \"batch:CancelJob\"\n  \"batch:SubmitJob\"\n  \"batch:ListJobs\"\n  \"batch:DescribeComputeEnvironments\"\n  \"batch:TerminateJob\"\n  \"batch:DescribeJobs\"\n  \"batch:RegisterJobDefinition\"\n  \"batch:DescribeJobDefinitions\"\n  ```\n\n- To view [EC2](https://aws.amazon.com/ec2/) instances:\n\n  ```\n  \"ecs:DescribeTasks\"\n  \"ec2:DescribeInstances\"\n  \"ec2:DescribeInstanceTypes\"\n  \"ec2:DescribeInstanceAttribute\"\n  \"ecs:DescribeContainerInstances\"\n  \"ec2:DescribeInstanceStatus\"\n  ```\n\n- To pull container images from [ECR](https://aws.amazon.com/ecr/) repositories:\n\n  ```\n  \"ecr:GetAuthorizationToken\"\n  \"ecr:BatchCheckLayerAvailability\"\n  \"ecr:GetDownloadUrlForLayer\"\n  \"ecr:GetRepositoryPolicy\"\n  \"ecr:DescribeRepositories\"\n  \"ecr:ListImages\"\n  \"ecr:DescribeImages\"\n  \"ecr:BatchGetImage\"\n  \"ecr:GetLifecyclePolicy\"\n  \"ecr:GetLifecyclePolicyPreview\"\n  \"ecr:ListTagsForResource\"\n  \"ecr:DescribeImageScanFindings\"\n  ```\n\n### S3 policies\n\nNextflow also requires policies to access [S3 buckets](https://aws.amazon.com/s3/) in order to use the work directory, pull input data, and publish results.\n\nDepending on the pipeline configuration, the above actions can be done all in a single bucket but, more likely, spread across multiple", "start_char_idx": 0, "end_char_idx": 2954, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "a0f026f5-95a0-4489-9a27-e8fb6aa0b87d": {"__data__": {"id_": "a0f026f5-95a0-4489-9a27-e8fb6aa0b87d", "embedding": null, "metadata": {"file_path": "docs/aws.md", "file_name": "aws.md"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "edc18535956f4832534c0eb7f12118f393382241", "node_type": null, "metadata": {"file_path": "docs/aws.md", "file_name": "aws.md"}, "hash": "154c98679f76c18bfd99631f49eb8e7e0ac30f7028a82c41867f554dd506983a"}, "2": {"node_id": "1b1c0d36-3430-4f38-b3c1-3c02cd7c555a", "node_type": null, "metadata": {"file_path": "docs/aws.md", "file_name": "aws.md"}, "hash": "6f6caf7e6324648fefde633a3621a1e130bd824d911d9145ec0db08d2be68d94"}, "3": {"node_id": "79916c62-fc44-45c7-8fe0-3a1f992df87d", "node_type": null, "metadata": {"file_path": "docs/aws.md", "file_name": "aws.md"}, "hash": "3383a3f1faed43f070d30aeeb92a20996001282a71a3a1a8ced13f0a0a3563fb"}}, "hash": "622dcbdd30b841e0096ff3aebe21d3a774e6a4e65c0525926f7ab273c1e010fb", "text": "above actions can be done all in a single bucket but, more likely, spread across multiple buckets. Once the list of buckets used by the pipeline is identified, there are two alternative ways to give Nextflow access to these buckets:\n\n1. Grant access to all buckets by attaching the policy `\"s3:*\"` to the IAM identity. This works only if buckets do not set their own access policies (see point 2);\n\n2. For more fine grained control, assign to each bucket the following policy (replace the placeholders with the actual values):\n\n    ```json\n    {\n        \"Version\": \"2012-10-17\",\n        \"Id\": \"<my policy id>\",\n        \"Statement\": [\n            {\n                \"Sid\": \"<my statement id>\",\n                \"Effect\": \"Allow\",\n                \"Principal\": {\n                    \"AWS\": \"<ARN of the nextflow identity>\"\n                },\n                \"Action\": [\n                    \"s3:GetObject\",\n                    \"s3:PutObject\",\n                    \"s3:DeleteObject\"\n                ],\n                \"Resource\": \"arn:aws:s3:::<bucket name>/*\"\n            },\n            {\n                \"Sid\": \"AllowSSLRequestsOnly\",\n                \"Effect\": \"Deny\",\n                \"Principal\": \"*\",\n                \"Action\": \"s3:*\",\n                \"Resource\": [\n                    \"arn:aws:s3:::<bucket name>\",\n                    \"arn:aws:s3:::<bucket name>/*\"\n                ],\n                \"Condition\": {\n                    \"Bool\": {\n                        \"aws:SecureTransport\": \"false\"\n                    }\n                }\n            }\n        ]\n    }\n    ```\n\nSee the [bucket policy documentation](https://docs.aws.amazon.com/config/latest/developerguide/s3-bucket-policy.html) for additional details.\n\n(aws-batch)=\n\n## AWS Batch\n\n[AWS Batch](https://aws.amazon.com/batch/) is a managed computing service that allows the execution of containerised workloads in the Amazon cloud infrastructure. It dynamically provisions the optimal quantity and type of compute resources (e.g., CPU or memory optimized compute resources) based on the volume and specific resource requirements", "start_char_idx": 2881, "end_char_idx": 4972, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "79916c62-fc44-45c7-8fe0-3a1f992df87d": {"__data__": {"id_": "79916c62-fc44-45c7-8fe0-3a1f992df87d", "embedding": null, "metadata": {"file_path": "docs/aws.md", "file_name": "aws.md"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "edc18535956f4832534c0eb7f12118f393382241", "node_type": null, "metadata": {"file_path": "docs/aws.md", "file_name": "aws.md"}, "hash": "154c98679f76c18bfd99631f49eb8e7e0ac30f7028a82c41867f554dd506983a"}, "2": {"node_id": "a0f026f5-95a0-4489-9a27-e8fb6aa0b87d", "node_type": null, "metadata": {"file_path": "docs/aws.md", "file_name": "aws.md"}, "hash": "622dcbdd30b841e0096ff3aebe21d3a774e6a4e65c0525926f7ab273c1e010fb"}, "3": {"node_id": "445a9234-9058-4838-91cf-f5dc12a42ee6", "node_type": null, "metadata": {"file_path": "docs/aws.md", "file_name": "aws.md"}, "hash": "b2fcfbc58d3caa9e8ab06f5730815e0a5b26f23853977e72056af4eeb0a7abf6"}}, "hash": "3383a3f1faed43f070d30aeeb92a20996001282a71a3a1a8ced13f0a0a3563fb", "text": "CPU or memory optimized compute resources) based on the volume and specific resource requirements of the jobs submitted.\n\nNextflow provides built-in support for AWS Batch, allowing the seamless deployment of Nextflow pipelines in the cloud, in which tasks are offloaded as Batch jobs.\n\nRead the {ref}`AWS Batch executor <awsbatch-executor>` section to learn more about the `awsbatch` executor in Nextflow.\n\n(aws-batch-config)=\n\n### AWS CLI\n\nNextflow needs the [AWS command line tool](https://aws.amazon.com/cli/) (`aws`) to be available in the container in which tasks are executed, in order to stage input files and output files to and from S3 storage.\n\n:::{tip}\nWhen using {ref}`wave-page` and {ref}`fusion-page`, the AWS command line tool is not needed for task containers or the underlying EC2 instances when running Nextflow on AWS Batch. See the {ref}`fusion-page` documentation for more details.\n:::\n\nThe `aws` command can be made available in the container in two ways:\n\n1. Installed in the Docker image(s) used during the pipeline execution,\n2. Installed in a custom [AMI (Amazon Machine Image)](https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/AMIs.html) to use in place of the default AMI when configuring AWS Batch (see next section).\n\nThe latter approach is preferred because it allows the use of existing Docker images without having to add the AWS CLI to each one.\n\nSee the sections below to learn how to create a custom AMI and install the AWS CLI tool in it.\n\n### Get started\n\n1. In the AWS Console, navigate to **AWS Batch** and create a [Compute environment](http://docs.aws.amazon.com/batch/latest/userguide/compute_environments.html) (CE).\n\n   1. If you are using a custom AMI (see following sections), the AMI ID must be specified in the CE configuration\n   2. Make sure to select an AMI (either custom or existing) with Docker installed (see following sections)\n   3. Make sure the policy `AmazonS3FullAccess` (granting access to S3 buckets) is attached to the instance role configured for the CE\n   4. If you plan to use Docker images from Amazon ECS container, make sure the `AmazonEC2ContainerServiceforEC2Role` policy is also attached to the instance role\n\n2. In the AWS Console, create (at least) one [Job Queue](https://docs.aws.amazon.com/batch/latest/userguide/job_queues.html) and bind it to the Compute environment.\n\n3. In the AWS Console, create an S3 bucket for the work directory (see below). You can also create separate buckets for input data and results, as needed.\n\n4. Make sure that every process in your pipeline specifies a Docker container with the {ref}`process-container` directive.\n\n5. Make sure that all of your container images are published in a Docker registry that can be reached by AWS Batch, such as [Docker Hub](https://hub.docker.com/), [Quay](https://quay.io/), or [Elastic Container Registry](https://aws.amazon.com/ecr/).\n\n### Configuration\n\nTo configure your pipeline for AWS Batch:\n\n1. Specify the AWS Batch {ref}`executor <awsbatch-executor>`\n2. Specify one or more AWS Batch queues with the {ref}`process-queue` directive\n3. Specify any Batch job container options with the {ref}`process-containerOptions` directive.\n\nAn example `nextflow.config` file is shown below:\n\n```groovy\nprocess {\n    executor = 'awsbatch'\n    queue = 'my-batch-queue'\n    container = 'quay.io/biocontainers/salmon'\n    containerOptions = '--shm-size 16000000", "start_char_idx": 4963, "end_char_idx": 8363, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "445a9234-9058-4838-91cf-f5dc12a42ee6": {"__data__": {"id_": "445a9234-9058-4838-91cf-f5dc12a42ee6", "embedding": null, "metadata": {"file_path": "docs/aws.md", "file_name": "aws.md"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "edc18535956f4832534c0eb7f12118f393382241", "node_type": null, "metadata": {"file_path": "docs/aws.md", "file_name": "aws.md"}, "hash": "154c98679f76c18bfd99631f49eb8e7e0ac30f7028a82c41867f554dd506983a"}, "2": {"node_id": "79916c62-fc44-45c7-8fe0-3a1f992df87d", "node_type": null, "metadata": {"file_path": "docs/aws.md", "file_name": "aws.md"}, "hash": "3383a3f1faed43f070d30aeeb92a20996001282a71a3a1a8ced13f0a0a3563fb"}, "3": {"node_id": "ef3ef38f-542a-401b-aa20-fbc728ae3b62", "node_type": null, "metadata": {"file_path": "docs/aws.md", "file_name": "aws.md"}, "hash": "22a37c5d91883a1ac7e4c84603432c077c987e986c0c27ad52e184f7694759cc"}}, "hash": "b2fcfbc58d3caa9e8ab06f5730815e0a5b26f23853977e72056af4eeb0a7abf6", "text": "   containerOptions = '--shm-size 16000000 --ulimit nofile=1280:2560 --ulimit nproc=16:32'\n}\n\naws {\n    batch {\n        // NOTE: this setting is only required if the AWS CLI tool is installed in a custom AMI\n        cliPath = '/home/ec2-user/miniconda/bin/aws'\n    }\n    region = 'us-east-1'\n}\n```\n\nDifferent queues bound to the same or different Compute Environments can be configured according to each process' requirements.\n\n## Container Options\n\n:::{versionadded} 21.12.1-edge\n:::\n\nThe {ref}`process-containerOptions` directive can be used to control the properties of the container execution associated with each Batch job.\n\nThe following container options are currently supported:\n\n```\n-e, --env string\n    Set environment variables (format: <name> or <name>=<value>)\n--init\n    Run an init inside the container that forwards signals and reaps processes\n--memory-swap int\n    The total amount of swap memory (in MiB) the container can use: '-1' to enable unlimited swap\n--memory-swappiness int\n    Tune container memory swappiness (0 to 100) (default -1)\n--privileged\n    Give extended privileges to the container\n--read-only\n    Mount the container's root filesystem as read only\n--shm-size int\n    Size (in MiB) of /dev/shm\n--tmpfs string\n    Mount a tmpfs directory (format: <path>:<options>,size=<int>), size is in MiB\n-u, --user string\n    Username or UID (format: <name|uid>[:<group|gid>])\n--ulimit string\n    Ulimit options (format: <type>=<soft limit>[:<hard limit>])\n```\n\nContainer options may be passed in long form (e.g `--option value`) or short form (e.g. `-o value`) where available.\n\nFew examples:\n\n```groovy\ncontainerOptions '--tmpfs /run:rw,noexec,nosuid,size=128 --tmpfs /app:ro,size=64'\n\ncontainerOptions '-e MYVAR1 --env MYVAR2=foo2 --env MYVAR3=foo3 --memory-swap 3240000 --memory-swappiness 20 --shm-size 16000000'\n\ncontainerOptions '--ulimit nofile=1280:2560 --ulimit nproc=16:32 --privileged'\n```\n\nCheck the [AWS documentation](https://docs.aws.amazon.com/batch/latest/APIReference/API_ContainerProperties.html) for further details.\n\n## Custom AMI\n\nThere are several reasons why you might need to create your own [AMI (Amazon Machine Image)](https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/AMIs.html) to use in your Compute Environments:\n\n- You do not want to install the AWS CLI into each of your Docker images and would rather provide it through the AMI\n- The existing AMI (selected from the marketplace) does not have Docker installed\n- You need to attach more storage to your EC2 instance (the default ECS instance AMI has only a 30GB EBS volume which is not enough for most data pipelines)\n- You need to install additional software that is not available in your Docker image\n\n### Create your custom AMI\n\nFrom the EC2 Dashboard, select **Launch Instance**, then select **AWS Marketplace** in the left-hand pane and search for \"ECS\". In the result list, select **Amazon ECS-Optimized Amazon Linux 2 AMI**, then continue as usual to configure and launch the instance.\n\n:::{note}\nThe selected instance has a bootstrap volume of 8GB and a second EBS volume of 30GB for scratch storage, which is not enough for real genomic workloads. Make sure to specify an additional volume with", "start_char_idx": 8412, "end_char_idx": 11626, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "ef3ef38f-542a-401b-aa20-fbc728ae3b62": {"__data__": {"id_": "ef3ef38f-542a-401b-aa20-fbc728ae3b62", "embedding": null, "metadata": {"file_path": "docs/aws.md", "file_name": "aws.md"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "edc18535956f4832534c0eb7f12118f393382241", "node_type": null, "metadata": {"file_path": "docs/aws.md", "file_name": "aws.md"}, "hash": "154c98679f76c18bfd99631f49eb8e7e0ac30f7028a82c41867f554dd506983a"}, "2": {"node_id": "445a9234-9058-4838-91cf-f5dc12a42ee6", "node_type": null, "metadata": {"file_path": "docs/aws.md", "file_name": "aws.md"}, "hash": "b2fcfbc58d3caa9e8ab06f5730815e0a5b26f23853977e72056af4eeb0a7abf6"}, "3": {"node_id": "ec2dc4a0-8008-4498-8774-2afa4893a22e", "node_type": null, "metadata": {"file_path": "docs/aws.md", "file_name": "aws.md"}, "hash": "29b47e9ff48bcd7b3792a02ba465a2d385a21c21a84b0d681f96722ae93f6461"}}, "hash": "22a37c5d91883a1ac7e4c84603432c077c987e986c0c27ad52e184f7694759cc", "text": "is not enough for real genomic workloads. Make sure to specify an additional volume with enough storage for your pipeline execution.\n:::\n\nWhen the instance is running, SSH into it (or connect with the Session Manager service), install the AWS CLI, and install any other tool that may be required (see following sections).\n\nFinally, select **Create Image** from the EC2 Dashboard to create a new AMI from the running instance (you can also do it through the AWS CLI).\n\nThe new AMI ID needs to be specified when creating the Batch Compute Environment.\n\n:::{warning}\nAny additional software must be installed on the EC2 instance *before* creating the AMI.\n:::\n\n(id2)=\n\n### AWS CLI installation\n\n:::{tip}\nWhen using {ref}`wave-page` and {ref}`fusion-page`, the AWS command line tool is not needed for task containers or the underlying EC2 instances when running Nextflow on AWS Batch. See the {ref}`fusion-page` documentation for more details.\n:::\n\nThe [AWS CLI tool](https://aws.amazon.com/cli) should be installed in your custom AMI using a self-contained package manager such as [Conda](https://conda.io). That way, you can control which version of Python is used by the AWS CLI (which is written in Python).\n\nIf you don't use Conda, the `aws` command will attempt to use the version of Python that is installed in the container, and it won't be able to find the necessary dependencies.\n\nThe following snippet shows how to install AWS CLI with [Miniconda](https://conda.io/miniconda.html) in the home folder:\n\n```bash\ncd $HOME\nsudo yum install -y bzip2 wget\nwget https://repo.continuum.io/miniconda/Miniconda3-latest-Linux-x86_64.sh\nbash Miniconda3-latest-Linux-x86_64.sh -b -f -p $HOME/miniconda\n$HOME/miniconda/bin/conda install -c conda-forge -y awscli\nrm Miniconda3-latest-Linux-x86_64.sh\n```\n\nAfterwards, verify that the AWS CLI package works correctly:\n\n```console\n$ ./miniconda/bin/aws --version\naws-cli/1.19.79 Python/3.8.5 Linux/4.14.231-173.361.amzn2.x86_64 botocore/1.20.79\n```\n\n:::{note}\nThe `aws` tool will be placed in a directory named `bin` in the main installation folder. Modifying this directory structure after the tool is installed will cause it to not work properly.\n:::\n\nTo configure Nextflow to use this installation, specify the `aws.batch.cliPath` option in the Nextflow configuration as shown below:\n\n```groovy\naws.batch.cliPath = '/home/ec2-user/miniconda/bin/aws'\n```\n\nReplace the path above with the one matching the location where the `aws` tool is installed in your AMI.\n\n:::{versionchanged} 19.07.0\nThe `executor.awscli` config option was replaced by `aws.batch.cliPath`.\n:::\n\n:::{warning}\nThe grandparent directory of the `aws` tool will be mounted into the container at the same path as the host, e.g. `/home/ec2-user/miniconda`, which will shadow existing files in the container. Make sure you use a path that is not already present in the container.\n:::\n\n### Docker installation\n\nDocker is required by Nextflow to execute tasks on AWS Batch. The **Amazon ECS-Optimized Amazon Linux 2** AMI has Docker installed, however, if you create your AMI from a different AMI that does not have Docker installed, you will need to install it manually.\n\nThe following snippet shows how to install Docker on an Amazon EC2 instance:\n\n```bash\n# install Docker\nsudo yum update -y\nsudo amazon-linux-extras install", "start_char_idx": 11589, "end_char_idx": 14920, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "ec2dc4a0-8008-4498-8774-2afa4893a22e": {"__data__": {"id_": "ec2dc4a0-8008-4498-8774-2afa4893a22e", "embedding": null, "metadata": {"file_path": "docs/aws.md", "file_name": "aws.md"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "edc18535956f4832534c0eb7f12118f393382241", "node_type": null, "metadata": {"file_path": "docs/aws.md", "file_name": "aws.md"}, "hash": "154c98679f76c18bfd99631f49eb8e7e0ac30f7028a82c41867f554dd506983a"}, "2": {"node_id": "ef3ef38f-542a-401b-aa20-fbc728ae3b62", "node_type": null, "metadata": {"file_path": "docs/aws.md", "file_name": "aws.md"}, "hash": "22a37c5d91883a1ac7e4c84603432c077c987e986c0c27ad52e184f7694759cc"}, "3": {"node_id": "9d86b46f-6ff1-41d4-a772-df4fe3b270c6", "node_type": null, "metadata": {"file_path": "docs/aws.md", "file_name": "aws.md"}, "hash": "8194b55eb12b0850b55c1872f9f9993e9c1a4ec019fd1fb728616e2c563ba253"}}, "hash": "29b47e9ff48bcd7b3792a02ba465a2d385a21c21a84b0d681f96722ae93f6461", "text": "install Docker\nsudo yum update -y\nsudo amazon-linux-extras install docker\nsudo yum install docker\n\n# start the Docker service\nsudo service docker start\n\n# empower your user to run Docker without sudo\nsudo usermod -a -G docker ec2-user\n```\n\nYou may have to reboot your instance for the changes to `ec2-user` to take effect.\n\nThese steps must be done *before* creating the AMI from the current EC2 instance.\n\n### Amazon ECS container agent installation\n\nThe [ECS container agent](https://docs.aws.amazon.com/AmazonECS/latest/developerguide/ECS_agent.html) is a component of Amazon Elastic Container Service (Amazon ECS) and is responsible for managing containers on behalf of ECS. AWS Batch uses ECS to execute containerized jobs, therefore it requires the agent to be installed on EC2 instances within your Compute Environments.\n\nThe ECS agent is included in the **Amazon ECS-Optimized Amazon Linux 2** AMI. If you use a different AMI, you can also install the agent on any EC2 instance that supports the Amazon ECS specification.\n\nTo install the agent, follow these steps:\n\n```bash\nsudo amazon-linux-extras disable docker\nsudo amazon-linux-extras install -y ecs\nsudo systemctl enable --now ecs\n```\n\nTo test the installation:\n\n```bash\ncurl -s http://localhost:51678/v1/metadata | python -mjson.tool (test)\n```\n\n:::{note}\nThe `AmazonEC2ContainerServiceforEC2Role` policy must be attached to the instance role in order to be able to connect the EC2 instance created by the Compute Environment to the ECS container.\n:::\n\n## Jobs & Execution\n\n### Custom job definition\n\nNextflow automatically creates the Batch [Job definitions](http://docs.aws.amazon.com/batch/latest/userguide/job_definitions.html) needed to execute tasks in your pipeline, so you don't need to define them beforehand.\n\nHowever, sometimes you may still need to specify a custom **Job Definition** to fine tune the configuration of a specific job, for example to define custom mount paths.\n\nTo do that, first create a **Job Definition** in the AWS Console (or by other means). Note the name of the Job definition you created. You can then associate a process execution with this Job definition by using the {ref}`process-container` directive and specifying, in place of the container image name, the Job definition name prefixed by `job-definition://`, as shown below:\n\n```groovy\nprocess.container = 'job-definition://your-job-definition-name'\n```\n\n### Pipeline execution\n\nThe pipeline can be launched either in a local computer or an EC2 instance. The latter is suggested for heavy or long-running workloads.\n\nPipeline input data can be stored either locally or in an [S3](https://aws.amazon.com/s3/) bucket. The pipeline execution must specify an S3 bucket to store intermediate results with the `-bucket-dir` (`-b`) command line option. For example:\n\n```bash\nnextflow run my-pipeline -bucket-dir s3://my-bucket/some/path\n```\n\n:::{warning}\nThe bucket path should include at least a top level directory name, e.g. `s3://my-bucket/work` rather than `s3://my-bucket`.\n:::\n\n### Hybrid workloads\n\nNextflow allows the use of multiple executors in the same workflow application. This feature enables the deployment of hybrid workloads in which some jobs are executed in the local computer or local computing cluster and some jobs are offloaded to AWS Batch.\n\nTo enable this feature, use one or more {ref}`config-process-selectors` in your Nextflow configuration to apply the AWS Batch {ref}`configuration <aws-batch-config>` to the subset of processes that you want to offload. For example:\n\n```groovy\naws {\n ", "start_char_idx": 14935, "end_char_idx": 18502, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "9d86b46f-6ff1-41d4-a772-df4fe3b270c6": {"__data__": {"id_": "9d86b46f-6ff1-41d4-a772-df4fe3b270c6", "embedding": null, "metadata": {"file_path": "docs/aws.md", "file_name": "aws.md"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "edc18535956f4832534c0eb7f12118f393382241", "node_type": null, "metadata": {"file_path": "docs/aws.md", "file_name": "aws.md"}, "hash": "154c98679f76c18bfd99631f49eb8e7e0ac30f7028a82c41867f554dd506983a"}, "2": {"node_id": "ec2dc4a0-8008-4498-8774-2afa4893a22e", "node_type": null, "metadata": {"file_path": "docs/aws.md", "file_name": "aws.md"}, "hash": "29b47e9ff48bcd7b3792a02ba465a2d385a21c21a84b0d681f96722ae93f6461"}, "3": {"node_id": "7a52b72f-a01d-4b63-943f-c796a7798925", "node_type": null, "metadata": {"file_path": "docs/aws.md", "file_name": "aws.md"}, "hash": "71a6c6e85186cbfaba68af70d7360b30d934453517f3598d557ec7401e9ea0fd"}}, "hash": "8194b55eb12b0850b55c1872f9f9993e9c1a4ec019fd1fb728616e2c563ba253", "text": "you want to offload. For example:\n\n```groovy\naws {\n    region = 'eu-west-1'\n    batch {\n        cliPath = '/home/ec2-user/miniconda/bin/aws'\n    }\n}\n\nprocess {\n    withLabel: bigTask {\n        executor = 'awsbatch'\n        queue = 'my-batch-queue'\n        container = 'my/image:tag'\n    }\n}\n```\n\nWith the above configuration, processes with the `bigTask` {ref}`process-label` will run on AWS Batch, while the remaining processes with run in the local computer.\n\n### Volume mounts\n\n:::{versionadded} 19.07.0\n:::\n\nUser provided container volume mounts can be provided as shown below:\n\n```groovy\naws {\n    region = 'eu-west-1'\n    batch {\n        volumes = '/tmp'\n    }\n}\n```\n\nMultiple volumes can be specified as a comma-separated list of paths. The usual Docker volume mount syntax can be used to specify complex volumes where the container path is different from the host path or the volume should be *read-only*. For example:\n\n```groovy\naws {\n    region = 'eu-west-1'\n    batch {\n        volumes = ['/tmp', '/host/path:/mnt/path:ro']\n    }\n}\n```\n\nThe above snippet defines two volume mounts for the jobs executed in your pipeline. The first volume mounts the host path `/tmp` to the same path in the container, with the *read-write* access mode. The second volume mounts the host path `/host/path` to `/mnt/path` in the container, with the *read-only* access mode.\n\n### Troubleshooting\n\n**Problem**: The Pipeline execution terminates with an AWS error message similar to the one shown below:\n\n```\nJobQueue <your queue> not found\n```\n\nMake sure you have defined a AWS region in the Nextflow configuration file and it matches the region in which your Batch environment has been created.\n\n**Problem**: A process execution fails reporting the following error message:\n\n```\nProcess <your task> terminated for an unknown reason -- Likely it has been terminated by the external system\n```\n\nThis may happen when Batch is unable to execute the process script. A common cause of this problem is that the Docker container image you have specified uses a non standard [entrypoint](https://docs.docker.com/engine/reference/builder/#entrypoint) which does not allow the execution of the Bash launcher script required by Nextflow to run the job.\n\nThis may also happen if the AWS CLI doesn't run correctly.\n\nOther places to check for error information:\n\n- The `.nextflow.log` file.\n- The Job execution log in the AWS Batch dashboard.\n- The [CloudWatch](https://aws.amazon.com/cloudwatch/) logs found in the `/aws/batch/job` log group.\n\n**Problem**: A process execution is stalled in the `RUNNABLE` status and the pipeline output is similar to the one below:\n\n```\nexecutor >  awsbatch (1)\nprocess > <your process> (1) [  0%] 0 of ....\n```\n\nIt may happen that the pipeline execution hangs indefinitely because one of the jobs is held in the queue and never gets executed. In AWS Console, the queue reports the job as `RUNNABLE` but it never moves from there.\n\nThere are multiple reasons why this can happen. They are mainly related to the Compute Environment workload/configuration, the docker service or container configuration, network status, etc.\n\nThis [AWS", "start_char_idx": 18518, "end_char_idx": 21662, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "7a52b72f-a01d-4b63-943f-c796a7798925": {"__data__": {"id_": "7a52b72f-a01d-4b63-943f-c796a7798925", "embedding": null, "metadata": {"file_path": "docs/aws.md", "file_name": "aws.md"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "edc18535956f4832534c0eb7f12118f393382241", "node_type": null, "metadata": {"file_path": "docs/aws.md", "file_name": "aws.md"}, "hash": "154c98679f76c18bfd99631f49eb8e7e0ac30f7028a82c41867f554dd506983a"}, "2": {"node_id": "9d86b46f-6ff1-41d4-a772-df4fe3b270c6", "node_type": null, "metadata": {"file_path": "docs/aws.md", "file_name": "aws.md"}, "hash": "8194b55eb12b0850b55c1872f9f9993e9c1a4ec019fd1fb728616e2c563ba253"}}, "hash": "71a6c6e85186cbfaba68af70d7360b30d934453517f3598d557ec7401e9ea0fd", "text": "the docker service or container configuration, network status, etc.\n\nThis [AWS page](https://aws.amazon.com/premiumsupport/knowledge-center/batch-job-stuck-runnable-status/) provides several resolutions and tips to investigate and work around the issue.\n\n## Advanced configuration\n\nRead the {ref}`AWS configuration<config-aws>` section to learn more about advanced configuration options.", "start_char_idx": 21629, "end_char_idx": 22016, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "e364f60e-08ed-418e-921c-ed51d94b3b24": {"__data__": {"id_": "e364f60e-08ed-418e-921c-ed51d94b3b24", "embedding": null, "metadata": {"file_path": "docs/azure.md", "file_name": "azure.md"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "aebf7bb9c1405f04479ebe20a4c05eecada7e126", "node_type": null, "metadata": {"file_path": "docs/azure.md", "file_name": "azure.md"}, "hash": "7b45648c4ea5cfddf62695813d91aa5975740f87c4b4e097f9849b14f9f167aa"}, "3": {"node_id": "a764c2d7-5329-45ca-92ba-1ba9e99490c7", "node_type": null, "metadata": {"file_path": "docs/azure.md", "file_name": "azure.md"}, "hash": "3f3201040277b2171d1e541f2030b885023b6ca950eb1b732e3b807454f4ffaf"}}, "hash": "99e9e9b53c60ef38b893499451d1b3f0efe999ef13dd229f42c6cee4e35b39a9", "text": "(azure-page)=\n\n# Azure Cloud\n\n:::{versionadded} 21.04.0\n:::\n\n(azure-blobstorage)=\n\n## Azure Blob Storage\n\nNextflow has built-in support for [Azure Blob Storage](https://azure.microsoft.com/en-us/services/storage/blobs/). Files stored in an Azure blob container can be accessed transparently in your pipeline script like any other file in the local file system.\n\nThe Blob storage account name and key need to be provided in the Nextflow configuration file as shown below:\n\n```groovy\nazure {\n    storage {\n        accountName = \"<YOUR BLOB ACCOUNT NAME>\"\n        accountKey = \"<YOUR BLOB ACCOUNT KEY>\"\n    }\n}\n```\n\nAlternatively, the **Shared Access Token** can be specified with the `sasToken` option instead of `accountKey`.\n\n:::{tip}\nWhen creating the Shared Access Token, make sure to allow the resource types `Container` and `Object` and allow the permissions: `Read`, `Write`, `Delete`, `List`, `Add`, `Create`.\n:::\n\n:::{tip}\nThe value of `sasToken` is the token stripped by the character `?` from the beginning of the token.\n:::\n\nOnce the Blob Storage credentials are set, you can access the files in the blob container like local files by prepending the file path with `az://` followed by the container name. For example, a blob container named `my-data` with a file named `foo.txt` can be specified in your Nextflow script as `az://my-data/foo.txt`.\n\n## Azure File Shares\n\n*New in `nf-azure` version `0.11.0`*\n\nNextflow has built-in support also for [Azure Files](https://azure.microsoft.com/en-us/services/storage/files/). Files available in the serverless Azure File shares can be mounted concurrently on the nodes of a pool executing the pipeline. These files become immediately available in the file system and can be referred as local files within the processes. This is especially useful when a task needs to access large amounts of data (such as genome indexes) during its execution. An arbitrary number of File shares can be mounted on each pool node.\n\nThe Azure File share must exist in the storage account configured for Blob Storage. The name of the source Azure File share and mount path (the destination path where the files are mounted) must be provided. Additional mount options (see the Azure Files documentation) can be set as well for further customisation of the mounting process.\n\nFor example:\n\n```groovy\nazure {\n    storage {\n        accountName = \"<YOUR BLOB ACCOUNT NAME>\"\n        accountKey = \"<YOUR BLOB ACCOUNT KEY>\"\n        fileShares {\n            <YOUR SOURCE FILE SHARE NAME> {\n                mountPath = \"<YOUR MOUNT DESTINATION>\"\n                mountOptions = \"<SAME AS MOUNT COMMAND>\" //optional\n            }\n            <YOUR SOURCE FILE SHARE NAME> {\n                mountPath = \"<YOUR MOUNT DESTINATION>\"\n                mountOptions = \"<SAME AS MOUNT COMMAND>\" //optional\n            }\n        }\n    }\n}\n```\n\nThe files in the File share are available to the task in the directory: `<YOUR MOUNT DESTINATION>/<YOUR SOURCE FILE SHARE NAME>`.\n\nFor instance, given the following configuration:\n\n```groovy\nazure {\n ", "start_char_idx": 0, "end_char_idx": 3056, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "a764c2d7-5329-45ca-92ba-1ba9e99490c7": {"__data__": {"id_": "a764c2d7-5329-45ca-92ba-1ba9e99490c7", "embedding": null, "metadata": {"file_path": "docs/azure.md", "file_name": "azure.md"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "aebf7bb9c1405f04479ebe20a4c05eecada7e126", "node_type": null, "metadata": {"file_path": "docs/azure.md", "file_name": "azure.md"}, "hash": "7b45648c4ea5cfddf62695813d91aa5975740f87c4b4e097f9849b14f9f167aa"}, "2": {"node_id": "e364f60e-08ed-418e-921c-ed51d94b3b24", "node_type": null, "metadata": {"file_path": "docs/azure.md", "file_name": "azure.md"}, "hash": "99e9e9b53c60ef38b893499451d1b3f0efe999ef13dd229f42c6cee4e35b39a9"}, "3": {"node_id": "81ecf67e-0411-4eba-8cab-8c6732b5d388", "node_type": null, "metadata": {"file_path": "docs/azure.md", "file_name": "azure.md"}, "hash": "8fbcbb8180f0252cc4d28b794080393080a5e78ccb5f085c2b71187d40deb1c6"}}, "hash": "3f3201040277b2171d1e541f2030b885023b6ca950eb1b732e3b807454f4ffaf", "text": "given the following configuration:\n\n```groovy\nazure {\n    storage {\n        // ...\n\n        fileShares {\n            dir1 {\n                mountPath = \"/mnt/mydata/\"\n            }\n        }\n    }\n}\n```\n\nThe task can access the File share in `/mnt/mydata/dir1`.\n\n(azure-batch)=\n\n## Azure Batch\n\n[Azure Batch](https://docs.microsoft.com/en-us/azure/batch/) is a managed computing service that allows the execution of containerised workloads in the Azure cloud infrastructure.\n\nNextflow provides built-in support for Azure Batch, allowing the seamless deployment of Nextflow pipelines in the cloud, in which tasks are offloaded as Batch jobs.\n\nRead the {ref}`Azore Batch executor <azurebatch-executor>` section to learn more about the `azurebatch` executor in Nextflow.\n\n### Get started\n\n1. Create a Batch account in the Azure portal. Take note of the account name and key.\n2. Make sure to adjust your quotas to the pipeline's needs. There are limits on certain resources associated with the Batch account. Many of these limits are default quotas applied by Azure at the subscription or account level. Quotas impact the number of Pools, CPUs and Jobs you can create at any given time.\n3. Create a Storage account and, within that, an Azure Blob Container in the same location where the Batch account was created. Take note of the account name and key.\n4. If you plan to use Azure Files, create an Azure File share within the same Storage account and upload your input data.\n5. Associate the Storage account with the Azure Batch account.\n6. Make sure every process in your pipeline specifies one or more Docker containers with the {ref}`process-container` directive.\n7. Make sure all of your container images are published in a Docker registry that can be accessed by your Azure Batch environment, such as [Docker Hub](https://hub.docker.com/), [Quay](https://quay.io/), or [Azure Container Registry](https://docs.microsoft.com/en-us/azure/container-registry/) .\n\nA minimal Nextflow configuration for Azure Batch looks like the following snippet:\n\n```groovy\nprocess {\n    executor = 'azurebatch'\n}\n\nazure {\n    storage {\n        accountName = \"<YOUR STORAGE ACCOUNT NAME>\"\n        accountKey = \"<YOUR STORAGE ACCOUNT KEY>\"\n    }\n    batch {\n        location = '<YOUR LOCATION>'\n        accountName = '<YOUR BATCH ACCOUNT NAME>'\n        accountKey = '<YOUR BATCH ACCOUNT KEY>'\n        autoPoolMode = true\n    }\n}\n```\n\nIn the above example, replace the location placeholder with the name of your Azure region and the account placeholders with the values corresponding to your configuration.\n\n:::{tip}\nThe list of Azure regions can be found by executing the following Azure CLI command:\n\n```bash\naz account list-locations -o table\n```\n:::\n\nFinally, launch your pipeline with the above configuration:\n\n```bash\nnextflow run <PIPELINE NAME> -w az://YOUR-CONTAINER/work\n```\n\nReplacing `<PIPELINE NAME>` with a pipeline name e.g. `nextflow-io/rnaseq-nf` and `YOUR-CONTAINER` with a blob container in the storage account defined in the above configuration.\n\nSee the [Batch", "start_char_idx": 3007, "end_char_idx": 6067, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "81ecf67e-0411-4eba-8cab-8c6732b5d388": {"__data__": {"id_": "81ecf67e-0411-4eba-8cab-8c6732b5d388", "embedding": null, "metadata": {"file_path": "docs/azure.md", "file_name": "azure.md"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "aebf7bb9c1405f04479ebe20a4c05eecada7e126", "node_type": null, "metadata": {"file_path": "docs/azure.md", "file_name": "azure.md"}, "hash": "7b45648c4ea5cfddf62695813d91aa5975740f87c4b4e097f9849b14f9f167aa"}, "2": {"node_id": "a764c2d7-5329-45ca-92ba-1ba9e99490c7", "node_type": null, "metadata": {"file_path": "docs/azure.md", "file_name": "azure.md"}, "hash": "3f3201040277b2171d1e541f2030b885023b6ca950eb1b732e3b807454f4ffaf"}, "3": {"node_id": "36792f6e-b573-4812-aeb9-341790196e85", "node_type": null, "metadata": {"file_path": "docs/azure.md", "file_name": "azure.md"}, "hash": "1d822463bbb06788d51d312f36ed62c6558b037e0ab121dd754a4f4b2ed7e6ff"}}, "hash": "8fbcbb8180f0252cc4d28b794080393080a5e78ccb5f085c2b71187d40deb1c6", "text": "container in the storage account defined in the above configuration.\n\nSee the [Batch documentation](https://docs.microsoft.com/en-us/azure/batch/quick-create-portal) for further details about the configuration for Azure Batch.\n\n### Pools configuration\n\nWhen using the `autoPoolMode` option, Nextflow automatically creates a `pool` of compute nodes to execute the jobs in your pipeline. By default, it only uses one compute node of the type `Standard_D4_v3`.\n\nThe pool is not removed when the pipeline terminates, unless the configuration setting `deletePoolsOnCompletion = true` is added in your Nextflow configuration file.\n\nPool specific settings, such as VM type and count, should be provided in the `auto` pool configuration scope, for example:\n\n```groovy\nazure {\n    batch {\n        pools {\n            auto {\n                vmType = 'Standard_D2_v2'\n                vmCount = 10\n            }\n        }\n    }\n}\n```\n\n:::{warning}\nTo avoid any extra charges in the Batch account, remember to clean up the Batch pools or use auto scaling.\n:::\n\n:::{warning}\nMake sure your Batch account has enough resources to satisfy the pipeline's requirements and the pool configuration.\n:::\n\n:::{warning}\nNextflow uses the same pool ID across pipeline executions, if the pool features have not changed. Therefore, when using `deletePoolsOnCompletion = true`, make sure the pool is completely removed from the Azure Batch account before re-running the pipeline. The following message is returned when the pool is still shutting down:\n\n```\nError executing process > '<process name> (1)'\nCaused by:\n    Azure Batch pool '<pool name>' not in active state\n```\n:::\n\n### Named pools\n\nIf you want to have more precise control over the compute node pools used in your pipeline, such as using a different pool depending on the task in your pipeline, you can use the {ref}`process-queue` directive in Nextflow to specify the ID of a Azure Batch compute pool that should be used to execute that process.\n\nThe pool is expected to be already available in the Batch environment, unless the setting `allowPoolCreation = true` is provided in the `azure.batch` config scope in the pipeline configuration file. In the latter case, Nextflow will create the pools on-demand.\n\nThe configuration details for each pool can be specified using a snippet as shown below:\n\n```groovy\nazure {\n    batch {\n        pools {\n            foo {\n                vmType = 'Standard_D2_v2'\n                vmCount = 10\n            }\n\n            bar {\n                vmType = 'Standard_E2_v3'\n                vmCount = 5\n            }\n        }\n    }\n}\n```\n\nThe above example defines the configuration for two node pools. The first will provision 10 compute nodes of type `Standard_D2_v2`, the second 5 nodes of type `Standard_E2_v3`. See the {ref}`Azure configuration <config-azure>` section for the complete list of available configuration options.\n\n:::{warning}\nThe pool name can only contain alphanumeric, hyphen and underscore", "start_char_idx": 6045, "end_char_idx": 9029, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "36792f6e-b573-4812-aeb9-341790196e85": {"__data__": {"id_": "36792f6e-b573-4812-aeb9-341790196e85", "embedding": null, "metadata": {"file_path": "docs/azure.md", "file_name": "azure.md"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "aebf7bb9c1405f04479ebe20a4c05eecada7e126", "node_type": null, "metadata": {"file_path": "docs/azure.md", "file_name": "azure.md"}, "hash": "7b45648c4ea5cfddf62695813d91aa5975740f87c4b4e097f9849b14f9f167aa"}, "2": {"node_id": "81ecf67e-0411-4eba-8cab-8c6732b5d388", "node_type": null, "metadata": {"file_path": "docs/azure.md", "file_name": "azure.md"}, "hash": "8fbcbb8180f0252cc4d28b794080393080a5e78ccb5f085c2b71187d40deb1c6"}, "3": {"node_id": "ab3cd4e3-2b68-4e33-9bd2-8075e6b99f51", "node_type": null, "metadata": {"file_path": "docs/azure.md", "file_name": "azure.md"}, "hash": "3bbef188dba29f8b8ca938fad5c3d57c8ae6ce1ab9f2253c0b5d27c66c1a0997"}}, "hash": "1d822463bbb06788d51d312f36ed62c6558b037e0ab121dd754a4f4b2ed7e6ff", "text": "pool name can only contain alphanumeric, hyphen and underscore characters.\n:::\n\n:::{warning}\nIf the pool name includes a hyphen, make sure to wrap it with single quotes. For example::\n\n```groovy\nazure {\n    batch {\n        pools {\n            'foo-2' {\n                ...\n            }\n        }\n    }\n}\n```\n:::\n\n### Requirements on pre-existing named pools\n\nWhen Nextflow is configured to use a pool already available in the Batch account, the target pool must satisfy the following requirements:\n\n1. The pool must be declared as `dockerCompatible` (`Container Type` property).\n2. The task slots per node must match the number of cores for the selected VM. Otherwise, Nextflow will return an error like \"Azure Batch pool 'ID' slots per node does not match the VM num cores (slots: N, cores: Y)\".\n\n### Pool autoscaling\n\nAzure Batch can automatically scale pools based on parameters that you define, saving you time and money. With automatic scaling, Batch dynamically adds nodes to a pool as task demands increase, and removes compute nodes as task demands decrease.\n\nTo enable this feature for pools created by Nextflow, add the option `autoScale = true` to the corresponding pool configuration scope. For example, when using the `autoPoolMode`, the setting looks like:\n\n```groovy\nazure {\n    batch {\n        pools {\n            auto {\n                autoScale = true\n                vmType = 'Standard_D2_v2'\n                vmCount = 5\n                maxVmCount = 50\n            }\n        }\n    }\n}\n```\n\nNextflow uses the formula shown below to determine the number of VMs to be provisioned in the pool:\n\n```\n// Get pool lifetime since creation.\nlifespan = time() - time(\"{{poolCreationTime}}\");\ninterval = TimeInterval_Minute * {{scaleInterval}};\n\n// Compute the target nodes based on pending tasks.\n// $PendingTasks == The sum of $ActiveTasks and $RunningTasks\n$samples = $PendingTasks.GetSamplePercent(interval);\n$tasks = $samples < 70 ? max(0, $PendingTasks.GetSample(1)) : max( $PendingTasks.GetSample(1), avg($PendingTasks.GetSample(interval)));\n$targetVMs = $tasks > 0 ? $tasks : max(0, $TargetDedicatedNodes/2);\ntargetPoolSize = max(0, min($targetVMs, {{maxVmCount}}));\n\n// For first interval deploy 1 node, for other intervals scale up/down as per tasks.\n$TargetDedicatedNodes = lifespan < interval ? {{vmCount}} : targetPoolSize;\n$NodeDeallocationOption = taskcompletion;\n```\n\nThe above formula initialises a pool with the number of VMs specified by the `vmCount` option, and scales up the pool on-demand, based on the number of pending tasks, up to `maxVmCount` nodes. If no jobs are submitted for execution, it scales down to zero nodes automatically.\n\nIf you need a different strategy, you can provide your own formula using the `scaleFormula` option. See the [Azure Batch](https://docs.microsoft.com/en-us/azure/batch/batch-automatic-scaling)", "start_char_idx": 9049, "end_char_idx": 11911, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "ab3cd4e3-2b68-4e33-9bd2-8075e6b99f51": {"__data__": {"id_": "ab3cd4e3-2b68-4e33-9bd2-8075e6b99f51", "embedding": null, "metadata": {"file_path": "docs/azure.md", "file_name": "azure.md"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "aebf7bb9c1405f04479ebe20a4c05eecada7e126", "node_type": null, "metadata": {"file_path": "docs/azure.md", "file_name": "azure.md"}, "hash": "7b45648c4ea5cfddf62695813d91aa5975740f87c4b4e097f9849b14f9f167aa"}, "2": {"node_id": "36792f6e-b573-4812-aeb9-341790196e85", "node_type": null, "metadata": {"file_path": "docs/azure.md", "file_name": "azure.md"}, "hash": "1d822463bbb06788d51d312f36ed62c6558b037e0ab121dd754a4f4b2ed7e6ff"}, "3": {"node_id": "68f5f974-7162-47a5-b198-ba401306bee4", "node_type": null, "metadata": {"file_path": "docs/azure.md", "file_name": "azure.md"}, "hash": "6de7831a68901e0a5580b6c605ff1b2c20c8d1beaab2fc6be84818093f3f383d"}}, "hash": "3bbef188dba29f8b8ca938fad5c3d57c8ae6ce1ab9f2253c0b5d27c66c1a0997", "text": "documentation for details.\n\n### Pool nodes\n\nWhen Nextflow creates a pool of compute nodes, it selects:\n\n- the virtual machine image reference to be installed on the node\n- the Batch node agent SKU, a program that runs on each node and provides an interface between the node and the Batch service\n\nTogether, these settings determine the Operating System and version installed on each node.\n\nBy default, Nextflow creates pool nodes based on CentOS 8, but this behavior can be customised in the pool configuration. Below are configurations for image reference/SKU combinations to select two popular systems.\n\n- Ubuntu 20.04 (default):\n\n  ```groovy\n  azure.batch.pools.<name>.sku = \"batch.node.ubuntu 20.04\"\n  azure.batch.pools.<name>.offer = \"ubuntu-server-container\"\n  azure.batch.pools.<name>.publisher = \"microsoft-azure-batch\"\n  ```\n\n- CentOS 8:\n\n  ```groovy\n  azure.batch.pools.<name>.sku = \"batch.node.centos 8\"\n  azure.batch.pools.<name>.offer = \"centos-container\"\n  azure.batch.pools.<name>.publisher = \"microsoft-azure-batch\"\n  ```\n\nIn the above snippet, replace `<name>` with the name of your Azure node pool.\n\nSee the {ref}`Azure configuration <config-azure>` section and the [Azure Batch nodes](https://docs.microsoft.com/en-us/azure/batch/batch-linux-nodes) documentation for more details.\n\n### Private container registry\n\n:::{versionadded} 21.05.0-edge\n:::\n\nA private container registry for Docker images can be specified as follows:\n\n```groovy\nazure {\n    registry {\n        server = '<YOUR REGISTRY SERVER>' // e.g.: docker.io, quay.io, <ACCOUNT>.azurecr.io, etc.\n        userName = '<YOUR REGISTRY USER NAME>'\n        password = '<YOUR REGISTRY PASSWORD>'\n    }\n}\n```\n\nThe private registry is an addition, not a replacement, to the existing configuration. Public images from other registries will still be pulled as normal, if they are requested.\n\n:::{note}\nWhen using containers hosted in a private registry, the registry name must also be provided in the container name specified via the {ref}`container <process-container>` directive using the format: `[server]/[your-organization]/[your-image]:[tag]`. Read more about fully qualified image names in the [Docker documentation](https://docs.docker.com/engine/reference/commandline/pull/#pull-from-a-different-registry).\n:::\n\n### Virtual Network\n\n:::{versionadded} 23.03.0-edge\n:::\n\nSometimes it might be useful to create a pool in an existing [Virtual Network](https://learn.microsoft.com/en-us/azure/virtual-network/). To do so, the \n`virtualNetwork` option can be added to the pool settings as follows:\n\n```groovy\nazure {\n    batch {\n        pools {\n            auto {\n                autoScale = true\n                vmType = 'Standard_D2_v2'\n                vmCount = 5\n                virtualNetwork = '<YOUR SUBNET ID>'\n            }\n        }\n    }\n}\n```\n\nThe value of the setting must be the identifier of a subnet available in the virtual network to join. A valid", "start_char_idx": 11966, "end_char_idx": 14904, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "68f5f974-7162-47a5-b198-ba401306bee4": {"__data__": {"id_": "68f5f974-7162-47a5-b198-ba401306bee4", "embedding": null, "metadata": {"file_path": "docs/azure.md", "file_name": "azure.md"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "aebf7bb9c1405f04479ebe20a4c05eecada7e126", "node_type": null, "metadata": {"file_path": "docs/azure.md", "file_name": "azure.md"}, "hash": "7b45648c4ea5cfddf62695813d91aa5975740f87c4b4e097f9849b14f9f167aa"}, "2": {"node_id": "ab3cd4e3-2b68-4e33-9bd2-8075e6b99f51", "node_type": null, "metadata": {"file_path": "docs/azure.md", "file_name": "azure.md"}, "hash": "3bbef188dba29f8b8ca938fad5c3d57c8ae6ce1ab9f2253c0b5d27c66c1a0997"}}, "hash": "6de7831a68901e0a5580b6c605ff1b2c20c8d1beaab2fc6be84818093f3f383d", "text": "setting must be the identifier of a subnet available in the virtual network to join. A valid subnet ID has the following form:\n\n```\n/subscriptions/<YOUR SUBSCRIPTION ID>/resourceGroups/<YOUR RESOURCE GROUP NAME>/providers/Microsoft.Network/virtualNetworks/<YOUR VIRTUAL NETWORK NAME>/subnets/<YOUR SUBNET NAME>\n```\n\n:::{warning}\nBatch Authentication with Shared Keys does not allow to link external resources (like Virtual Networks) to the pool. Therefore, Active Directory Authentication must be used in conjunction with the `virtualNetwork` setting.\n:::\n\n## Active Directory Authentication\n\n:::{versionadded} 22.11.0-edge\n:::\n\n[Service Principal](https://learn.microsoft.com/en-us/azure/active-directory/develop/howto-create-service-principal-portal) credentials can optionally be used instead of Shared Keys for Azure Batch and Storage accounts.\n\nThe Service Principal should have the at least the following role assignments:\n\n1. Contributor\n2. Storage Blob Data Reader\n3. Storage Blob Data Contributor\n\n:::{note}\nTo assign the necessary roles to the Service Principal, refer to the [official Azure documentation](https://learn.microsoft.com/en-us/azure/role-based-access-control/role-assignments-portal?tabs=current).\n:::\n\nThe credentials for Service Principal can be specified as follows:\n\n```groovy\nazure {\n    activeDirectory {\n        servicePrincipalId = '<YOUR SERVICE PRINCIPAL CLIENT ID>'\n        servicePrincipalSecret = '<YOUR SERVICE PRINCIPAL CLIENT SECRET>'\n        tenantId = '<YOUR TENANT ID>'\n    }\n\n    storage {\n        accountName = '<YOUR STORAGE ACCOUNT NAME>'\n    }\n\n    batch {\n        accountName = '<YOUR BATCH ACCOUNT NAME>'\n        location = '<YOUR BATCH ACCOUNT LOCATION>'\n    }\n}\n```\n\n## Advanced configuration\n\nRead the {ref}`Azure configuration<config-azure>` section to learn more about advanced configuration options.", "start_char_idx": 14812, "end_char_idx": 16667, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "865f459a-485e-43fb-ad14-e3b09686f266": {"__data__": {"id_": "865f459a-485e-43fb-ad14-e3b09686f266", "embedding": null, "metadata": {"file_path": "docs/basic.md", "file_name": "basic.md"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "6f7b64b74e52d83f0c64de07cde0e3d49b26acaa", "node_type": null, "metadata": {"file_path": "docs/basic.md", "file_name": "basic.md"}, "hash": "4dca7461ee31ed8fe0ae058c9f84172f2d14d68bce3ed05ac5467333ba27523e"}, "3": {"node_id": "32e4722a-009e-4f49-86d1-f4e68f38a76c", "node_type": null, "metadata": {"file_path": "docs/basic.md", "file_name": "basic.md"}, "hash": "4c6d742a1786be7a0f8f9030cb8be94be1de4f855541abdc7d96212ec1d83563"}}, "hash": "f7d405118804904311020fdce589f90996d65b62d602cc8f1299eae701c564ef", "text": "# Basic concepts\n\nNextflow is a reactive workflow framework and a programming [DSL](http://en.wikipedia.org/wiki/Domain-specific_language) that eases the writing of data-intensive computational pipelines.\n\nIt is designed around the idea that the Linux platform is the lingua franca of data science. Linux provides many simple but powerful command-line and scripting tools that, when chained together, facilitate complex data manipulations.\n\nNextflow extends this approach, adding the ability to define complex program interactions and a high-level parallel computational environment based on the *dataflow* programming model.\n\n## Processes and channels\n\nIn practice a Nextflow pipeline script is made by joining together different processes. Each process can be written in any scripting language that can be executed by the Linux platform (Bash, Perl, Ruby, Python, etc.).\n\nProcesses are executed independently and are isolated from each other, i.e. they do not share a common (writable) state. The only way they can communicate is via asynchronous FIFO queues, called *channels* in Nextflow.\n\nAny process can define one or more channels as *input* and *output*. The interaction between these processes, and ultimately the pipeline execution flow itself, is implicitly defined by these input and output declarations.\n\nA Nextflow script looks like this:\n\n```groovy\n// Declare syntax version\nnextflow.enable.dsl=2\n\n// Script parameters\nparams.query = \"/some/data/sample.fa\"\nparams.db = \"/some/path/pdb\"\n\nprocess blastSearch {\n  input:\n    path query\n    path db\n  output:\n    path \"top_hits.txt\"\n\n    \"\"\"\n    blastp -db $db -query $query -outfmt 6 > blast_result\n    cat blast_result | head -n 10 | cut -f 2 > top_hits.txt\n    \"\"\"\n}\n\nprocess extractTopHits {\n  input:\n    path top_hits\n\n  output:\n    path \"sequences.txt\"\n\n    \"\"\"\n    blastdbcmd -db $db -entry_batch $top_hits > sequences.txt\n    \"\"\"\n}\n\nworkflow {\n  def query_ch = Channel.fromPath(params.query)\n  blastSearch(query_ch, params.db) | extractTopHits | view\n}\n```\n\nThe above example defines two processes. Their execution order is not determined by the fact that the `blastSearch` process comes before `extractTopHits` in the script (it could also be written the other way around). Instead, the pipe operator (`|`) in the workflow between `blastSearch` and `extractTopHits` forwards the outputs from one process to the inputs of the following one.\n\nWhen the workflow is started, it will create two processes and one channel (`query_ch`) and it will link all of them. Both processes will be started at the same time and they will listen to their respective input channels. Whenever `blastSearch` emits a value, `extractTopHits` will receive it (i.e. `extractTopHits` consumes the channel in a *reactive* way).\n\nRead the {ref}`Channel <channel-page>` and {ref}`Process <process-page>` sections to learn more about these features.\n\n## Execution abstraction\n\nWhile a process defines *what* command or script has to be executed, the *executor* determines *how* that script is actually run on the target system.\n\nIf not otherwise specified, processes are executed on the local computer. The local executor is very useful for pipeline development and testing purposes, but for real world computational pipelines an HPC or cloud platform is often required.\n\nIn other words, Nextflow provides an abstraction between the pipeline's functional logic and the underlying execution system. Thus it is possible to write a pipeline once and to seamlessly run it on your computer, a grid platform,", "start_char_idx": 0, "end_char_idx": 3542, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "32e4722a-009e-4f49-86d1-f4e68f38a76c": {"__data__": {"id_": "32e4722a-009e-4f49-86d1-f4e68f38a76c", "embedding": null, "metadata": {"file_path": "docs/basic.md", "file_name": "basic.md"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "6f7b64b74e52d83f0c64de07cde0e3d49b26acaa", "node_type": null, "metadata": {"file_path": "docs/basic.md", "file_name": "basic.md"}, "hash": "4dca7461ee31ed8fe0ae058c9f84172f2d14d68bce3ed05ac5467333ba27523e"}, "2": {"node_id": "865f459a-485e-43fb-ad14-e3b09686f266", "node_type": null, "metadata": {"file_path": "docs/basic.md", "file_name": "basic.md"}, "hash": "f7d405118804904311020fdce589f90996d65b62d602cc8f1299eae701c564ef"}}, "hash": "4c6d742a1786be7a0f8f9030cb8be94be1de4f855541abdc7d96212ec1d83563", "text": "a pipeline once and to seamlessly run it on your computer, a grid platform, or the cloud, without modifying it, by simply defining the target execution platform in the configuration file.\n\nThe following batch schedulers are supported:\n\n- [Open grid engine](http://gridscheduler.sourceforge.net/)\n- [Univa grid engine](http://www.univa.com/)\n- [Platform LSF](http://www.ibm.com/systems/technicalcomputing/platformcomputing/products/lsf/)\n- [Linux SLURM](https://computing.llnl.gov/linux/slurm/)\n- [Flux Framework](https://flux-framework.org/)\n- [PBS Works](http://www.pbsworks.com/gridengine/)\n- [Torque](http://www.adaptivecomputing.com/products/open-source/torque/)\n- [HTCondor](https://research.cs.wisc.edu/htcondor/)\n\nThe following cloud platforms are supported:\n\n- [Amazon Web Services (AWS)](https://aws.amazon.com/)\n- [Google Cloud Platform (GCP)](https://cloud.google.com/)\n- [Kubernetes](https://kubernetes.io/)\n\nRead the {ref}`executor-page` to learn more about the Nextflow executors.\n\n## Scripting language\n\nNextflow is designed to have a minimal learning curve, without having to pick up a new programming language. In most cases, users can utilise their current skills to develop Nextflow workflows. However, it also provides a powerful scripting DSL.\n\nNextflow scripting is an extension of the [Groovy programming language](<http://en.wikipedia.org/wiki/Groovy_(programming_language)>), which in turn is a super-set of the Java programming language. Groovy can be considered as Python for Java in that it simplifies the writing of code and is more approachable.\n\nRead the {ref}`script-page` section to learn about the Nextflow scripting language.\n\n<!-- TODO Running pipeline -->\n\n<!-- TODO Pipeline parameters -->\n\n## Configuration options\n\nPipeline configuration properties are defined in a file named `nextflow.config` in the pipeline execution directory.\n\nThis file can be used to define which executor to use, the process's environment variables, pipeline parameters etc.\n\nA basic configuration file might look like this:\n\n```groovy\nprocess {\n  executor = 'sge'\n  queue = 'cn-el6'\n}\n```\n\nRead the {ref}`config-page` section to learn more about the Nextflow configuration file and settings.", "start_char_idx": 3467, "end_char_idx": 5674, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "7bdafbdc-652c-4fad-acc4-dbe3db44623f": {"__data__": {"id_": "7bdafbdc-652c-4fad-acc4-dbe3db44623f", "embedding": null, "metadata": {"file_path": "docs/channel.md", "file_name": "channel.md"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "fcab491dca76f6cbed6abef8c60c3955d641fd52", "node_type": null, "metadata": {"file_path": "docs/channel.md", "file_name": "channel.md"}, "hash": "6640949cc4ce39fcf740021e39c851364273cd93dca40a6bf4475cb48809a401"}, "3": {"node_id": "cf99a2be-2c1c-489c-a1fa-852c9982b88a", "node_type": null, "metadata": {"file_path": "docs/channel.md", "file_name": "channel.md"}, "hash": "d0264a0a8123a5556d6fdb69ba182a8f43b6408840063c0e749e3609554d6755"}}, "hash": "8931151ac951d6e13dbdfeb0de7d32cba7439be06c50fc2ddd3f190f6ad02fc2", "text": "(channel-page)=\n\n# Channels\n\nNextflow is based on the Dataflow programming model in which processes communicate through channels.\n\nA channel has two major properties:\n\n1. Sending a message is an *asynchronous* operation which completes immediately, without having to wait for the receiving process.\n2. Receiving data is a blocking operation which stops the receiving process until the message has arrived.\n\n(channel-types)=\n\n## Channel types\n\nIn Nextflow there are two kinds of channels: *queue channels* and *value channels*.\n\n(channel-type-queue)=\n\n### Queue channel\n\nA *queue channel* is a non-blocking unidirectional FIFO queue which connects two processes, channel factories, or operators.\n\nA queue channel can be created by factory methods ([of](#of), [fromPath](#frompath), etc), operators ({ref}`operator-map`, {ref}`operator-flatmap`, etc), and processes (see {ref}`Process outputs <process-output>`).\n\n(channel-type-value)=\n\n### Value channel\n\nA *value channel* a.k.a. *singleton channel* is bound to a single value and can be read any number of times without being consumed.\n\nA value channel can be created with the [value](#value) factory method or by any operator that produces a single value ({ref}`operator-first`, {ref}`operator-collect`, {ref}`operator-reduce`, etc). Additionally, a process will emit value channels if it is invoked with all value channels, including simple values which are implicitly wrapped in a value channel.\n\nA value channel is implicitly created by a process when it is invoked with a simple value. Furthermore, a value channel is also implicitly created as output for a process whose inputs are all value channels.\n\nFor example:\n\n```groovy\nprocess foo {\n  input:\n  val x\n\n  output:\n  path 'x.txt'\n\n  \"\"\"\n  echo $x > x.txt\n  \"\"\"\n}\n\nworkflow {\n  result = foo(1)\n  result.view { \"Result: ${it}\" }\n}\n```\n\nIn the above example, since the `foo` process is invoked with a simple value instead of a channel, the input is implicitly converted to a value channel, and the output is also emitted as a value channel.\n\nSee also: {ref}`process-multiple-input-channels`.\n\n(channel-factory)=\n\n## Channel factories\n\nChannels may be created explicitly using the following channel factory methods.\n\n:::{versionadded} 20.07.0\n`channel` was introduced as an alias of `Channel`, allowing factory methods to be specified as `channel.of()` or `Channel.of()`, and so on.\n:::\n\n(channel-empty)=\n\n### empty\n\nThe `empty` factory method, by definition, creates a channel that doesn't emit any value.\n\nSee also: {ref}`operator-ifempty`.\n\n(channel-from)=\n\n### from\n\n:::{deprecated} 19.09.0-edge\nUse [of](#of) or [fromList](#fromlist) instead.\n:::\n\nThe `from` method allows you to create a channel emitting any sequence of values that are specified as the method argument, for example:\n\n```groovy\nch = Channel.from( 1, 3, 5, 7 )\nch.subscribe { println \"value: $it\" }\n```\n\nThe first line in this example creates a variable `ch` which holds a channel object. This channel emits the values specified as a parameter in the `from` method. Thus the second line will print the following:\n\n```\nvalue: 1\nvalue: 3\nvalue: 5\nvalue: 7\n```\n\nThe following example shows how to create a channel from a *range* of numbers or strings:\n\n```groovy\nzeroToNine = Channel.from( 0..9 )\nstrings = Channel.from( 'A'..'Z' )\n```\n\n:::{note}\nWhen the `from` argument is an object implementing", "start_char_idx": 0, "end_char_idx": 3372, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "cf99a2be-2c1c-489c-a1fa-852c9982b88a": {"__data__": {"id_": "cf99a2be-2c1c-489c-a1fa-852c9982b88a", "embedding": null, "metadata": {"file_path": "docs/channel.md", "file_name": "channel.md"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "fcab491dca76f6cbed6abef8c60c3955d641fd52", "node_type": null, "metadata": {"file_path": "docs/channel.md", "file_name": "channel.md"}, "hash": "6640949cc4ce39fcf740021e39c851364273cd93dca40a6bf4475cb48809a401"}, "2": {"node_id": "7bdafbdc-652c-4fad-acc4-dbe3db44623f", "node_type": null, "metadata": {"file_path": "docs/channel.md", "file_name": "channel.md"}, "hash": "8931151ac951d6e13dbdfeb0de7d32cba7439be06c50fc2ddd3f190f6ad02fc2"}, "3": {"node_id": "944c48e5-f036-4200-a31e-71d697017d5d", "node_type": null, "metadata": {"file_path": "docs/channel.md", "file_name": "channel.md"}, "hash": "e88a0ba318a6815380e58a07eba241171ae58e09270237b3d961c6cb915edc91"}}, "hash": "d0264a0a8123a5556d6fdb69ba182a8f43b6408840063c0e749e3609554d6755", "text": "the `from` argument is an object implementing the (Java) [Collection](http://docs.oracle.com/javase/7/docs/api/java/util/Collection.html) interface, the resulting channel emits the collection entries as individual items.\n:::\n\nThus the following two declarations produce an identical result even though in the first case the items are specified as multiple arguments while in the second case as a single list object argument:\n\n```groovy\nChannel.from( 1, 3, 5, 7, 9 )\nChannel.from( [1, 3, 5, 7, 9] )\n```\n\nBut when more than one argument is provided, they are always managed as *single* emissions. Thus, the following example creates a channel emitting three entries each of which is a list containing two elements:\n\n```groovy\nChannel.from( [1, 2], [5,6], [7,9] )\n```\n\n(channel-fromlist)=\n\n### fromList\n\n:::{versionadded} 19.10.0\n:::\n\nThe `fromList` method allows you to create a channel emitting the values provided as a list of elements, for example:\n\n```groovy\nChannel\n    .fromList( ['a', 'b', 'c', 'd'] )\n    .view { \"value: $it\" }\n```\n\nPrints:\n\n```\nvalue: a\nvalue: b\nvalue: c\nvalue: d\n```\n\nSee also: [of](#of) factory method.\n\n(channel-path)=\n\n### fromPath\n\nYou can create a channel emitting one or more file paths by using the `fromPath` method and specifying a path string as an argument. For example:\n\n```groovy\nmyFileChannel = Channel.fromPath( '/data/some/bigfile.txt' )\n```\n\nThe above line creates a channel and binds it to a [Path](http://docs.oracle.com/javase/7/docs/api/java/nio/file/Path.html) object for the specified file.\n\n:::{note}\n`fromPath` does not check whether the file exists.\n:::\n\nWhenever the `fromPath` argument contains a `*` or `?` wildcard character it is interpreted as a [glob][glob] path matcher. For example:\n\n```groovy\nmyFileChannel = Channel.fromPath( '/data/big/*.txt' )\n```\n\nThis example creates a channel and emits as many `Path` items as there are files with `txt` extension in the `/data/big` folder.\n\n:::{tip}\nTwo asterisks, i.e. `**`, works like `*` but crosses directory boundaries. This syntax is generally used for matching complete paths. Curly brackets specify a collection of sub-patterns.\n:::\n\nFor example:\n\n```groovy\nfiles = Channel.fromPath( 'data/**.fa' )\nmoreFiles = Channel.fromPath( 'data/**/*.fa' )\npairFiles = Channel.fromPath( 'data/file_{1,2}.fq' )\n```\n\nThe first line returns a channel emitting the files ending with the suffix `.fa` in the `data` folder *and* recursively in all its sub-folders. While the second one only emits the files which have the same suffix in *any* sub-folder in the `data` path. Finally the last example emits two files: `data/file_1.fq` and `data/file_2.fq`.\n\n:::{note}\nAs in Linux Bash, the `*` wildcard does not catch hidden files (i.e. files whose name starts with a `.` character).\n:::\n\nMultiple paths or glob patterns can be specified using a list:\n\n```groovy\nChannel.fromPath( ['/some/path/*.fq', '/other/path/*.fastq'] )\n```\n\nIn order to include hidden files, you need to start your pattern with a period character or specify the `hidden: true` option. For example:\n\n```groovy\nexpl1 = Channel.fromPath(", "start_char_idx": 3334, "end_char_idx": 6432, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "944c48e5-f036-4200-a31e-71d697017d5d": {"__data__": {"id_": "944c48e5-f036-4200-a31e-71d697017d5d", "embedding": null, "metadata": {"file_path": "docs/channel.md", "file_name": "channel.md"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "fcab491dca76f6cbed6abef8c60c3955d641fd52", "node_type": null, "metadata": {"file_path": "docs/channel.md", "file_name": "channel.md"}, "hash": "6640949cc4ce39fcf740021e39c851364273cd93dca40a6bf4475cb48809a401"}, "2": {"node_id": "cf99a2be-2c1c-489c-a1fa-852c9982b88a", "node_type": null, "metadata": {"file_path": "docs/channel.md", "file_name": "channel.md"}, "hash": "d0264a0a8123a5556d6fdb69ba182a8f43b6408840063c0e749e3609554d6755"}, "3": {"node_id": "1ede687e-b6b9-4190-9182-2111f6a1c8e6", "node_type": null, "metadata": {"file_path": "docs/channel.md", "file_name": "channel.md"}, "hash": "ac2e37afea77db640bf6ae9e72623dd319b82e65f8e062208bc2915e2cab12fd"}}, "hash": "e88a0ba318a6815380e58a07eba241171ae58e09270237b3d961c6cb915edc91", "text": "option. For example:\n\n```groovy\nexpl1 = Channel.fromPath( '/path/.*' )\nexpl2 = Channel.fromPath( '/path/.*.fa' )\nexpl3 = Channel.fromPath( '/path/*', hidden: true )\n```\n\nThe first example returns all hidden files in the specified path. The second one returns all hidden files ending with the `.fa` suffix. Finally the last example returns all files (hidden and non-hidden) in that path.\n\nBy default a [glob][glob] pattern only looks for regular file paths that match the specified criteria, i.e. it won't return directory paths.\n\nYou can use the `type` option specifying the value `file`, `dir` or `any` in order to define what kind of paths you want. For example:\n\n```groovy\nmyFileChannel = Channel.fromPath( '/path/*b', type: 'dir' )\nmyFileChannel = Channel.fromPath( '/path/a*', type: 'any' )\n```\n\nThe first example will return all *directory* paths ending with the `b` suffix, while the second will return any file or directory starting with a `a` prefix.\n\nAvailable options:\n\n`checkIfExists`\n: When `true` throws an exception of the specified path do not exist in the file system (default: `false`)\n\n`followLinks`\n: When `true` it follows symbolic links during directories tree traversal, otherwise they are managed as files (default: `true`)\n\n`glob`\n: When `true` interprets characters `*`, `?`, `[]` and `{}` as glob wildcards, otherwise handles them as normal characters (default: `true`)\n\n`hidden`\n: When `true` includes hidden files in the resulting paths (default: `false`)\n\n`maxDepth`\n: Maximum number of directory levels to visit (default: no limit)\n\n`relative`\n: When `true` returned paths are relative to the top-most common directory (default: `false`)\n\n`type`\n: Type of paths returned, either `file`, `dir` or `any` (default: `file`)\n\n(channel-filepairs)=\n\n### fromFilePairs\n\nThe `fromFilePairs` method creates a channel emitting the file pairs matching a [glob][glob] pattern provided by the user. The matching files are emitted as tuples in which the first element is the grouping key of the matching pair and the second element is the list of files (sorted in lexicographical order). For example:\n\n```groovy\nChannel\n    .fromFilePairs('/my/data/SRR*_{1,2}.fastq')\n    .view()\n```\n\nIt will produce an output similar to the following:\n\n```\n[SRR493366, [/my/data/SRR493366_1.fastq, /my/data/SRR493366_2.fastq]]\n[SRR493367, [/my/data/SRR493367_1.fastq, /my/data/SRR493367_2.fastq]]\n[SRR493368, [/my/data/SRR493368_1.fastq, /my/data/SRR493368_2.fastq]]\n[SRR493369, [/my/data/SRR493369_1.fastq, /my/data/SRR493369_2.fastq]]\n[SRR493370, [/my/data/SRR493370_1.fastq, /my/data/SRR493370_2.fastq]]\n[SRR493371, [/my/data/SRR493371_1.fastq, /my/data/SRR493371_2.fastq]]\n```\n\n:::{note}\nThe glob pattern must contain at least one `*` wildcard character.\n:::\n\nMultiple glob patterns can be specified using a list:\n\n```groovy\nChannel.fromFilePairs(", "start_char_idx": 6419, "end_char_idx": 9271, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "1ede687e-b6b9-4190-9182-2111f6a1c8e6": {"__data__": {"id_": "1ede687e-b6b9-4190-9182-2111f6a1c8e6", "embedding": null, "metadata": {"file_path": "docs/channel.md", "file_name": "channel.md"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "fcab491dca76f6cbed6abef8c60c3955d641fd52", "node_type": null, "metadata": {"file_path": "docs/channel.md", "file_name": "channel.md"}, "hash": "6640949cc4ce39fcf740021e39c851364273cd93dca40a6bf4475cb48809a401"}, "2": {"node_id": "944c48e5-f036-4200-a31e-71d697017d5d", "node_type": null, "metadata": {"file_path": "docs/channel.md", "file_name": "channel.md"}, "hash": "e88a0ba318a6815380e58a07eba241171ae58e09270237b3d961c6cb915edc91"}, "3": {"node_id": "a2f1a300-5ed5-49e7-ba30-3e09b09849f0", "node_type": null, "metadata": {"file_path": "docs/channel.md", "file_name": "channel.md"}, "hash": "10c12122c6a6740ca77bc9dfb17d6a3dfbfdeb4f220977de65304d7ba88f1b72"}}, "hash": "ac2e37afea77db640bf6ae9e72623dd319b82e65f8e062208bc2915e2cab12fd", "text": "be specified using a list:\n\n```groovy\nChannel.fromFilePairs( ['/some/data/SRR*_{1,2}.fastq', '/other/data/QFF*_{1,2}.fastq'] )\n```\n\nAlternatively, it is possible to implement a custom file pair grouping strategy providing a closure which, given the current file as parameter, returns the grouping key. For example:\n\n```groovy\nChannel\n    .fromFilePairs('/some/data/*', size: -1) { file -> file.extension }\n    .view { ext, files -> \"Files with the extension $ext are $files\" }\n```\n\nAvailable options:\n\n`checkIfExists`\n: When `true` throws an exception of the specified path do not exist in the file system (default: `false`)\n\n`followLinks`\n: When `true` it follows symbolic links during directories tree traversal, otherwise they are managed as files (default: `true`)\n\n`flat`\n: When `true` the matching files are produced as sole elements in the emitted tuples (default: `false`).\n\n`hidden`\n: When `true` includes hidden files in the resulting paths (default: `false`)\n\n`maxDepth`\n: Maximum number of directory levels to visit (default: no limit)\n\n`size`\n: Defines the number of files each emitted item is expected to hold (default: 2). Set to `-1` for any.\n\n`type`\n: Type of paths returned, either `file`, `dir` or `any` (default: `file`)\n\n(channel-fromsra)=\n\n### fromSRA\n\n:::{versionadded} 19.04.0\n:::\n\nThe `fromSRA` method queries the [NCBI SRA](https://www.ncbi.nlm.nih.gov/sra) database and returns a channel emitting the FASTQ files matching the specified criteria i.e project or accession number(s). For example:\n\n```groovy\nChannel\n    .fromSRA('SRP043510')\n    .view()\n```\n\nIt returns:\n\n```\n[SRR1448794, ftp://ftp.sra.ebi.ac.uk/vol1/fastq/SRR144/004/SRR1448794/SRR1448794.fastq.gz]\n[SRR1448795, ftp://ftp.sra.ebi.ac.uk/vol1/fastq/SRR144/005/SRR1448795/SRR1448795.fastq.gz]\n[SRR1448792, ftp://ftp.sra.ebi.ac.uk/vol1/fastq/SRR144/002/SRR1448792/SRR1448792.fastq.gz]\n[SRR1448793, ftp://ftp.sra.ebi.ac.uk/vol1/fastq/SRR144/003/SRR1448793/SRR1448793.fastq.gz]\n[SRR1910483, ftp://ftp.sra.ebi.ac.uk/vol1/fastq/SRR191/003/SRR1910483/SRR1910483.fastq.gz]\n[SRR1910482, ftp://ftp.sra.ebi.ac.uk/vol1/fastq/SRR191/002/SRR1910482/SRR1910482.fastq.gz]\n(remaining omitted)\n```\n\nMultiple accession IDs can be specified using a list object:\n\n```groovy\nids = ['ERR908507', 'ERR908506', 'ERR908505']\nChannel\n    .fromSRA(ids)\n    .view()\n```\n\n```\n[ERR908507, [ftp://ftp.sra.ebi.ac.uk/vol1/fastq/ERR908/ERR908507/ERR908507_1.fastq.gz,", "start_char_idx": 9269, "end_char_idx": 11691, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "a2f1a300-5ed5-49e7-ba30-3e09b09849f0": {"__data__": {"id_": "a2f1a300-5ed5-49e7-ba30-3e09b09849f0", "embedding": null, "metadata": {"file_path": "docs/channel.md", "file_name": "channel.md"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "fcab491dca76f6cbed6abef8c60c3955d641fd52", "node_type": null, "metadata": {"file_path": "docs/channel.md", "file_name": "channel.md"}, "hash": "6640949cc4ce39fcf740021e39c851364273cd93dca40a6bf4475cb48809a401"}, "2": {"node_id": "1ede687e-b6b9-4190-9182-2111f6a1c8e6", "node_type": null, "metadata": {"file_path": "docs/channel.md", "file_name": "channel.md"}, "hash": "ac2e37afea77db640bf6ae9e72623dd319b82e65f8e062208bc2915e2cab12fd"}, "3": {"node_id": "487e6953-b868-421d-a102-4c50e24c5e39", "node_type": null, "metadata": {"file_path": "docs/channel.md", "file_name": "channel.md"}, "hash": "47da5a22edb2422350c9debba1a4e5600b706e8d463bc8f64d2dd80a66d6ec03"}}, "hash": "10c12122c6a6740ca77bc9dfb17d6a3dfbfdeb4f220977de65304d7ba88f1b72", "text": "ftp://ftp.sra.ebi.ac.uk/vol1/fastq/ERR908/ERR908507/ERR908507_2.fastq.gz]]\n[ERR908506, [ftp://ftp.sra.ebi.ac.uk/vol1/fastq/ERR908/ERR908506/ERR908506_1.fastq.gz, ftp://ftp.sra.ebi.ac.uk/vol1/fastq/ERR908/ERR908506/ERR908506_2.fastq.gz]]\n[ERR908505, [ftp://ftp.sra.ebi.ac.uk/vol1/fastq/ERR908/ERR908505/ERR908505_1.fastq.gz, ftp://ftp.sra.ebi.ac.uk/vol1/fastq/ERR908/ERR908505/ERR908505_2.fastq.gz]]\n```\n\n:::{note}\nEach read pair is implicitly managed and returned as a list of files.\n:::\n\nThis method uses the NCBI [ESearch](https://www.ncbi.nlm.nih.gov/books/NBK25499/#chapter4.ESearch) API behind the scenes, therefore it allows the use of any query term supported by this API.\n\nTo access the ESearch API, you must provide your [NCBI API keys](https://ncbiinsights.ncbi.nlm.nih.gov/2017/11/02/new-api-keys-for-the-e-utilities) through one of the following ways:\n\n- The `apiKey` option:\n  ```groovy\n  Channel.fromSRA(ids, apiKey:'0123456789abcdef')\n  ```\n\n- The `NCBI_API_KEY` variable in your environment:\n  ```bash\n  export NCBI_API_KEY=0123456789abcdef\n  ```\n\nAvailable options:\n\n`apiKey`\n: NCBI user API key.\n\n`cache`\n: Enable/disable the caching API requests (default: `true`).\n\n`max`\n: Maximum number of entries that can be retried (default: unlimited) .\n\n`protocol`\n: Allow choosing the protocol for the resulting remote URLs. Available choices: `ftp`, `http`, `https` (default: `ftp`).\n\n(channel-of)=\n\n### of\n\n:::{versionadded} 19.10.0\n:::\n\nThe `of` method allows you to create a channel that emits the arguments provided to it, for example:\n\n```groovy\nch = Channel.of( 1, 3, 5, 7 )\nch.view { \"value: $it\" }\n```\n\nThe first line in this example creates a variable `ch` which holds a channel object. This channel emits the arguments supplied to the `of` method. Thus the second line prints the following:\n\n```\nvalue: 1\nvalue: 3\nvalue: 5\nvalue: 7\n```\n\nRanges of values are expanded accordingly:\n\n```groovy\nChannel\n    .of(1..23, 'X', 'Y')\n    .view()\n```\n\nPrints:\n\n```\n1\n2\n3\n4\n:\n23\nX\nY\n```\n\nSee also: [fromList](#fromlist) factory method.\n\n(channel-value)=\n\n### value\n\nThe `value` method is used to create a value channel. An optional (not `null`) argument can be specified to bind the channel to a specific value. For example:\n\n```groovy\nexpl1 = Channel.value()\nexpl2 = Channel.value( 'Hello there' )\nexpl3 = Channel.value( [1,2,3,4,5] )\n```\n\nThe first line in the example creates an 'empty' variable. The second line creates a channel and binds a string to it. The third line creates a channel and binds a list object to it that will be emitted as a single", "start_char_idx": 11748, "end_char_idx": 14312, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "487e6953-b868-421d-a102-4c50e24c5e39": {"__data__": {"id_": "487e6953-b868-421d-a102-4c50e24c5e39", "embedding": null, "metadata": {"file_path": "docs/channel.md", "file_name": "channel.md"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "fcab491dca76f6cbed6abef8c60c3955d641fd52", "node_type": null, "metadata": {"file_path": "docs/channel.md", "file_name": "channel.md"}, "hash": "6640949cc4ce39fcf740021e39c851364273cd93dca40a6bf4475cb48809a401"}, "2": {"node_id": "a2f1a300-5ed5-49e7-ba30-3e09b09849f0", "node_type": null, "metadata": {"file_path": "docs/channel.md", "file_name": "channel.md"}, "hash": "10c12122c6a6740ca77bc9dfb17d6a3dfbfdeb4f220977de65304d7ba88f1b72"}}, "hash": "47da5a22edb2422350c9debba1a4e5600b706e8d463bc8f64d2dd80a66d6ec03", "text": "creates a channel and binds a list object to it that will be emitted as a single value.\n\n(channel-watchpath)=\n\n### watchPath\n\nThe `watchPath` method watches a folder for one or more files matching a specified pattern. As soon as there is a file that meets the specified condition, it is emitted over the channel that is returned by the `watchPath` method. The condition on files to watch can be specified by using `*` or `?` wildcard characters i.e. by specifying a [glob][glob] path matching criteria.\n\nFor example:\n\n```groovy\nChannel\n    .watchPath( '/path/*.fa' )\n    .subscribe { println \"Fasta file: $it\" }\n```\n\nBy default it watches only for new files created in the specified folder. Optionally, it is possible to provide a second argument that specifies what event(s) to watch. The supported events are:\n\n- `create`: A new file is created (default)\n- `modify`: A file is modified\n- `delete`: A file is deleted\n\nYou can specify more than one of these events by using a comma separated string as shown below:\n\n```groovy\nChannel\n    .watchPath( '/path/*.fa', 'create,modify' )\n    .subscribe { println \"File created or modified: $it\" }\n```\n\n:::{warning}\nThe `watchPath` factory waits endlessly for files that match the specified pattern and event(s), which means that it will cause your pipeline to run forever. Consider using the `take` or `until` operator to close the channel when a certain condition is met (e.g. after receiving 10 files, receiving a file named `DONE`).\n:::\n\nSee also: [fromPath](#frompath) factory method.\n\n[glob]: http://docs.oracle.com/javase/tutorial/essential/io/fileOps.html#glob", "start_char_idx": 14232, "end_char_idx": 15843, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "125170b8-9579-4228-aaef-bf6a18ad2b84": {"__data__": {"id_": "125170b8-9579-4228-aaef-bf6a18ad2b84", "embedding": null, "metadata": {"file_path": "docs/cli.md", "file_name": "cli.md"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "164ca94a0451118caab80ef9f9fe59d9b70e1ac7", "node_type": null, "metadata": {"file_path": "docs/cli.md", "file_name": "cli.md"}, "hash": "a3a18127b3c8b2dfe4351fbe845315df87f47e169d6540fbf9bde55b11cbd872"}, "3": {"node_id": "2298a7b1-911b-490c-a2ed-1877652579ee", "node_type": null, "metadata": {"file_path": "docs/cli.md", "file_name": "cli.md"}, "hash": "c43d841afc99370c869c5637fc39dd49f8e733357b8b6ea4ecc331d7ed0909f8"}}, "hash": "50ea7d5e69a44d7cac1114c255db670289075dc6d96cdb83705839d8bc8a6c0a", "text": "(cli-page)=\n\n# Command line interface (CLI)\n\nNextflow provides a robust command line interface for the management and execution pipelines.\n\nSimply run `nextflow` with no options or `nextflow -h` to see the list of available top-level options and commands.\n\n(cli-options)=\n\n## Options\n\nThe top-level options are meant to be invoked in relation to the core Nextflow application and are applied to all commands. For options specific to any command, refer the CLI Commands section.\n\n:::{note}\nNextflow options use a single dash prefix, e.g. `-foo`. Do not confuse with double dash notation, e.g. `--foo`, which is instead used for {ref}`Pipeline parameters <cli-params>`.\n:::\n\nAvailable options:\n\n`-C`\n: Use the specified configuration file(s) overriding any defaults.\n\n`-D`\n: Set JVM properties.\n\n`-bg`\n: Execute nextflow in background.\n\n`-c, -config`\n: Add the specified file to configuration set.\n\n`-d, -dockerize`\n: Launch nextflow via Docker (experimental).\n\n`-h`\n: Print this help.\n\n`-log`\n: Set nextflow log file path.\n\n`-q, -quiet`\n: Do not print information messages.\n\n`-syslog`\n: Send logs to syslog server (e.g. localhost:514).\n\n`-v, -version`\n: Print the program version.\n\n### Hard configuration override\n\nUse the specified configuration file(s) overriding any defaults.\n\n```console\n$ nextflow -C my.config COMMAND [arg...]\n```\n\nThe `-C` option is used to override *all* settings specified in the default config file. For soft override, please refer the `-c` option.\n\n- Override **any** default configuration with a custom configuration file:\n\n  ```\n  $ nextflow -C my.config run nextflow-io/hello\n  ```\n\n### JVM properties\n\nSet JVM properties.\n\n```console\n$ nextflow -Dkey=value COMMAND [arg...]\n```\n\nThis options allows the definition of custom Java system properties that can be used to properly configure or fine tuning the JVM instance used by the Nextflow runtime.\n\nFor specifying other JVM level options, please refer to the {ref}`config-env-vars` section.\n\n- Add JVM properties to the invoked pipeline:\n\n  ```console\n  $ nextflow -Dfile.encoding=UTF-8 run nextflow-io/hello\n  ```\n\n### Execution as a background job\n\nExecute `nextflow` in the background.\n\n```console\n$ nextflow -bg COMMAND [arg...]\n```\n\nThe `-bg` option is used to invoke the nextflow execution in the background and allows the user to continue interacting with the terminal. This option is similar to `nohup` in behavior.\n\n- Invoke any execution as a background job:\n\n  ```console\n  $ nextflow -bg run nextflow-io/hello\n  ```\n\n### Soft configuration override\n\nAdd the specified file to configuration set.\n\n```console\n$ nextflow -c nxf.config COMMAND [arg...]\n```\n\nThe `-c` option is used to append a new configuration to the default configuration. The `-c` option allows us to update the config in an additive manner. For **hard override**, refer to the `-C` option.\n\n- Update *some* fields of the default config for any pipeline:\n\n  ```console\n  $ nextflow -c nxf.config run nextflow-io/hello\n  ```\n\n### Docker driven execution\n\n:::{warning} *Experimental: not recommended for production environments.*\n:::\n\nLaunch Nextflow via Docker.\n\n```console\n$ nextflow -dockerize COMMAND [arg...]\n```\n\nThe `-dockerize` option is used to invoke the execution of Nextflow within a", "start_char_idx": 0, "end_char_idx": 3252, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "2298a7b1-911b-490c-a2ed-1877652579ee": {"__data__": {"id_": "2298a7b1-911b-490c-a2ed-1877652579ee", "embedding": null, "metadata": {"file_path": "docs/cli.md", "file_name": "cli.md"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "164ca94a0451118caab80ef9f9fe59d9b70e1ac7", "node_type": null, "metadata": {"file_path": "docs/cli.md", "file_name": "cli.md"}, "hash": "a3a18127b3c8b2dfe4351fbe845315df87f47e169d6540fbf9bde55b11cbd872"}, "2": {"node_id": "125170b8-9579-4228-aaef-bf6a18ad2b84", "node_type": null, "metadata": {"file_path": "docs/cli.md", "file_name": "cli.md"}, "hash": "50ea7d5e69a44d7cac1114c255db670289075dc6d96cdb83705839d8bc8a6c0a"}, "3": {"node_id": "a4f6a240-67cc-4018-9ba1-33ac586fd589", "node_type": null, "metadata": {"file_path": "docs/cli.md", "file_name": "cli.md"}, "hash": "eab843354cd697c2ea73123f8745ec95dce17a2bd09c1162cad0ba285a7d665b"}}, "hash": "c43d841afc99370c869c5637fc39dd49f8e733357b8b6ea4ecc331d7ed0909f8", "text": "`-dockerize` option is used to invoke the execution of Nextflow within a Docker container itself without installing a Java VM in the hosting environment.\n\nThis option is *not* needed to run containerised pipeline jobs. For invoking a pipeline with the `docker` profile or executor, please refer to the `-with-docker` options in the `run` command. When using the `-dockerize` option in combination with containerized tasks, Nextflow will launch the tasks as sibling containers in the host environment (i.e. no Docker-in-Docker).\n\n- Invoke `nextflow` as a Docker container to execute a pipeline:\n\n  ```console\n  $ nextflow -dockerize run nextflow-io/hello\n  ```\n\n### Help\n\nPrint the help message.\n\n```console\n$ nextflow -h\n```\n\nThe `-h` option prints out the overview of the CLI interface and enumerates the top-level *options* and *commands*.\n\n### Execution logs\n\nSets the path of the nextflow log file.\n\n```console\n$ nextflow -log custom.log COMMAND [arg...]\n```\n\nThe `-log` option takes a path of the new log file which to be used instead of the default `.nextflow.log` or to save logs files to another directory.\n\n- Save all execution logs to the custom `/var/log/nextflow.log` file:\n\n  ```console\n  $ nextflow -log /var/log/nextflow.log run nextflow-io/hello\n  ```\n\n### Quiet execution\n\nDisable the printing of information to the terminal.\n\n```console\n$ nextflow -q COMMAND [arg...]\n```\n\nThe `-q` option suppresses the banner and process-related info, and exits once the execution is completed. Please note that it does not affect any explicit print statement within a pipeline.\n\n- Invoke the pipeline execution without the banner and pipeline information:\n\n  ```console\n  $ nextflow -q run nextflow-io/hello\n  ```\n\n### Logging to a syslog server\n\nSend logs to [Syslog](https://en.wikipedia.org/wiki/Syslog) server endpoint.\n\n```console\n$ nextflow -syslog localhost:1234 COMMAND [arg...]\n```\n\nThe `-syslog` option is used to send logs to a Syslog logging server at the specified endpoint.\n\n- Send the logs to a Syslog server at specific endpoint:\n\n  ```console\n  $ nextflow -syslog localhost:1234 run nextflow-io/hello\n  ```\n\n### Version\n\nPrint the Nextflow version information.\n\n```console\n$ nextflow -v\n```\n\nThe `-v` option prints out information about Nextflow, such as the version and build. The `-version` option in addition prints out the citation reference and official website.\n\n- The short version:\n\n  ```\n  $ nextflow -v\n  nextflow version 20.07.1.5412\n  ```\n\n- The full version info with citation and website link:\n\n  ```\n  $ nextflow -version\n  N E X T F L O W\n  version 20.07.1 build 5412\n  created 24-07-2020 15:18 UTC (20:48 IDT)\n  cite doi:10.1038/nbt.3820\n  http://nextflow.io\n  ```\n\n(cli-commands)=\n\n## Commands\n\n### clean\n\nClean up *cache* and *work* directories.\n\n**Usage**\n\n```console\n$ nextflow clean [run_name|session_id] [options]\n```\n\n**Description**\n\nUpon invocation within a directory, `nextflow` creates a project specific `.nextflow.log` file, `.nextflow` cache directory as well as a `work` directory. The `clean` command is designed to facilitate removal of these files from previous executions. A list of run names and session ids can be generated by invoking `nextflow log -q`.\n\nIf no run name", "start_char_idx": 3192, "end_char_idx": 6421, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "a4f6a240-67cc-4018-9ba1-33ac586fd589": {"__data__": {"id_": "a4f6a240-67cc-4018-9ba1-33ac586fd589", "embedding": null, "metadata": {"file_path": "docs/cli.md", "file_name": "cli.md"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "164ca94a0451118caab80ef9f9fe59d9b70e1ac7", "node_type": null, "metadata": {"file_path": "docs/cli.md", "file_name": "cli.md"}, "hash": "a3a18127b3c8b2dfe4351fbe845315df87f47e169d6540fbf9bde55b11cbd872"}, "2": {"node_id": "2298a7b1-911b-490c-a2ed-1877652579ee", "node_type": null, "metadata": {"file_path": "docs/cli.md", "file_name": "cli.md"}, "hash": "c43d841afc99370c869c5637fc39dd49f8e733357b8b6ea4ecc331d7ed0909f8"}, "3": {"node_id": "c25a9759-eb01-484a-87af-7787accef6c9", "node_type": null, "metadata": {"file_path": "docs/cli.md", "file_name": "cli.md"}, "hash": "1fdaf9836310bee99f3ac0720955ff5dc76ee0277cbeefe7b17b611bab0d198d"}}, "hash": "eab843354cd697c2ea73123f8745ec95dce17a2bd09c1162cad0ba285a7d665b", "text": "ids can be generated by invoking `nextflow log -q`.\n\nIf no run name or session id is provided, it will clean the latest run.\n\n**Options**\n\n`-after`\n: Clean up runs executed *after* the specified one.\n\n`-before`\n: Clean up runs executed *before* the specified one.\n\n`-but`\n: Clean up all runs *except* the specified one.\n\n`-n, -dry-run`\n: Print names of files to be removed without deleting them.\n\n`-f, -force`\n: Force clean command.\n\n`-h, -help`\n: Print the command usage.\n\n`-k, -keep-logs`\n: Removes only temporary files but retains execution log entries and metadata.\n\n`-q, -quiet`\n: Do not print names of files removed.\n\n**Examples**\n\nDry run to remove work directories for the run name `boring_euler`:\n\n```console\n$ nextflow clean boring_euler -n\n\nWould remove work/92/c1a9cd9a96e0531d81ca69f5dc3bb7\nWould remove work/3f/70944c7a549b6221e1ccc7b4b21b62\nWould remove work/0e/2ebdba85f76f6068b21a1bcbf10cab\n```\n\nRemove work directories for the run name `boring_euler`.\n\n```console\n$ nextflow clean boring_euler -f\n\nRemoved work/92/c1a9cd9a96e0531d81ca69f5dc3bb7\nRemoved work/3f/70944c7a549b6221e1ccc7b4b21b62\nRemoved work/0e/2ebdba85f76f6068b21a1bcbf10cab\n```\n\nRemove the execution entries *except* for a specific execution.\n\n```console\n$ nextflow clean -but tiny_leavitt -f\n\nRemoved work/1f/f1ea9158fb23b53d5083953121d6b6\nRemoved work/bf/334115deec60929dc18edf0010032a\nRemoved work/a3/06521d75da296d4dd7f4f8caaddad8\n```\n\nDry run to remove the execution data *before* a specific execution.\n\n```console\n$ nextflow clean -before tiny_leavitt -n\n\nWould remove work/5d/ad76f7b7ab3500cf616814ef644b61\nWould remove work/c4/69a82b080a477612ba8d8e4c27b579\nWould remove work/be/a4fa2aa38f76fd324958c81c2e4603\nWould remove work/54/39116773891c47a91e3c1733aad4de\n```\n\nDry run to remove the execution data *after* a specific execution.\n\n```console\n$ nextflow clean -after focused_payne -n\n\nWould remove work/1f/f1ea9158fb23b53d5083953121d6b6\nWould remove work/bf/334115deec60929dc18edf0010032a\nWould remove work/a3/06521d75da296d4dd7f4f8caaddad8\n```\n\nDry run to remove the temporary execution data for a specific execution, while keeping the log files.\n\n```console\n$ nextflow clean -keep-logs tiny_leavitt -n\n\nWould remove temp files from work/1f/f1ea9158fb23b53d5083953121d6b6\nWould remove temp files from work/bf/334115deec60929dc18edf0010032a\nWould remove temp files from work/a3/06521d75da296d4dd7f4f8caaddad8\n```\n\n### clone\n\nClone a remote project into a folder.\n\n**Usage**\n\n```console\n$ nextflow clone [options]", "start_char_idx": 6427, "end_char_idx": 8933, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "c25a9759-eb01-484a-87af-7787accef6c9": {"__data__": {"id_": "c25a9759-eb01-484a-87af-7787accef6c9", "embedding": null, "metadata": {"file_path": "docs/cli.md", "file_name": "cli.md"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "164ca94a0451118caab80ef9f9fe59d9b70e1ac7", "node_type": null, "metadata": {"file_path": "docs/cli.md", "file_name": "cli.md"}, "hash": "a3a18127b3c8b2dfe4351fbe845315df87f47e169d6540fbf9bde55b11cbd872"}, "2": {"node_id": "a4f6a240-67cc-4018-9ba1-33ac586fd589", "node_type": null, "metadata": {"file_path": "docs/cli.md", "file_name": "cli.md"}, "hash": "eab843354cd697c2ea73123f8745ec95dce17a2bd09c1162cad0ba285a7d665b"}, "3": {"node_id": "d3327c84-8696-4709-a1cb-f9cf4f353e60", "node_type": null, "metadata": {"file_path": "docs/cli.md", "file_name": "cli.md"}, "hash": "08e5978e60f7c607115051c0cc69e7d8ee3021a70c2b86d32112a5a2261d26a1"}}, "hash": "1fdaf9836310bee99f3ac0720955ff5dc76ee0277cbeefe7b17b611bab0d198d", "text": "folder.\n\n**Usage**\n\n```console\n$ nextflow clone [options] [project]\n```\n\n**Description**\n\nThe `clone` command downloads a pipeline from a Git-hosting platform into the *current directory* and modifies it accordingly. For downloading a pipeline into the global cache `~/.nextflow/assets`, please refer to the `nextflow pull` command.\n\n**Options**\n\n`-d, -deep`\n: Create a shallow clone of the specified depth.\n\n`-h, -help`\n: Print the command usage.\n\n`-hub` (`github`)\n: Service hub where the project is hosted. Options: `gitlab` or `bitbucket`.\n\n`-r` (`master`)\n: Revision to clone - It can be a git branch, tag, or revision number.\n\n`-user`\n: Private repository user name.\n\n**Examples**\n\nClone the latest revision of a pipeline.\n\n```console\n$ nextflow clone nextflow-io/hello\nnextflow-io/hello cloned to: hello\n```\n\nClone a specific revision of a pipeline.\n\n```console\n$ nextflow clone nextflow-io/hello -r v1.1\nnextflow-io/hello cloned to: hello\n```\n\n### config\n\nPrint the resolved pipeline configuration.\n\n**Usage**\n\n```console\n$ nextflow config [options] [project name or path]\n```\n\n**Description**\n\nThe `config` command is used for printing the project's configuration i.e. the `nextflow.config` and is especially useful for understanding the resolved profiles and parameters that Nextflow will use run a pipeline. For in-depth information, please refer the {ref}`config-profiles` section.\n\n**Options**\n\n`-flat`\n: Print config using flat notation.\n\n`-h, -help`\n: Print the command usage.\n\n`-profile`\n: Choose a configuration profile.\n\n`-properties`\n: Print config using Java properties notation.\n\n`-a, -show-profiles`\n: Show all configuration profiles.\n\n`-sort`\n: Sort config attributes.\n\n**Examples**\n\nPrint out the inferred config using a the default group key-value notation.\n\n```console\n$ nextflow config\n\ndocker {\n    enabled = true\n}\n\nprocess {\n    executor = 'local'\n}\n```\n\nPrint out the config using a flat notation.\n\n```console\n$ nextflow config -flat\n\ndocker.enabled = true\nprocess.executor = 'local'\n```\n\nPrint out the config using the Java properties notation.\n\n```console\n$ nextflow config -properties\n\ndocker.enabled = true\nprocess.executor = local\n```\n\nPrint out all profiles from the project's configuration.\n\n```console\n$ nextflow config -show-profiles\n\ndocker {\n    enabled = true\n}\n\nprofiles {\n    standard {\n        process {\n            executor = 'local'\n        }\n    }\n    cloud {\n        process {\n            executor = 'cirrus'\n            container = 'cbcrg/imagex'\n        }\n    }\n}\n```\n\n### console\n\nLaunch the Nextflow interactive console.\n\n**Usage**\n\n```console\n$ nextflow console\n```\n\n**Description**\n\nThe `console` command provides a Graphical User Interface (GUI) and an interactive REPL (Read-Eval-Print-Loop) for quick experimentation.\n\n**Options**\n\nNone available\n\n**Examples**\n\nLaunch the `console` GUI.\n\n```console\n$ nextflow console\n```\n\n### drop\n\nDelete the local copy of a project.\n\n**Usage**\n\n```console\n$ nextflow drop", "start_char_idx": 8936, "end_char_idx": 11903, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "d3327c84-8696-4709-a1cb-f9cf4f353e60": {"__data__": {"id_": "d3327c84-8696-4709-a1cb-f9cf4f353e60", "embedding": null, "metadata": {"file_path": "docs/cli.md", "file_name": "cli.md"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "164ca94a0451118caab80ef9f9fe59d9b70e1ac7", "node_type": null, "metadata": {"file_path": "docs/cli.md", "file_name": "cli.md"}, "hash": "a3a18127b3c8b2dfe4351fbe845315df87f47e169d6540fbf9bde55b11cbd872"}, "2": {"node_id": "c25a9759-eb01-484a-87af-7787accef6c9", "node_type": null, "metadata": {"file_path": "docs/cli.md", "file_name": "cli.md"}, "hash": "1fdaf9836310bee99f3ac0720955ff5dc76ee0277cbeefe7b17b611bab0d198d"}, "3": {"node_id": "bb05c387-9ce5-459f-b950-31dfa2df0a26", "node_type": null, "metadata": {"file_path": "docs/cli.md", "file_name": "cli.md"}, "hash": "0b24ae3f7b37f14403116a40aaec4d6b0161da29a5fc5d8156beea0b92af1881"}}, "hash": "08e5978e60f7c607115051c0cc69e7d8ee3021a70c2b86d32112a5a2261d26a1", "text": "copy of a project.\n\n**Usage**\n\n```console\n$ nextflow drop [options] [project]\n```\n\n**Description**\n\nThe `drop` command is used to remove the projects which have been downloaded into the global cache. Please refer the `list` command for generating a list of downloaded pipelines.\n\n**Options**\n\n`-f`\n: Delete the repository without taking care of local changes.\n\n`-h, -help`\n: Print the command usage.\n\n**Examples**\n\nDrop the `nextflow-io/hello` project.\n\n```console\n$ nextflow drop nextflow-io/hello\n```\n\nForcefully drop the `nextflow-io/hello` pipeline, ignoring any local changes.\n\n```console\n$ nextflow drop nextflow-io/hello -f\n```\n\n### help\n\nPrint the top-level help or specific help for a command.\n\n**Usage**\n\n```console\n$ nextflow help [options] [command]\n```\n\n**Description**\n\nThe `help` command prints out the overview of the CLI interface and enumerates the top-level *options* and *commands*. Note that this command is equivalent to simply invoking `nextflow` at the command line.\n\n**Options**\n\n`-h, -help`\n: Print the command usage.\n\n**Examples**\n\nInvoke the `help` option for the `drop` command.\n\n```console\n$ nextflow help drop\n\nDelete the local copy of a project\nUsage: drop [options] name of the project to drop\n   Options:\n     -f\n          Delete the repository without taking care of local changes\n          Default: false\n     -h, -help\n          Print the command usage\n          Default: false\n```\n\n### info\n\nPrint project or system runtime information.\n\n**Usage**\n\n```console\n$ nextflow info [options] [project]\n```\n\n**Description**\n\nThe `info` command prints out the nextflow runtime information about the hardware as well as the software versions of the Nextflow version and build, operating system, and Groovy and Java runtime. It can also be used to display information about a specific project.\n\nIf no run name or session id is provided, it will clean the latest run.\n\n**Options**\n\n`-u, -check-updates`\n: Check for remote updates.\n\n`-d`\n: Show detailed information.\n\n`-h, -help`\n: Print the command usage.\n\n`-o` (`text`)\n: Output format, either `text`, `json` or `yaml`.\n\n**Examples**\n\nDisplay Nextflow runtime and system info:\n\n```console\n$ nextflow info\n\n  Version: 20.07.1 build 5412\n  Created: 24-07-2020 15:18 UTC (20:48 IDT)\n  System: Mac OS X 10.15.6\n  Runtime: Groovy 2.5.11 on OpenJDK 64-Bit Server VM 1.8.0_192-b01\n  Encoding: UTF-8 (UTF-8)\n```\n\nDisplay information about a specific project:\n\n```console\n$ nextflow info nextflow-io/hello\n\n  project name: nextflow-io/hello\n  repository  : https://github.com/nextflow-io/hello\n  local path  : /Users/evanfloden/.nextflow/assets/nextflow-io/hello\n  main script : main.nf\n  revisions   :\n  * master (default)\n    mybranch\n    testing\n    v1.1 [t]\n    v1.2 [t]\n```\n\n### kuberun\n\nLaunch a Nextflow pipeline on a Kubernetes cluster.\n\n**Usage**\n\n```console\n$ nextflow kuberun [options] [project]\n```\n\n**Description**\n\nThe `kuberun` command builds upon the `run` command and offers a deep integration with the Kubernetes", "start_char_idx": 11906, "end_char_idx": 14905, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "bb05c387-9ce5-459f-b950-31dfa2df0a26": {"__data__": {"id_": "bb05c387-9ce5-459f-b950-31dfa2df0a26", "embedding": null, "metadata": {"file_path": "docs/cli.md", "file_name": "cli.md"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "164ca94a0451118caab80ef9f9fe59d9b70e1ac7", "node_type": null, "metadata": {"file_path": "docs/cli.md", "file_name": "cli.md"}, "hash": "a3a18127b3c8b2dfe4351fbe845315df87f47e169d6540fbf9bde55b11cbd872"}, "2": {"node_id": "d3327c84-8696-4709-a1cb-f9cf4f353e60", "node_type": null, "metadata": {"file_path": "docs/cli.md", "file_name": "cli.md"}, "hash": "08e5978e60f7c607115051c0cc69e7d8ee3021a70c2b86d32112a5a2261d26a1"}, "3": {"node_id": "21f20236-84b5-40a0-a78f-956cefb72524", "node_type": null, "metadata": {"file_path": "docs/cli.md", "file_name": "cli.md"}, "hash": "1564d1c8d9f88f6949c12a81cafdce2c84cca8ac362aedbe2471768d3175a935"}}, "hash": "0b24ae3f7b37f14403116a40aaec4d6b0161da29a5fc5d8156beea0b92af1881", "text": "upon the `run` command and offers a deep integration with the Kubernetes execution environment. This command deploys the Nextflow runtime as a Kubernetes pod and assumes that you've already installed the `kubectl` CLI. The `kuberun` command does not allow the execution of local Nextflow scripts. For more information please refer to the {ref}`k8s-page` page.\n\n**Options**\n\nThe `kuberun` command supports the following options from [`run`](#run):\n\n- `-cache`\n- `-disable-jobs-cancellation`\n- `-dsl1`\n- `-dsl2`\n- `-dump-channels`\n- `-dump-hashes`\n- `-e.<key>=<value>`\n- `-entry`\n- `-h, -help`\n- `-hub`\n- `-latest`\n- `-main-script`\n- `-name`\n- `-offline`\n- `-params-file`\n- `-plugins`\n- `-preview`\n- `-process.<key>=<value>`\n- `-profile`\n- `-qs, -queue-size`\n- `-resume`\n- `-r, -revision`\n- `-stub, -stub-run`\n- `-user`\n- `-with-conda`\n- `-with-dag`\n- `-N, -with-notification`\n- `-with-report`\n- `-with-spack`\n- `-with-timeline`\n- `-with-tower`\n- `-with-trace`\n- `-with-wave`\n- `-with-weblog`\n- `-without-spack`\n- `-without-wave`\n- `-w, -work-dir`\n\nThe following new options are also available:\n\n`-head-cpus`\n: :::{versionadded} 22.01.0-edge\n  :::\n: Specify number of CPUs requested for the Nextflow pod.\n\n`-head-image`\n: :::{versionadded} 22.07.1-edge\n  :::\n: Specify the container image for the Nextflow driver pod.\n\n`-head-memory`\n: :::{versionadded} 22.01.0-edge\n  :::\n: Specify amount of memory requested for the Nextflow pod.\n\n`-head-prescript`\n: :::{versionadded} 22.05.0-edge\n  :::\n: Specify script to be run before the Nextflow pod starts.\n\n`-n, -namespace`\n: Specify the K8s namespace to use.\n\n`-remoteConfig`\n: Add the specified file from the K8s cluster to configuration set.\n\n`-remoteProfile`\n: Choose a configuration profile in the remoteConfig.\n\n`-v, -volume-mount`\n: Volume claim mounts, e.g. `my-pvc:/mnt/path`.\n\n**Examples**\n\nExecute a pipeline into a Kubernetes cluster.\n\n```console\n$ nextflow kuberun nextflow-io/hello\n```\n\n### list\n\nList all downloaded projects.\n\n**Usage**\n\n```console\n$ nextflow list [options]\n```\n\n**Description**\n\nThe `list` commands prints a list of the projects which are already downloaded into the global cache `~/.nextflow/assets`.\n\n**Options**\n\n`-h, -help`\n: Print the command usage.\n\n**Examples**\n\nList the downloaded pipelines.\n\n```console\n$ nextflow list\n\nnextflow-io/hello\nnextflow-hub/fastqc\n```\n\n### log\n\nPrint the execution history and log information.\n\n**Usage**\n\n```console\n$ nextflow log [options] [run_name | session_id]\n```\n\n**Description**\n\nThe `log` command is used to query the execution metadata associated with pipelines executed by Nextflow. The list of executed pipelines can be generated by running `nextflow log`. Instead of run name, it's also", "start_char_idx": 14897, "end_char_idx": 17607, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "21f20236-84b5-40a0-a78f-956cefb72524": {"__data__": {"id_": "21f20236-84b5-40a0-a78f-956cefb72524", "embedding": null, "metadata": {"file_path": "docs/cli.md", "file_name": "cli.md"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "164ca94a0451118caab80ef9f9fe59d9b70e1ac7", "node_type": null, "metadata": {"file_path": "docs/cli.md", "file_name": "cli.md"}, "hash": "a3a18127b3c8b2dfe4351fbe845315df87f47e169d6540fbf9bde55b11cbd872"}, "2": {"node_id": "bb05c387-9ce5-459f-b950-31dfa2df0a26", "node_type": null, "metadata": {"file_path": "docs/cli.md", "file_name": "cli.md"}, "hash": "0b24ae3f7b37f14403116a40aaec4d6b0161da29a5fc5d8156beea0b92af1881"}, "3": {"node_id": "c1000cdd-019c-41b7-8646-4c2dcb2f6b62", "node_type": null, "metadata": {"file_path": "docs/cli.md", "file_name": "cli.md"}, "hash": "991d2ff9c578d1c5136fc6831f048af73a52b197cc1f1fd30a2af992b03ede32"}}, "hash": "1564d1c8d9f88f6949c12a81cafdce2c84cca8ac362aedbe2471768d3175a935", "text": "can be generated by running `nextflow log`. Instead of run name, it's also possible to use a session id. Moreover, this command contains multiple options to facilitate the queries and is especially useful while debugging a pipeline and while inspecting pipeline execution metadata.\n\n**Options**\n\n`-after`\n: Show log entries for runs executed *after* the specified one.\n\n`-before`\n: Show log entries for runs executed *before* the specified one.\n\n`-but`\n: Show log entries for runs executed *but* the specified one.\n\n`-f, -fields`\n: Comma-separated list of fields to include in the printed log. Use the `-l` option to see the list of available fields.\n\n`-F, -filter`\n: Filter log entries by a custom expression, e.g. `process =~ /foo.*/ && status == 'COMPLETED'`.\n\n`-h, -help`\n: Print the command usage.\n\n`-l, -list-fields`\n: Show all available fields.\n\n`-quiet`\n: Show only run names.\n\n`-s`\n: Character used to separate column values.\n\n`-t, -template`\n: Text template used to each record in the log.\n\n**Examples**\n\nListing the execution logs of previous invocations of all pipelines in a project.\n\n```console\n$ nextflow log\n\nTIMESTAMP           DURATION        RUN NAME        STATUS  REVISION ID     SESSION ID                              COMMAND\n2020-10-07 11:52:24 2.1s            focused_payne   OK      96eb04d6a4      af6adaaa-ad4f-48a2-9f6a-b121e789adf5    nextflow run nextflow-io/hello -r master\n2020-10-07 11:53:00 3.1s            tiny_leavitt    OK      e3b475a61b      4d3b95c5-4385-42b6-b430-c865a70d56a4    nextflow run ./tutorial.nf\n2020-10-07 11:53:29 2.5s            boring_euler    OK      e3b475a61b      a6276975-7173-4208-ae09-ab9d6dce8737    nextflow run tutorial.nf\n```\n\nListing only the *run names* of the execution logs of all pipelines invocations in a project.\n\n```console\n$ nextflow log -quiet\n\nfocused_payne\ntiny_leavitt\nboring_euler\n```\n\nList the execution entries *only* a specific execution.\n\n```console\n$ nextflow log tiny_leavitt\n\nwork/1f/f1ea9158fb23b53d5083953121d6b6\nwork/bf/334115deec60929dc18edf0010032a\nwork/a3/06521d75da296d4dd7f4f8caaddad8\n```\n\nList the execution entries *after* a specific execution.\n\n```console\n$ nextflow log -after tiny_leavitt\n\nwork/92/c1a9cd9a96e0531d81ca69f5dc3bb7\nwork/3f/70944c7a549b6221e1ccc7b4b21b62\nwork/0e/2ebdba85f76f6068b21a1bcbf10cab\n```\n\nList the execution entries *before* a specific execution.\n\n```console\n$ nextflow log -before", "start_char_idx": 17607, "end_char_idx": 20014, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "c1000cdd-019c-41b7-8646-4c2dcb2f6b62": {"__data__": {"id_": "c1000cdd-019c-41b7-8646-4c2dcb2f6b62", "embedding": null, "metadata": {"file_path": "docs/cli.md", "file_name": "cli.md"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "164ca94a0451118caab80ef9f9fe59d9b70e1ac7", "node_type": null, "metadata": {"file_path": "docs/cli.md", "file_name": "cli.md"}, "hash": "a3a18127b3c8b2dfe4351fbe845315df87f47e169d6540fbf9bde55b11cbd872"}, "2": {"node_id": "21f20236-84b5-40a0-a78f-956cefb72524", "node_type": null, "metadata": {"file_path": "docs/cli.md", "file_name": "cli.md"}, "hash": "1564d1c8d9f88f6949c12a81cafdce2c84cca8ac362aedbe2471768d3175a935"}, "3": {"node_id": "17cb78f4-7d8d-4113-8e88-cde6b42ad9b9", "node_type": null, "metadata": {"file_path": "docs/cli.md", "file_name": "cli.md"}, "hash": "548308320216139aec82299a1f3bf43666291f90953aeaa9c68fd2bc167c86c2"}}, "hash": "991d2ff9c578d1c5136fc6831f048af73a52b197cc1f1fd30a2af992b03ede32", "text": "*before* a specific execution.\n\n```console\n$ nextflow log -before tiny_leavitt\n\nwork/5d/ad76f7b7ab3500cf616814ef644b61\nwork/c4/69a82b080a477612ba8d8e4c27b579\nwork/be/a4fa2aa38f76fd324958c81c2e4603\nwork/54/39116773891c47a91e3c1733aad4de\n```\n\nList the execution entries *except* for a specific execution.\n\n```console\n$ nextflow log -but tiny_leavitt\n\nwork/5d/ad76f7b7ab3500cf616814ef644b61\nwork/c4/69a82b080a477612ba8d8e4c27b579\nwork/be/a4fa2aa38f76fd324958c81c2e4603\nwork/54/39116773891c47a91e3c1733aad4de\n```\n\nFilter specific fields from the execution log of a process.\n\n```console\n$ nextflow log tiny_leavitt -f 'process,exit,hash,duration'\n\nsplitLetters        0       1f/f1ea91       112ms\nconvertToUpper      0       bf/334115       144ms\nconvertToUpper      0       a3/06521d       139ms\n```\n\nFilter fields from the execution log of a process based on a criteria.\n\n```console\n$ nextflow log tiny_leavitt -F 'process =~ /splitLetters/'\n\nwork/1f/f1ea9158fb23b53d5083953121d6b6\n```\n\n### pull\n\nDownload or update a project.\n\n**Usage**\n\n```console\n$ nextflow pull [options] [project]\n```\n\n**Description**\n\nThe `pull` command downloads a pipeline from a Git-hosting platform into the global cache `~/.nextflow/assets` and modifies it accordingly. For downloading a pipeline into a local directory, please refer to the `nextflow clone` command.\n\n**Options**\n\n`-all`\n: Update all downloaded projects.\n\n`-d, -deep`\n: Create a shallow clone of the specified depth.\n\n`-h, -help`\n: Print the command usage.\n\n`-hub` (`github`)\n: Service hub where the project is hosted. Options: `gitlab` or `bitbucket`\n\n`-r, -revision`\n: Revision of the project to run (either a git branch, tag or commit hash).\n: When passing a git tag or branch, the `workflow.revision` and `workflow.commitId` fields are populated. When passing only the commit hash, `workflow.revision` is not defined.\n\n`-user`\n: Private repository user name.\n\n**Examples**\n\nDownload a new pipeline or pull the latest revision for a specific project.\n\n```console\n$ nextflow pull nextflow-io/hello\n\nChecking nextflow-io/hello ...\ndone - revision: 96eb04d6a4 [master]\n```\n\nPull the latest revision for all downloaded projects.\n\n```console\n$ nextflow pull -all\n\nChecking nextflow-io/hello ...\ndone - revision: 96eb04d6a4 [master]\nChecking nextflow-hub/fastqc ...\ndone - revision: 087659b18e [master]\n```\n\nDownload a specific revision of a new project or pull the latest revision for a specific project.\n\n```console\n$ nextflow pull nextflow-io/hello -r v1.1\n\nChecking nextflow-io/hello ...\ncheckout-out at AnyObjectId[1c3e9e7404127514d69369cd87f8036830f5cf64]", "start_char_idx": 20018, "end_char_idx": 22619, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "17cb78f4-7d8d-4113-8e88-cde6b42ad9b9": {"__data__": {"id_": "17cb78f4-7d8d-4113-8e88-cde6b42ad9b9", "embedding": null, "metadata": {"file_path": "docs/cli.md", "file_name": "cli.md"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "164ca94a0451118caab80ef9f9fe59d9b70e1ac7", "node_type": null, "metadata": {"file_path": "docs/cli.md", "file_name": "cli.md"}, "hash": "a3a18127b3c8b2dfe4351fbe845315df87f47e169d6540fbf9bde55b11cbd872"}, "2": {"node_id": "c1000cdd-019c-41b7-8646-4c2dcb2f6b62", "node_type": null, "metadata": {"file_path": "docs/cli.md", "file_name": "cli.md"}, "hash": "991d2ff9c578d1c5136fc6831f048af73a52b197cc1f1fd30a2af992b03ede32"}, "3": {"node_id": "075a46ac-b470-4883-8720-389a85aa84ec", "node_type": null, "metadata": {"file_path": "docs/cli.md", "file_name": "cli.md"}, "hash": "96ddebc906b5a19724b2480318f8b0b3df4e66824f8b01df12cc07462515fd76"}}, "hash": "548308320216139aec82299a1f3bf43666291f90953aeaa9c68fd2bc167c86c2", "text": "- revision: 1c3e9e7404 [v1.1]\n```\n\n### run\n\nExecute a pipeline.\n\n**Usage**\n\n```console\n$ nextflow run [options] [project]\n```\n\n**Description**\n\nThe `run` command is used to execute a local pipeline script or remote pipeline project.\n\n**Options**\n\n`-E`\n: Exports all current system environment.\n\n`-ansi-log`\n: Enable/disable ANSI console logging.\n\n`-bucket-dir`\n: Remote bucket where intermediate result files are stored.\n\n`-cache`\n: Enable/disable processes caching.\n\n`-d, -deep`\n: Create a shallow clone of the specified depth.\n\n`-disable-jobs-cancellation`\n: Prevent the cancellation of child jobs on execution termination\n\n`-dsl1`\n: Execute the workflow using DSL1 syntax.\n\n`-dsl2`\n: Execute the workflow using DSL2 syntax.\n\n`-dump-channels`\n: Dump channels for debugging purpose.\n\n`-dump-hashes`\n: Dump task hash keys for debugging purpose.\n\n`-e.<key>=<value>`\n: Add the specified variable to execution environment.\n\n`-entry`\n: Entry workflow to be executed.\n\n`-h, -help`\n: Print the command usage.\n\n`-hub` (`github`)\n: Service hub where the project is hosted. Options: `gitlab` or `bitbucket`\n\n`-latest`\n: Pull latest changes before run.\n\n`-lib`\n: Library extension path.\n\n`-main-script` (`main.nf`)\n: :::{versionadded} 20.09.1-edge\n  :::\n: The script file to be executed when launching a project directory or repository.\n\n`-name`\n: Assign a mnemonic name to the a pipeline run.\n\n`-offline`\n: Do not check for remote project updates.\n\n`-params-file`\n: Load script parameters from a JSON/YAML file.\n\n`-plugins`\n: Comma separated list of plugin ids to be applied in the pipeline execution.\n\n`-preview`\n: :::{versionadded} 22.06.0-edge\n  :::\n: Run the workflow script skipping the execution of all processes\n\n`-process.<key>=<value>`\n: Set process config options.\n\n`-profile`\n: Choose a configuration profile.\n\n`-qs, -queue-size`\n: Max number of processes that can be executed in parallel by each executor.\n\n`-resume`\n: Execute the script using the cached results, useful to continue executions that was stopped by an error.\n\n`-r, -revision`\n: Revision of the project to run (either a git branch, tag or commit hash).\n: When passing a git tag or branch, the `workflow.revision` and `workflow.commitId` fields are populated. When passing only the commit hash, `workflow.revision` is not defined.\n\n`-stub-run, -stub`\n: Execute the workflow replacing process scripts with command stubs\n\n`-test`\n: Test a script function with the name specified.\n\n`-user`\n: Private repository user name.\n\n`-with-apptainer`\n: Enable process execution in an Apptainer container.\n\n`-with-charliecloud`\n: Enable process execution in a Charliecloud container.\n\n`-with-conda`\n: Use the specified Conda environment package or file (must end with `.yml` or `.yaml`)\n\n`-with-dag` (`dag.dot`)\n: Create pipeline DAG file.\n\n`-with-docker`\n: Enable process execution in a Docker container.\n\n`-N, -with-notification`\n: Send a notification email on workflow completion to the specified", "start_char_idx": 22679, "end_char_idx": 25630, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "075a46ac-b470-4883-8720-389a85aa84ec": {"__data__": {"id_": "075a46ac-b470-4883-8720-389a85aa84ec", "embedding": null, "metadata": {"file_path": "docs/cli.md", "file_name": "cli.md"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "164ca94a0451118caab80ef9f9fe59d9b70e1ac7", "node_type": null, "metadata": {"file_path": "docs/cli.md", "file_name": "cli.md"}, "hash": "a3a18127b3c8b2dfe4351fbe845315df87f47e169d6540fbf9bde55b11cbd872"}, "2": {"node_id": "17cb78f4-7d8d-4113-8e88-cde6b42ad9b9", "node_type": null, "metadata": {"file_path": "docs/cli.md", "file_name": "cli.md"}, "hash": "548308320216139aec82299a1f3bf43666291f90953aeaa9c68fd2bc167c86c2"}, "3": {"node_id": "0a92b93f-276b-4593-acb2-06b141803419", "node_type": null, "metadata": {"file_path": "docs/cli.md", "file_name": "cli.md"}, "hash": "f3ee0f9088654b3e6c81ce32b7dae1770d0eb60574001f6071c593073faeb343"}}, "hash": "96ddebc906b5a19724b2480318f8b0b3df4e66824f8b01df12cc07462515fd76", "text": "Send a notification email on workflow completion to the specified recipients.\n\n`-with-podman`\n: Enable process execution in a Podman container.\n\n`-with-report` (`report.html`)\n: Create workflow execution HTML report.\n\n`-with-singularity`\n: Enable process execution in a Singularity container.\n\n`-with-spack`\n: Use the specified Spack environment package or file (must end with `.yaml`)\n\n`-with-timeline` (`timeline.html`)\n: Create workflow execution timeline.\n\n`-with-tower`\n: Monitor workflow execution with [Tower](https://cloud.tower.nf/).\n\n`-with-trace` (`trace.txt`)\n: Create workflow execution trace file.\n\n`-with-wave`\n: Enable the use of Wave containers.\n\n`-with-weblog`\n: Send workflow status messages via HTTP to target URL.\n\n`-without-conda`\n: Disable process execution with Conda.\n\n`-without-docker`\n: Disable process execution with Docker.\n\n`-without-podman`\n: Disable process execution in a Podman container.\n\n`-without-spack`\n: Disable process execution with Spack.\n\n`-without-wave`\n: Disable the use of Wave containers.\n\n`-w, -work-dir` (`work`)\n: Directory where intermediate result files are stored.\n\n**Examples**\n\n- Run a specific revision of a remote pipeline.\n\n  ```console\n  $ nextflow run nextflow-io/hello -r v1.1\n\n  N E X T F L O W  ~  version 20.07.1\n  Launching `nextflow-io/hello` [grave_cajal] - revision: 1c3e9e7404 [v1.1]\n  ```\n\n- Choose a `profile` for running the project. Assumes that a profile named `docker` has already been defined in the config file.\n\n  ```console\n  $ nextflow run main.nf -profile docker\n  ```\n\n- Execute a pipeline and generate the summary HTML report. For more information on the metrics, please refer the {ref}`tracing-page` section:\n\n  ```console\n  $ nextflow run main.nf -with-report\n  ```\n\n- Execute a pipeline with a custom queue size. By default, the queue size is the number of available CPUs.\n\n  ```console\n  $ nextflow run nextflow-io/hello -qs 4\n  ```\n\n- Execute the pipeline with DSL-2 syntax.\n\n  ```console\n  $ nextflow run nextflow-io/hello -dsl2\n  ```\n\n- Execute a pipeline with a specific workflow as the entry-point, this option is meant to be used with DSL-2. For more information on DSL-2, please refer to {ref}`dsl2-page`\n\n  ```console\n  $ nextflow run main.nf -entry workflow_A\n  ```\n\n- Execute a pipeline with integrated monitoring in [Tower](https://cloud.tower.nf).\n\n  ```console\n  $ nextflow run nextflow-io/hello -with-tower\n  ```\n\n- Execute a pipeline with a custom parameters file (YAML or JSON).\n\n  ```console\n  $ nextflow run main.nf -params-file pipeline_params.yml\n  ```\n\n  For example, the following params file in YAML format:\n\n  ```yaml\n  alpha: 1\n  beta: 'foo'\n  ```\n\n  Or in JSON format:\n\n  ```json\n  {\n    \"alpha\": 1,\n    \"beta\": \"foo\"\n  }\n  ```\n\n  Is equivalent to the following command line:\n\n  ```console\n  $ nextflow run main.nf --alpha 1 --beta foo\n  ```\n\n  The parameters specified with this mechanism are merged with the resolved configuration (base configuration and", "start_char_idx": 25575, "end_char_idx": 28544, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "0a92b93f-276b-4593-acb2-06b141803419": {"__data__": {"id_": "0a92b93f-276b-4593-acb2-06b141803419", "embedding": null, "metadata": {"file_path": "docs/cli.md", "file_name": "cli.md"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "164ca94a0451118caab80ef9f9fe59d9b70e1ac7", "node_type": null, "metadata": {"file_path": "docs/cli.md", "file_name": "cli.md"}, "hash": "a3a18127b3c8b2dfe4351fbe845315df87f47e169d6540fbf9bde55b11cbd872"}, "2": {"node_id": "075a46ac-b470-4883-8720-389a85aa84ec", "node_type": null, "metadata": {"file_path": "docs/cli.md", "file_name": "cli.md"}, "hash": "96ddebc906b5a19724b2480318f8b0b3df4e66824f8b01df12cc07462515fd76"}, "3": {"node_id": "0013b21a-8efc-4460-8593-1d48294c47df", "node_type": null, "metadata": {"file_path": "docs/cli.md", "file_name": "cli.md"}, "hash": "30460460d2ccbe8f7c82e2bad5299980a7f7dfe71cc58fa67ecd27cd54a695f4"}}, "hash": "f3ee0f9088654b3e6c81ce32b7dae1770d0eb60574001f6071c593073faeb343", "text": "specified with this mechanism are merged with the resolved configuration (base configuration and profiles). The values provided via a params file overwrite those of the same name in the Nextflow configuration file.\n\n### self-update\n\nUpdate the nextflow runtime to the latest available version.\n\n**Usage**\n\n```console\n$ nextflow self-update\n```\n\n**Description**\n\nThe `self-update` command directs the `nextflow` CLI to update itself to the latest stable release.\n\n**Examples**\n\nUpdate Nextflow.\n\n```console\n$ nextflow self-update\n\n      N E X T F L O W\n      version 20.07.1 build 5412\n      created 24-07-2020 15:18 UTC (20:48 IDT)\n      cite doi:10.1038/nbt.3820\n      http://nextflow.io\n\n\nNextflow installation completed. Please note:\n- the executable file `nextflow` has been created in the folder: /usr/local/bin\n```\n\n### view\n\nView a project's script file(s).\n\n**Usage**\n\n```console\n$ nextflow view [options] [project]\n```\n\n**Description**\n\nThe `view` command is used to inspect the pipelines that are already stored in the global nextflow cache. For downloading a pipeline into the global cache `~/.nextflow/assets`, refer to the `pull` command.\n\n**Options**\n\n`-h, -help`\n: Print the command usage.\n\n`-l`\n: List repository content.\n\n`-q`\n: Hide header line.\n\n**Examples**\n\nViewing the contents of a downloaded pipeline.\n\n```console\n$ nextflow view nextflow-io/hello\n\n== content of file: .nextflow/assets/nextflow-io/hello/main.nf\n#!/usr/bin/env nextflow\nnextflow.enable.dsl=2\n\nprocess sayHello {\n  input:\n    val x\n  output:\n    stdout\n  script:\n    \"\"\"\n    echo '$x world!'\n    \"\"\"\n}\n\nworkflow {\n  Channel.of('Bonjour', 'Ciao', 'Hello', 'Hola') | sayHello | view\n}\n```\n\nList the folder structure of the downloaded pipeline:\n\n```console\n$ nextflow view -l nextflow-io/hello\n\n== content of path: .nextflow/assets/nextflow-io/hello\nLICENSE\nREADME.md\nnextflow.config\n.gitignore\ncircle.yml\nfoo.nf\n.git\n.travis.yml\nmain.nf\n```\n\nView the contents of a downloaded pipeline without omitting the header:\n\n```console\n$ nextflow view -q nextflow-io/hello\n\n#!/usr/bin/env nextflow\nnextflow.enable.dsl=2\n\nprocess sayHello {\n  input:\n    val x\n  output:\n    stdout\n  script:\n    \"\"\"\n    echo '$x world!'\n    \"\"\"\n}\n\nworkflow {\n  Channel.of('Bonjour', 'Ciao', 'Hello', 'Hola') | sayHello | view\n}\n```\n\n(cli-params)=\n\n## Pipeline parameters\n\nPipeline scripts can use an arbitrary number of parameters that can be overridden, either using the command line or the Nextflow configuration file. Any script parameter can be specified on the command line, prefixing the parameter name with double dash characters, e.g.:\n\n```bash\nnextflow run <my script> --foo Hello\n```\n\nThen, the parameter can be accessed in the pipeline script using the `params.foo` identifier.\n\n:::{note}\nWhen the parameter name is formatted using `camelCase`, a second parameter is created with the same value using `kebab-case`, and vice versa.\n:::\n\n:::{warning}\nWhen a command line", "start_char_idx": 28517, "end_char_idx": 31455, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "0013b21a-8efc-4460-8593-1d48294c47df": {"__data__": {"id_": "0013b21a-8efc-4460-8593-1d48294c47df", "embedding": null, "metadata": {"file_path": "docs/cli.md", "file_name": "cli.md"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "164ca94a0451118caab80ef9f9fe59d9b70e1ac7", "node_type": null, "metadata": {"file_path": "docs/cli.md", "file_name": "cli.md"}, "hash": "a3a18127b3c8b2dfe4351fbe845315df87f47e169d6540fbf9bde55b11cbd872"}, "2": {"node_id": "0a92b93f-276b-4593-acb2-06b141803419", "node_type": null, "metadata": {"file_path": "docs/cli.md", "file_name": "cli.md"}, "hash": "f3ee0f9088654b3e6c81ce32b7dae1770d0eb60574001f6071c593073faeb343"}}, "hash": "30460460d2ccbe8f7c82e2bad5299980a7f7dfe71cc58fa67ecd27cd54a695f4", "text": "and vice versa.\n:::\n\n:::{warning}\nWhen a command line parameter includes one or more glob characters, i.e. wildcards like `*` or `?`, the parameter value must be enclosed in quotes to prevent Bash expansion and preserve the glob characters. For example:\n\n```bash\nnextflow run <my script> --files \"*.fasta\"\n```\n:::", "start_char_idx": 31486, "end_char_idx": 31799, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "82bce502-5b2d-40bf-a61b-69dec53a6a8b": {"__data__": {"id_": "82bce502-5b2d-40bf-a61b-69dec53a6a8b", "embedding": null, "metadata": {"file_path": "docs/conda.md", "file_name": "conda.md"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "b4f05092efb2fb768e7b064ec7f3c0bbe80f2688", "node_type": null, "metadata": {"file_path": "docs/conda.md", "file_name": "conda.md"}, "hash": "697f7401ad8990992f82bffa275503bd0523e34a89c4eadb5645572896c4ad34"}, "3": {"node_id": "b9abe666-6658-45ca-bf79-1dcc42eeef00", "node_type": null, "metadata": {"file_path": "docs/conda.md", "file_name": "conda.md"}, "hash": "55cbcd322374174872f21dc086b8c3e794dc66033bb647d6dc077c2ba1ff05ad"}}, "hash": "a6698392ec6b339cf6fdfb6f58cb57c0bc85b03f9c6faf3c0c32b518036f9b92", "text": "(conda-page)=\n\n# Conda environments\n\n[Conda](https://conda.io/) is an open source package and environment management system that simplifies the installation and the configuration of complex software packages in a platform agnostic manner.\n\nNextflow has built-in support for Conda that allows the configuration of workflow dependencies using Conda recipes and environment files.\n\nThis allows Nextflow applications to use popular tool collections such as [Bioconda](https://bioconda.github.io) whilst taking advantage of the configuration flexibility provided by Nextflow.\n\n## Prerequisites\n\nThis feature requires the Conda or [Miniconda](https://conda.io/miniconda.html) package manager to be installed on your system.\n\n## How it works\n\nNextflow automatically creates and activates the Conda environment(s) given the dependencies specified by each process.\n\nDependencies are specified by using the {ref}`process-conda` directive, providing either the names of the required Conda packages, the path of a Conda environment yaml file or the path of an existing Conda environment directory.\n\n:::{note}\nConda environments are stored on the file system. By default Nextflow instructs Conda to save the required environments in the pipeline work directory. Therefore the same environment can be created/saved multiple times across multiple executions when using a different work directory.\n:::\n\nYou can specify the directory where the Conda environments are stored using the `conda.cacheDir` configuration property (see the {ref}`configuration page <config-conda>` for details). When using a computing cluster, make sure to use a shared file system path accessible from all compute nodes.\n\n:::{warning}\nThe Conda environment feature is not supported by executors that use remote object storage as a work directory e.g. AWS Batch.\n:::\n\n### Enabling Conda environment\n\n:::{versionadded} 22.08.0-edge\n:::\n\nThe use of Conda recipes specified using the {ref}`process-conda` directive needs to be enabled explicitly by setting the option shown below in the pipeline configuration file (i.e. `nextflow.config`):\n\n```groovy\nconda.enabled = true\n```\n\nAlternatively, it can be specified by setting the variable `NXF_CONDA_ENABLED=true` in your environment or by using the `-with-conda` command line option.\n\n### Use Conda package names\n\nConda package names can specified using the `conda` directive. Multiple package names can be specified by separating them with a blank space. For example:\n\n```groovy\nprocess foo {\n  conda 'bwa samtools multiqc'\n\n  '''\n  your_command --here\n  '''\n}\n```\n\nUsing the above definition, a Conda environment that includes BWA, Samtools and MultiQC tools is created and activated when the process is executed.\n\nThe usual Conda package syntax and naming conventions can be used. The version of a package can be specified after the package name as shown here `bwa=0.7.15`.\n\nThe name of the channel where a package is located can be specified prefixing the package with the channel name as shown here `bioconda::bwa=0.7.15`.\n\n### Use Conda environment files\n\nConda environments can also be defined using one or more Conda environment files. This is a file that lists the required packages and channels structured using the YAML format. For example:\n\n```yaml\nname: my-env\nchannels:\n  - conda-forge\n  - bioconda\n  - defaults\ndependencies:\n  - star=2.5.4a\n  - bwa=0.7.15\n```\n\nRead the Conda documentation for more details about how to create [environment files](https://conda.io/docs/user-guide/tasks/manage-environments.html#creating-an-environment-file-manually).\n\nThe path of an environment file can be specified using the `conda` directive:\n\n```groovy\nprocess foo {\n  conda", "start_char_idx": 0, "end_char_idx": 3682, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "b9abe666-6658-45ca-bf79-1dcc42eeef00": {"__data__": {"id_": "b9abe666-6658-45ca-bf79-1dcc42eeef00", "embedding": null, "metadata": {"file_path": "docs/conda.md", "file_name": "conda.md"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "b4f05092efb2fb768e7b064ec7f3c0bbe80f2688", "node_type": null, "metadata": {"file_path": "docs/conda.md", "file_name": "conda.md"}, "hash": "697f7401ad8990992f82bffa275503bd0523e34a89c4eadb5645572896c4ad34"}, "2": {"node_id": "82bce502-5b2d-40bf-a61b-69dec53a6a8b", "node_type": null, "metadata": {"file_path": "docs/conda.md", "file_name": "conda.md"}, "hash": "a6698392ec6b339cf6fdfb6f58cb57c0bc85b03f9c6faf3c0c32b518036f9b92"}}, "hash": "55cbcd322374174872f21dc086b8c3e794dc66033bb647d6dc077c2ba1ff05ad", "text": "directive:\n\n```groovy\nprocess foo {\n  conda '/some/path/my-env.yaml'\n\n  '''\n  your_command --here\n  '''\n}\n```\n\n:::{warning}\nThe environment file name **must** have a `.yml` or `.yaml` extension or else it won't be properly recognised.\n:::\n\nAlternatively, it is possible to provide the dependencies using a plain text file, just listing each package name as a separate line. For example:\n\n```\nbioconda::star=2.5.4a\nbioconda::bwa=0.7.15\nbioconda::multiqc=1.4\n```\n\n:::{warning}\nLike before, the extension matters. Make sure the dependencies file has a `.txt` extension.\n:::\n\n### Use existing Conda environments\n\nIf you already have a local Conda environment, you can use it in your workflow specifying the installation directory of such environment by using the `conda` directive:\n\n```groovy\nprocess foo {\n  conda '/path/to/an/existing/env/directory'\n\n  '''\n  your_command --here\n  '''\n}\n```\n\n### Use Mamba to resolve packages\n\n:::{warning} *Experimental: may change in a future release.*\n:::\n\nIt is also possible to use [mamba](https://github.com/mamba-org/mamba) to speed up the creation of conda environments. For more information on how to enable this feature please refer to {ref}`Conda <config-conda>`.\n\n## Best practices\n\nWhen a `conda` directive is used in any `process` definition within the workflow script, Conda tool is required for the workflow execution.\n\nSpecifying the Conda environments in a separate configuration {ref}`profile <config-profiles>` is therefore recommended to allow the execution via a command line option and to enhance the workflow portability. For example:\n\n```groovy\nprofiles {\n  conda {\n    process.conda = 'samtools'\n  }\n\n  docker {\n    process.container = 'biocontainers/samtools'\n    docker.enabled = true\n  }\n}\n```\n\nThe above configuration snippet allows the execution either with Conda or Docker specifying `-profile conda` or `-profile docker` when running the workflow script.\n\n## Advanced settings\n\nConda advanced configuration settings are described in the {ref}`Conda <config-conda>` section on the Nextflow configuration page.", "start_char_idx": 3639, "end_char_idx": 5711, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "5902745f-43f5-4e02-8ffe-4c32bb240d27": {"__data__": {"id_": "5902745f-43f5-4e02-8ffe-4c32bb240d27", "embedding": null, "metadata": {"file_path": "docs/config.md", "file_name": "config.md"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "5f43dd876f21e8b1a4c175d44180bc9a99b46e3d", "node_type": null, "metadata": {"file_path": "docs/config.md", "file_name": "config.md"}, "hash": "8847079eb331e85ef73b38aefeee43dae1d4f4e09a7d20e4f059d4a72a73f105"}, "3": {"node_id": "13e2b4fd-a55e-40f4-aea0-0a6a21ce5adc", "node_type": null, "metadata": {"file_path": "docs/config.md", "file_name": "config.md"}, "hash": "2e7a41837043cf48289d21ed4db61156e8c505e7e5302810b90529fac3bb1d99"}}, "hash": "750282f02fdacc4ce893528154a28638734120287a4ba0803fa90cd3646edee4", "text": "(config-page)=\n\n# Configuration\n\n## Configuration file\n\nWhen a pipeline script is launched, Nextflow looks for configuration files in multiple locations. Since each configuration file can contain conflicting settings, the sources are ranked to determine which settings are applied. Possible configuration sources, in order of priority:\n\n1. Parameters specified on the command line (`--something value`)\n2. Parameters provided using the `-params-file` option\n3. Config file specified using the `-c my_config` option\n4. The config file named `nextflow.config` in the current directory\n5. The config file named `nextflow.config` in the workflow project directory\n6. The config file `$HOME/.nextflow/config`\n7. Values defined within the pipeline script itself (e.g. `main.nf`)\n\nWhen more than one of these options for specifying configurations are used, they are merged, so that the settings in the first override the same settings appearing in the second, and so on.\n\n:::{tip}\nIf you want to ignore any default configuration files and use only a custom one, use `-C <config file>`.\n:::\n\n### Config syntax\n\nA Nextflow configuration file is a simple text file containing a set of properties defined using the syntax:\n\n```groovy\nname = value\n```\n\nPlease note, string values need to be wrapped in quotation characters while numbers and boolean values (`true`, `false`) do not. Also note that values are typed. This means that, for example, `1` is different from `'1'` \u2014 the former is interpreted as the number one, while the latter is interpreted as a string value.\n\n### Config variables\n\nConfiguration properties can be used as variables in the configuration file by using the usual `$propertyName` or `${expression}` syntax.\n\nFor example:\n\n```groovy\npropertyOne = 'world'\nanotherProp = \"Hello $propertyOne\"\ncustomPath = \"$PATH:/my/app/folder\"\n```\n\nPlease note, the usual rules for {ref}`string-interpolation` are applied, thus a string containing a variable reference must be wrapped in double-quote chars instead of single-quote chars.\n\nThe same mechanism allows you to access environment variables defined in the hosting system. Any variable name not defined in the Nextflow configuration file(s) is interpreted to be a reference to an environment variable with that name. So, in the above example, the property `customPath` is defined as the current system `PATH` to which the string `/my/app/folder` is appended.\n\n### Config comments\n\nConfiguration files use the same conventions for comments used by the Groovy or Java programming languages. Thus, use `//` to comment a single line, or `/*` .. `*/` to comment a block on multiple lines.\n\n### Config include\n\nA configuration file can include one or more configuration files using the keyword `includeConfig`. For example:\n\n```groovy\nprocess.executor = 'sge'\nprocess.queue = 'long'\nprocess.memory = '10G'\n\nincludeConfig 'path/foo.config'\n```\n\nWhen a relative path is used, it is resolved against the actual location of the including file.\n\n## Config scopes\n\nConfiguration settings can be organized in different scopes by dot prefixing the property names with a scope identifier, or grouping the properties in the same scope using the curly brackets notation. For example:\n\n```groovy\nalpha.x = 1\nalpha.y = 'string value..'\n\nbeta {\n     p = 2\n     q = 'another string ..'\n}\n```\n\n(config-apptainer)=\n\n### Scope `apptainer`\n\nThe `apptainer` scope controls how [Apptainer](https://apptainer.org) containers are executed by Nextflow.\n\nThe following settings are available:\n\n`apptainer.autoMounts`\n: When `true` Nextflow automatically mounts host paths in the executed container. It requires the `user bind control` feature to be enabled in your Apptainer installation (default:", "start_char_idx": 0, "end_char_idx": 3719, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "13e2b4fd-a55e-40f4-aea0-0a6a21ce5adc": {"__data__": {"id_": "13e2b4fd-a55e-40f4-aea0-0a6a21ce5adc", "embedding": null, "metadata": {"file_path": "docs/config.md", "file_name": "config.md"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "5f43dd876f21e8b1a4c175d44180bc9a99b46e3d", "node_type": null, "metadata": {"file_path": "docs/config.md", "file_name": "config.md"}, "hash": "8847079eb331e85ef73b38aefeee43dae1d4f4e09a7d20e4f059d4a72a73f105"}, "2": {"node_id": "5902745f-43f5-4e02-8ffe-4c32bb240d27", "node_type": null, "metadata": {"file_path": "docs/config.md", "file_name": "config.md"}, "hash": "750282f02fdacc4ce893528154a28638734120287a4ba0803fa90cd3646edee4"}, "3": {"node_id": "4416d839-f94b-45e7-9dbf-c1516926babf", "node_type": null, "metadata": {"file_path": "docs/config.md", "file_name": "config.md"}, "hash": "2e50afeb81e8744586cd628c0231e590782fb08b661b566c5a2b1e568e33a5ca"}}, "hash": "2e7a41837043cf48289d21ed4db61156e8c505e7e5302810b90529fac3bb1d99", "text": "the `user bind control` feature to be enabled in your Apptainer installation (default: `false`).\n\n`apptainer.cacheDir`\n: The directory where remote Apptainer images are stored. When using a computing cluster it must be a shared folder accessible to all compute nodes.\n\n`apptainer.enabled`\n: Enable Apptainer execution (default: `false`).\n\n`apptainer.engineOptions`\n: This attribute can be used to provide any option supported by the Apptainer engine i.e. `apptainer [OPTIONS]`.\n\n`apptainer.envWhitelist`\n: Comma separated list of environment variable names to be included in the container environment.\n\n`apptainer.noHttps`\n: Pull the Apptainer image with http protocol (default: `false`).\n\n`apptainer.pullTimeout`\n: The amount of time the Apptainer pull can last, exceeding which the process is terminated (default: `20 min`).\n\n`apptainer.registry`\n: The registry from where Docker images are pulled. It should be only used to specify a private registry server. It should NOT include the protocol prefix i.e. `http://`.\n\n`apptainer.runOptions`\n: This attribute can be used to provide any extra command line options supported by `apptainer exec`.\n\nRead the {ref}`container-apptainer` page to learn more about how to use Apptainer containers with Nextflow.\n\n(config-aws)=\n\n### Scope `aws`\n\nThe `aws` scope controls the interactions with AWS, including AWS Batch and S3. For example:\n\n```groovy\naws {\n    accessKey = '<YOUR S3 ACCESS KEY>'\n    secretKey = '<YOUR S3 SECRET KEY>'\n    region = 'us-east-1'\n\n    client {\n        maxConnections = 20\n        connectionTimeout = 10000\n        uploadStorageClass = 'INTELLIGENT_TIERING'\n        storageEncryption = 'AES256'\n    }\n    batch {\n        cliPath = '/home/ec2-user/miniconda/bin/aws'\n        maxTransferAttempts = 3\n        delayBetweenAttempts = '5 sec'\n    }\n}\n```\n\n:::{tip}\nThis scope can also be used to configure access to S3-compatible storage outside of AWS, such as [Ceph](https://ceph.com/en/) and [MinIO](https://min.io/).\n:::\n\nRead the {ref}`aws-page` and {ref}`amazons3-page` pages for more information.\n\nThe following settings are available:\n\n`aws.accessKey`\n: AWS account access key\n\n`aws.profile`\n: :::{versionadded} 22.12.0-edge\n  :::\n: AWS profile from `~/.aws/credentials`\n\n`aws.region`\n: AWS region (e.g. `us-east-1`)\n\n`aws.secretKey`\n: AWS account secret key\n\n`aws.batch.cliPath`\n: The path where the AWS command line tool is installed in the host AMI.\n\n`aws.batch.delayBetweenAttempts`\n: Delay between download attempts from S3 (default: `10 sec`).\n\n`aws.batch.jobRole`\n: The AWS Job Role ARN that needs to be used to execute the Batch Job.\n\n`aws.batch.logsGroup`\n: :::{versionadded} 22.09.0-edge\n  :::\n: The name of the logs group used by Batch Jobs (default: `/aws/batch`).\n\n`aws.batch.maxParallelTransfers`\n: Max parallel upload/download transfer operations *per job* (default: `4`).\n\n`aws.batch.maxSpotAttempts`\n: :::{versionadded} 22.04.0\n  :::\n: Max", "start_char_idx": 3646, "end_char_idx": 6574, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "4416d839-f94b-45e7-9dbf-c1516926babf": {"__data__": {"id_": "4416d839-f94b-45e7-9dbf-c1516926babf", "embedding": null, "metadata": {"file_path": "docs/config.md", "file_name": "config.md"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "5f43dd876f21e8b1a4c175d44180bc9a99b46e3d", "node_type": null, "metadata": {"file_path": "docs/config.md", "file_name": "config.md"}, "hash": "8847079eb331e85ef73b38aefeee43dae1d4f4e09a7d20e4f059d4a72a73f105"}, "2": {"node_id": "13e2b4fd-a55e-40f4-aea0-0a6a21ce5adc", "node_type": null, "metadata": {"file_path": "docs/config.md", "file_name": "config.md"}, "hash": "2e7a41837043cf48289d21ed4db61156e8c505e7e5302810b90529fac3bb1d99"}, "3": {"node_id": "4c5bfd13-0e96-4acb-b7b9-27afecb12995", "node_type": null, "metadata": {"file_path": "docs/config.md", "file_name": "config.md"}, "hash": "a9546d5795dbad8ed1e17b97ba4d5e818aed273b1c6af1b8f45adb719c404bed"}}, "hash": "2e50afeb81e8744586cd628c0231e590782fb08b661b566c5a2b1e568e33a5ca", "text": ":::{versionadded} 22.04.0\n  :::\n: Max number of execution attempts of a job interrupted by a EC2 spot reclaim event (default: `5`)\n\n`aws.batch.maxTransferAttempts`\n: Max number of downloads attempts from S3 (default: `1`).\n\n`aws.batch.retryMode`\n: The retry mode configuration setting, to accommodate rate-limiting on [AWS services](https://docs.aws.amazon.com/cli/latest/userguide/cli-configure-retries.html) (default: `standard`)\n\n`aws.batch.schedulingPriority`\n: :::{versionadded} 23.01.0-edge\n  :::\n: The scheduling priority for all tasks when using [fair-share scheduling for AWS Batch](https://aws.amazon.com/blogs/hpc/introducing-fair-share-scheduling-for-aws-batch/) (default: `0`)\n\n`aws.batch.shareIdentifier`\n: :::{versionadded} 22.09.0-edge\n  :::\n: The share identifier for all tasks when using [fair-share scheduling for AWS Batch](https://aws.amazon.com/blogs/hpc/introducing-fair-share-scheduling-for-aws-batch/)\n\n`aws.batch.volumes`\n: One or more container mounts. Mounts can be specified as simple e.g. `/some/path` or canonical format e.g. `/host/path:/mount/path[:ro|rw]`. Multiple mounts can be specified separating them with a comma or using a list object.\n\n`aws.client.anonymous`\n: Allow the access of public S3 buckets without the need to provide AWS credentials. Any service that does not accept unsigned requests will return a service access error.\n\n`aws.client.s3Acl`\n: Allow the setting of predefined bucket permissions, also known as *canned ACL*. Permitted values are `Private`, `PublicRead`, `PublicReadWrite`, `AuthenticatedRead`, `LogDeliveryWrite`, `BucketOwnerRead`, `BucketOwnerFullControl`, and `AwsExecRead`. See [Amazon docs](https://docs.aws.amazon.com/AmazonS3/latest/userguide/acl-overview.html#canned-acl) for details.\n\n`aws.client.connectionTimeout`\n: The amount of time to wait (in milliseconds) when initially establishing a connection before timing out.\n\n`aws.client.endpoint`\n: The AWS S3 API entry point e.g. `s3-us-west-1.amazonaws.com`.\n\n`aws.client.glacierAutoRetrieval`\n: :::{versionadded} 22.12.0-edge\n  :::\n: *Experimental: may change in a future release.*\n: Enable auto retrieval of S3 objects stored with Glacier class store (default: `false`).\n\n`aws.client.glacierExpirationDays`\n: :::{versionadded} 22.12.0-edge\n  :::\n: *Experimental: may change in a future release.*\n: The time, in days, between when an object is restored to the bucket and when it expires (default: `7`).\n\n`aws.client.glacierRetrievalTier`\n: :::{versionadded} 23.03.0-edge\n  :::\n: *Experimental: may change in a future release.*\n: The retrieval tier to use when restoring objects from Glacier, one of [`Expedited`, `Standard`, `Bulk`].\n\n`aws.client.maxConnections`\n: The maximum number of allowed open HTTP connections.\n\n`aws.client.maxErrorRetry`\n: The maximum number of retry attempts for failed retryable requests.\n\n`aws.client.protocol`\n: The protocol (i.e. HTTP or HTTPS) to use when connecting to AWS.\n\n`aws.client.proxyHost`\n: The proxy host to connect through.\n\n`aws.client.proxyPort`\n: The port on the proxy host to connect", "start_char_idx": 6616, "end_char_idx": 9674, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "4c5bfd13-0e96-4acb-b7b9-27afecb12995": {"__data__": {"id_": "4c5bfd13-0e96-4acb-b7b9-27afecb12995", "embedding": null, "metadata": {"file_path": "docs/config.md", "file_name": "config.md"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "5f43dd876f21e8b1a4c175d44180bc9a99b46e3d", "node_type": null, "metadata": {"file_path": "docs/config.md", "file_name": "config.md"}, "hash": "8847079eb331e85ef73b38aefeee43dae1d4f4e09a7d20e4f059d4a72a73f105"}, "2": {"node_id": "4416d839-f94b-45e7-9dbf-c1516926babf", "node_type": null, "metadata": {"file_path": "docs/config.md", "file_name": "config.md"}, "hash": "2e50afeb81e8744586cd628c0231e590782fb08b661b566c5a2b1e568e33a5ca"}, "3": {"node_id": "eb6668ea-4b84-4d25-a38b-351a718b8eed", "node_type": null, "metadata": {"file_path": "docs/config.md", "file_name": "config.md"}, "hash": "5e8c86c2e0043e4882956fb460023442a76e3e77b79ef25f7da1197e858126f5"}}, "hash": "a9546d5795dbad8ed1e17b97ba4d5e818aed273b1c6af1b8f45adb719c404bed", "text": "The port on the proxy host to connect through.\n\n`aws.client.proxyUsername`\n: The user name to use when connecting through a proxy.\n\n`aws.client.proxyPassword`\n: The password to use when connecting through a proxy.\n\n`aws.client.s3PathStyleAccess`\n: Enable the use of path-based access model that is used to specify the address of an object in S3-compatible storage systems.\n\n`aws.client.signerOverride`\n: The name of the signature algorithm to use for signing requests made by the client.\n\n`aws.client.socketSendBufferSizeHint`\n: The Size hint (in bytes) for the low level TCP send buffer.\n\n`aws.client.socketRecvBufferSizeHint`\n: The Size hint (in bytes) for the low level TCP receive buffer.\n\n`aws.client.socketTimeout`\n: The amount of time to wait (in milliseconds) for data to be transferred over an established, open connection before the connection is timed out.\n\n`aws.client.storageEncryption`\n: The S3 server side encryption to be used when saving objects on S3, either `AES256` or `aws:kms` values are allowed.\n\n`aws.client.storageKmsKeyId`\n: :::{versionadded} 22.05.0-edge\n  :::\n: The AWS KMS key Id to be used to encrypt files stored in the target S3 bucket ().\n\n`aws.client.userAgent`\n: The HTTP user agent header passed with all HTTP requests.\n\n`aws.client.uploadChunkSize`\n: The size of a single part in a multipart upload (default: `100 MB`).\n\n`aws.client.uploadMaxAttempts`\n: The maximum number of upload attempts after which a multipart upload returns an error (default: `5`).\n\n`aws.client.uploadMaxThreads`\n: The maximum number of threads used for multipart upload.\n\n`aws.client.uploadRetrySleep`\n: The time to wait after a failed upload attempt to retry the part upload (default: `500ms`).\n\n`aws.client.uploadStorageClass`\n: The S3 storage class applied to stored objects, one of \\[`STANDARD`, `STANDARD_IA`, `ONEZONE_IA`, `INTELLIGENT_TIERING`\\] (default: `STANDARD`).\n\n(config-azure)=\n\n### Scope `azure`\n\nThe `azure` scope allows you to configure the interactions with Azure, including Azure Batch and Azure Blob Storage.\n\nRead the {ref}`azure-page` page for more information.\n\nThe following settings are available:\n\n`azure.activeDirectory.servicePrincipalId`\n: The service principal client ID\n\n`azure.activeDirectory.servicePrincipalSecret`\n: The service principal client secret\n\n`azure.activeDirectory.tenantId`\n: The Azure tenant ID\n\n`azure.batch.accountName`\n: The batch service account name.\n\n`azure.batch.accountKey`\n: The batch service account key.\n\n`azure.batch.allowPoolCreation`\n: Enable the automatic creation of batch pools specified in the Nextflow configuration file (default: `false`).\n\n`azure.batch.autoPoolMode`\n: Enable the automatic creation of batch pools depending on the pipeline resources demand (default: `true`).\n\n`azure.batch.copyToolInstallMode`\n: Specify where the `azcopy` tool used by Nextflow. When `node` is specified it's copied once during the pool creation. When `task` is provider, it's installed for each task execution (default: `node`).\n\n`azure.batch.terminateJobsOnCompletion`\n: Enables the Batch Job to automatically terminate a job once all tasks have completed (default: `true`).\n\n`azure.batch.deleteJobsOnCompletion`\n: Enable the automatic deletion of jobs created by the pipeline execution (default:", "start_char_idx": 9678, "end_char_idx": 12942, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "eb6668ea-4b84-4d25-a38b-351a718b8eed": {"__data__": {"id_": "eb6668ea-4b84-4d25-a38b-351a718b8eed", "embedding": null, "metadata": {"file_path": "docs/config.md", "file_name": "config.md"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "5f43dd876f21e8b1a4c175d44180bc9a99b46e3d", "node_type": null, "metadata": {"file_path": "docs/config.md", "file_name": "config.md"}, "hash": "8847079eb331e85ef73b38aefeee43dae1d4f4e09a7d20e4f059d4a72a73f105"}, "2": {"node_id": "4c5bfd13-0e96-4acb-b7b9-27afecb12995", "node_type": null, "metadata": {"file_path": "docs/config.md", "file_name": "config.md"}, "hash": "a9546d5795dbad8ed1e17b97ba4d5e818aed273b1c6af1b8f45adb719c404bed"}, "3": {"node_id": "1966337f-81ce-468d-80b3-1714fea6e49d", "node_type": null, "metadata": {"file_path": "docs/config.md", "file_name": "config.md"}, "hash": "f981516c86abbbc9b806793fadabdc748dfc7edb0ead81cbb1d83c6a335fda3b"}}, "hash": "5e8c86c2e0043e4882956fb460023442a76e3e77b79ef25f7da1197e858126f5", "text": "Enable the automatic deletion of jobs created by the pipeline execution (default: `true`).\n\n`azure.batch.deletePoolsOnCompletion`\n: Enable the automatic deletion of compute node pools upon pipeline completion (default: `false`).\n\n`azure.batch.endpoint`\n: The batch service endpoint e.g. `https://nfbatch1.westeurope.batch.azure.com`.\n\n`azure.batch.location`\n: The name of the batch service region, e.g. `westeurope` or `eastus2`. This is not needed when the endpoint is specified.\n\n`azure.batch.pools.<name>.autoScale`\n: Enable autoscaling feature for the pool identified with `<name>`.\n\n`azure.batch.pools.<name>.fileShareRootPath`\n: *New in `nf-azure` version `0.11.0`*\n: If mounting File Shares, this is the internal root mounting point. Must be `/mnt/resource/batch/tasks/fsmounts` for CentOS nodes or `/mnt/batch/tasks/fsmounts` for Ubuntu nodes (default is for CentOS).\n\n`azure.batch.pools.<name>.maxVmCount`\n: Specify the max of virtual machine when using auto scale option.\n\n`azure.batch.pools.<name>.mountOptions`\n: *New in `nf-azure` version `0.11.0`*\n: Specify the mount options for mounting the file shares (default: `-o vers=3.0,dir_mode=0777,file_mode=0777,sec=ntlmssp`).\n\n`azure.batch.pools.<name>.offer`\n: *New in `nf-azure` version `0.11.0`*\n: Specify the offer type of the virtual machine type used by the pool identified with `<name>` (default: `centos-container`).\n\n`azure.batch.pools.<name>.privileged`\n: Enable the task to run with elevated access. Ignored if `runAs` is set (default: `false`).\n\n`azure.batch.pools.<name>.publisher`\n: *New in `nf-azure` version `0.11.0`*\n: Specify the publisher of virtual machine type used by the pool identified with `<name>` (default: `microsoft-azure-batch`).\n\n`azure.batch.pools.<name>.runAs`\n: Specify the username under which the task is run. The user must already exist on each node of the pool.\n\n`azure.batch.pools.<name>.scaleFormula`\n: Specify the scale formula for the pool identified with `<name>`. See Azure Batch [scaling documentation](https://docs.microsoft.com/en-us/azure/batch/batch-automatic-scaling) for details.\n\n`azure.batch.pools.<name>.scaleInterval`\n: Specify the interval at which to automatically adjust the Pool size according to the autoscale formula. The minimum and maximum value are 5 minutes and 168 hours respectively (default: `10 mins`).\n\n`azure.batch.pools.<name>.schedulePolicy`\n: Specify the scheduling policy for the pool identified with `<name>`. It can be either `spread` or `pack` (default: `spread`).\n\n`azure.batch.pools.<name>.sku`\n: *New in `nf-azure` version `0.11.0`*\n: Specify the ID of the Compute Node agent SKU which the pool identified with `<name>` supports (default: `batch.node.centos 8`).\n\n`azure.batch.pools.<name>.virtualNetwork`\n: :::{versionadded} 23.03.0-edge\n  :::\n: Specify the subnet ID of a virtual network in which to create the pool.\n\n`azure.batch.pools.<name>.vmCount`\n: Specify the number of virtual machines provisioned by the pool identified with", "start_char_idx": 12903, "end_char_idx": 15879, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "1966337f-81ce-468d-80b3-1714fea6e49d": {"__data__": {"id_": "1966337f-81ce-468d-80b3-1714fea6e49d", "embedding": null, "metadata": {"file_path": "docs/config.md", "file_name": "config.md"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "5f43dd876f21e8b1a4c175d44180bc9a99b46e3d", "node_type": null, "metadata": {"file_path": "docs/config.md", "file_name": "config.md"}, "hash": "8847079eb331e85ef73b38aefeee43dae1d4f4e09a7d20e4f059d4a72a73f105"}, "2": {"node_id": "eb6668ea-4b84-4d25-a38b-351a718b8eed", "node_type": null, "metadata": {"file_path": "docs/config.md", "file_name": "config.md"}, "hash": "5e8c86c2e0043e4882956fb460023442a76e3e77b79ef25f7da1197e858126f5"}, "3": {"node_id": "82b36099-7ef0-4cb3-afe6-6d391f4b5c27", "node_type": null, "metadata": {"file_path": "docs/config.md", "file_name": "config.md"}, "hash": "ddc199a88ab0f71705b68504689933f2cafa21d1577df1c0aae91b577590e6b3"}}, "hash": "f981516c86abbbc9b806793fadabdc748dfc7edb0ead81cbb1d83c6a335fda3b", "text": "Specify the number of virtual machines provisioned by the pool identified with `<name>`.\n\n`azure.batch.pools.<name>.vmType`\n: Specify the virtual machine type used by the pool identified with `<name>`.\n\n`azure.registry.server`\n: *New in `nf-azure` version `0.9.8`*\n: Specify the container registry from which to pull the Docker images (default: `docker.io`).\n\n`azure.registry.userName`\n: *New in `nf-azure` version `0.9.8`*\n: Specify the username to connect to a private container registry.\n\n`azure.registry.password`\n: *New in `nf-azure` version `0.9.8`*\n: Specify the password to connect to a private container registry.\n\n`azure.retryPolicy.delay`\n: Delay when retrying failed API requests (default: `500ms`).\n\n`azure.retryPolicy.jitter`\n: Jitter value when retrying failed API requests (default: `0.25`).\n\n`azure.retryPolicy.maxAttempts`\n: Max attempts when retrying failed API requests (default: `10`).\n\n`azure.retryPolicy.maxDelay`\n: Max delay when retrying failed API requests (default: `90s`).\n\n`azure.storage.accountName`\n: The blob storage account name\n\n`azure.storage.accountKey`\n: The blob storage account key\n\n`azure.storage.sasToken`\n: The blob storage shared access signature token. This can be provided as an alternative to the `accountKey` setting.\n\n`azure.storage.tokenDuration`\n: The duration of the shared access signature token created by Nextflow when the `sasToken` option is *not* specified (default: `48h`).\n\n(config-charliecloud)=\n\n### Scope `charliecloud`\n\nThe `charliecloud` scope controls how [Charliecloud](https://hpc.github.io/charliecloud/) containers are executed by Nextflow.\n\nThe following settings are available:\n\n`charliecloud.cacheDir`\n: The directory where remote Charliecloud images are stored. When using a computing cluster it must be a shared folder accessible to all compute nodes.\n\n`charliecloud.enabled`\n: Enable Charliecloud execution (default: `false`).\n\n`charliecloud.envWhitelist`\n: Comma separated list of environment variable names to be included in the container environment.\n\n`charliecloud.pullTimeout`\n: The amount of time the Charliecloud pull can last, exceeding which the process is terminated (default: `20 min`).\n\n`charliecloud.runOptions`\n: This attribute can be used to provide any extra command line options supported by the `ch-run` command.\n\n`charliecloud.temp`\n: Mounts a path of your choice as the `/tmp` directory in the container. Use the special value `auto` to create a temporary directory each time a container is created.\n\nRead the {ref}`container-charliecloud` page to learn more about how to use Charliecloud containers with Nextflow.\n\n(config-conda)=\n\n### Scope `conda`\n\nThe `conda` scope controls the creation of a Conda environment by the Conda package manager.\n\nThe following settings are available:\n\n`conda.cacheDir`\n: Defines the path where Conda environments are stored. When using a compute cluster make sure to provide a shared file system path accessible from all compute nodes.\n\n`conda.createOptions`\n: Defines any extra command line options supported by the `conda create` command. For details see the [Conda documentation](https://docs.conda.io/projects/conda/en/latest/commands/create.html).\n\n`conda.createTimeout`\n: Defines the amount of time the Conda environment creation can last. The creation process is terminated when the timeout is exceeded (default: `20", "start_char_idx": 15883, "end_char_idx": 19234, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "82b36099-7ef0-4cb3-afe6-6d391f4b5c27": {"__data__": {"id_": "82b36099-7ef0-4cb3-afe6-6d391f4b5c27", "embedding": null, "metadata": {"file_path": "docs/config.md", "file_name": "config.md"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "5f43dd876f21e8b1a4c175d44180bc9a99b46e3d", "node_type": null, "metadata": {"file_path": "docs/config.md", "file_name": "config.md"}, "hash": "8847079eb331e85ef73b38aefeee43dae1d4f4e09a7d20e4f059d4a72a73f105"}, "2": {"node_id": "1966337f-81ce-468d-80b3-1714fea6e49d", "node_type": null, "metadata": {"file_path": "docs/config.md", "file_name": "config.md"}, "hash": "f981516c86abbbc9b806793fadabdc748dfc7edb0ead81cbb1d83c6a335fda3b"}, "3": {"node_id": "ebafea1a-b940-4cbb-b1d4-d2350ad77905", "node_type": null, "metadata": {"file_path": "docs/config.md", "file_name": "config.md"}, "hash": "30b71723e9ca6911898fbf8f505739716b6e2e340cc123efda1666f8516b05c4"}}, "hash": "ddc199a88ab0f71705b68504689933f2cafa21d1577df1c0aae91b577590e6b3", "text": "last. The creation process is terminated when the timeout is exceeded (default: `20 min`).\n\n`conda.useMamba`\n: Uses the `mamba` binary instead of `conda` to create the Conda environments. For details see the [Mamba documentation](https://github.com/mamba-org/mamba).\n\n`conda.useMicromamba`\n: :::{versionadded} 22.05.0-edge\n  :::\n: uses the `micromamba` binary instead of `conda` to create the Conda environments. For details see the [Micromamba documentation](https://mamba.readthedocs.io/en/latest/user_guide/micromamba.html).\n\nRead the {ref}`conda-page` page to learn more about how to use Conda environments with Nextflow.\n\n(config-dag)=\n\n### Scope `dag`\n\nThe `dag` scope controls the layout of the execution graph diagram generated by Nextflow.\n\nThe following settings are available:\n\n`dag.enabled`\n: When `true` turns on the generation of the DAG file (default: `false`).\n\n`dag.file`\n: Graph file name (default: `dag-<timestamp>.dot`).\n\n`dag.overwrite`\n: When `true` overwrites any existing DAG file with the same name.\n\nRead the {ref}`dag-visualisation` page to learn more about the execution graph that can be generated by Nextflow.\n\n(config-docker)=\n\n### Scope `docker`\n\nThe `docker` scope controls how [Docker](https://www.docker.com) containers are executed by Nextflow.\n\nThe following settings are available:\n\n`docker.enabled`\n: Enable Docker execution (default: `false`).\n\n`docker.engineOptions`\n: This attribute can be used to provide any option supported by the Docker engine i.e. `docker [OPTIONS]`.\n\n`docker.envWhitelist`\n: Comma separated list of environment variable names to be included in the container environment.\n\n`docker.fixOwnership`\n: Fix ownership of files created by the docker container.\n\n`docker.legacy`\n: Use command line options removed since Docker 1.10.0 (default: `false`).\n\n`docker.mountFlags`\n: Add the specified flags to the volume mounts e.g. `mountFlags = 'ro,Z'`.\n\n`docker.registry`\n: The registry from where Docker images are pulled. It should be only used to specify a private registry server. It should NOT include the protocol prefix i.e. `http://`.\n\n`docker.remove`\n: Clean-up the container after the execution (default: `true`). See the [Docker documentation](https://docs.docker.com/engine/reference/run/#clean-up---rm) for details.\n\n`docker.runOptions`\n: This attribute can be used to provide any extra command line options supported by the `docker run` command. See the [Docker documentation](https://docs.docker.com/engine/reference/run/) for details.\n\n`docker.sudo`\n: Executes Docker run command as `sudo` (default: `false`).\n\n`docker.temp`\n: Mounts a path of your choice as the `/tmp` directory in the container. Use the special value `auto` to create a temporary directory each time a container is created.\n\n`docker.tty`\n: Allocates a pseudo-tty (default: `false`).\n\nRead the {ref}`container-docker` page to learn more about how to use Docker containers with Nextflow.\n\n(config-env)=\n\n### Scope `env`\n\nThe `env` scope allows the definition one or more variables that will be exported into the environment where workflow tasks are executed.\n\nSimply prefix your variable names with the `env` scope or surround them by curly brackets, as shown below:\n\n```groovy\nenv.ALPHA = 'some value'\nenv.BETA = \"$HOME/some/path\"\n\nenv {\n     DELTA = 'one", "start_char_idx": 19231, "end_char_idx": 22522, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "ebafea1a-b940-4cbb-b1d4-d2350ad77905": {"__data__": {"id_": "ebafea1a-b940-4cbb-b1d4-d2350ad77905", "embedding": null, "metadata": {"file_path": "docs/config.md", "file_name": "config.md"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "5f43dd876f21e8b1a4c175d44180bc9a99b46e3d", "node_type": null, "metadata": {"file_path": "docs/config.md", "file_name": "config.md"}, "hash": "8847079eb331e85ef73b38aefeee43dae1d4f4e09a7d20e4f059d4a72a73f105"}, "2": {"node_id": "82b36099-7ef0-4cb3-afe6-6d391f4b5c27", "node_type": null, "metadata": {"file_path": "docs/config.md", "file_name": "config.md"}, "hash": "ddc199a88ab0f71705b68504689933f2cafa21d1577df1c0aae91b577590e6b3"}, "3": {"node_id": "599dbdfb-3049-4a7c-afa1-88d0372625ab", "node_type": null, "metadata": {"file_path": "docs/config.md", "file_name": "config.md"}, "hash": "9660a6ca008fb66d7f804d1dc0cb08bb9a21b62873883580b99ca0b7b75abe0b"}}, "hash": "30b71723e9ca6911898fbf8f505739716b6e2e340cc123efda1666f8516b05c4", "text": "{\n     DELTA = 'one more'\n     GAMMA = \"/my/path:$PATH\"\n}\n```\n\n:::{note}\nIn the above example, variables like `$HOME` and `$PATH` are evaluated when the workflow is launched. If you want these variables to be evaluated during task execution, escape them with `\\$`. This difference is important for variables like `$PATH`, which may be different in the workflow environment versus the task environment.\n:::\n\n:::{warning}\nThe `env` scope provides environment variables to *tasks*, not Nextflow itself. Nextflow environment variables such as `NXF_VER` should be set in the environment in which Nextflow is launched.\n:::\n\n(config-executor)=\n\n### Scope `executor`\n\nThe `executor` scope controls various executor behaviors.\n\nThe following settings are available:\n\n`executor.cpus`\n: The maximum number of CPUs made available by the underlying system. Used only by the `local` executor.\n\n`executor.dumpInterval`\n: Determines how often to log the executor status (default: `5min`).\n\n`executor.exitReadTimeout`\n: Determines how long to wait before returning an error status when a process is terminated but the `.exitcode` file does not exist or is empty (default: `270 sec`). Used only by grid executors.\n\n`executor.jobName`\n: Determines the name of jobs submitted to the underlying cluster executor e.g. `executor.jobName = { \"$task.name - $task.hash\" }`. Make sure the resulting job name matches the validation constraints of the underlying batch scheduler.\n\n`executor.killBatchSize`\n: Determines the number of jobs that can be killed in a single command execution (default: `100`).\n\n`executor.memory`\n: The maximum amount of memory made available by the underlying system. Used only by the `local` executor.\n\n`executor.name`\n: The name of the executor to be used (default: `local`).\n\n`executor.perJobMemLimit`\n: Specifies Platform LSF *per-job* memory limit mode. See {ref}`lsf-executor`.\n\n`executor.perTaskReserve`\n: Specifies Platform LSF *per-task* memory reserve mode. See {ref}`lsf-executor`.\n\n`executor.pollInterval`\n: Determines how often to check for process termination. Default varies for each executor (see below).\n\n`executor.queueGlobalStatus`\n: :::{versionadded} 23.01.0-edge\n  :::\n: Determines how job status is retrieved. When `false` only the queue associated with the job execution is queried. When `true` the job status is queried globally i.e. irrespective of the submission queue (default: `false`).\n\n`executor.queueSize`\n: The number of tasks the executor will handle in a parallel manner. Default varies for each executor (see below).\n\n`executor.queueStatInterval`\n: Determines how often to fetch the queue status from the scheduler (default: `1min`). Used only by grid executors.\n\n`executor.retry.delay`\n: :::{versionadded} 22.03.0-edge\n  :::\n: Delay when retrying failed job submissions (default: `500ms`). Used only by grid executors.\n\n`executor.retry.jitter`\n: :::{versionadded} 22.03.0-edge\n  :::\n: Jitter value when retrying failed job submissions (default: `0.25`). Used only by grid executors.\n\n`executor.retry.maxAttempt`\n: :::{versionadded} 22.03.0-edge\n  :::\n: Max attempts when retrying failed job submissions (default: `3`). Used only by grid executors.\n\n`executor.retry.maxDelay`\n: :::{versionadded}", "start_char_idx": 22582, "end_char_idx": 25811, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "599dbdfb-3049-4a7c-afa1-88d0372625ab": {"__data__": {"id_": "599dbdfb-3049-4a7c-afa1-88d0372625ab", "embedding": null, "metadata": {"file_path": "docs/config.md", "file_name": "config.md"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "5f43dd876f21e8b1a4c175d44180bc9a99b46e3d", "node_type": null, "metadata": {"file_path": "docs/config.md", "file_name": "config.md"}, "hash": "8847079eb331e85ef73b38aefeee43dae1d4f4e09a7d20e4f059d4a72a73f105"}, "2": {"node_id": "ebafea1a-b940-4cbb-b1d4-d2350ad77905", "node_type": null, "metadata": {"file_path": "docs/config.md", "file_name": "config.md"}, "hash": "30b71723e9ca6911898fbf8f505739716b6e2e340cc123efda1666f8516b05c4"}, "3": {"node_id": "40da5828-2dba-4cbb-acda-83c02bdd8b63", "node_type": null, "metadata": {"file_path": "docs/config.md", "file_name": "config.md"}, "hash": "5c32a37e1a1742e2ef32b4097cd2af28edbed6667d60867a22e7fdc748c13484"}}, "hash": "9660a6ca008fb66d7f804d1dc0cb08bb9a21b62873883580b99ca0b7b75abe0b", "text": ":::{versionadded} 22.03.0-edge\n  :::\n: Max delay when retrying failed job submissions (default: `30s`). Used only by grid executors.\n\n`executor.retry.reason`\n: :::{versionadded} 22.03.0-edge\n  :::\n: Regex pattern that when verified cause a failed submit operation to be re-tried (default: `Socket timed out`). Used only by grid executors.\n\n`executor.submitRateLimit`\n: Determines the max rate of job submission per time unit, for example `'10sec'` (10 jobs per second) or `'50/2min'` (50 jobs every 2 minutes) (default: unlimited).\n\nSome executor settings have different default values depending on the executor.\n\n| Executor       | `queueSize` | `pollInterval` |\n| -------------- | ----------- | -------------- |\n| AWS Batch      | `1000`      | `10s`          |\n| Azure Batch    | `1000`      | `10s`          |\n| Google Batch   | `1000`      | `10s`          |\n| Grid Executors | `100`       | `5s`           |\n| Kubernetes     | `100`       | `5s`           |\n| Local          | N/A         | `100ms`        |\n\nThe executor settings can be defined as shown below:\n\n```groovy\nexecutor {\n    name = 'sge'\n    queueSize = 200\n    pollInterval = '30 sec'\n}\n```\n\nWhen using two (or more) different executors in your pipeline, you can specify their settings separately by prefixing the executor name with the symbol `$` and using it as special scope identifier. For example:\n\n```groovy\nexecutor {\n  $sge {\n      queueSize = 100\n      pollInterval = '30sec'\n  }\n\n  $local {\n      cpus = 8\n      memory = '32 GB'\n  }\n}\n```\n\nThe above configuration example can be rewritten using the dot notation as shown below:\n\n```groovy\nexecutor.$sge.queueSize = 100\nexecutor.$sge.pollInterval = '30sec'\nexecutor.$local.cpus = 8\nexecutor.$local.memory = '32 GB'\n```\n\n(config-google)=\n\n### Scope `google`\n\nThe `google` scope allows you to configure the interactions with Google Cloud, including Google Cloud Batch, Google Life Sciences, and Google Cloud Storage.\n\nRead the {ref}`google-page` page for more information.\n\nThe following settings are available:\n\n`google.enableRequesterPaysBuckets`\n: When `true` uses the given Google Cloud project ID as the billing project for storage access. This is required when accessing data from *requester pays enabled* buckets. See [Requester Pays on Google Cloud Storage documentation](https://cloud.google.com/storage/docs/requester-pays) (default: `false`).\n\n`google.httpConnectTimeout`\n: :::{versionadded} 23.06.0-edge\n  :::\n: Defines the HTTP connection timeout for Cloud Storage API requests (default: `'60s'`).\n\n`google.httpReadTimeout`\n: :::{versionadded} 23.06.0-edge\n  :::\n: Defines the HTTP read timeout for Cloud Storage API requests (default: `'60s'`).\n\n`google.location`\n: The Google Cloud location where jobs are executed (default: `us-central1`).\n\n`google.project`\n: The Google Cloud project ID to use for pipeline", "start_char_idx": 25807, "end_char_idx": 28657, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "40da5828-2dba-4cbb-acda-83c02bdd8b63": {"__data__": {"id_": "40da5828-2dba-4cbb-acda-83c02bdd8b63", "embedding": null, "metadata": {"file_path": "docs/config.md", "file_name": "config.md"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "5f43dd876f21e8b1a4c175d44180bc9a99b46e3d", "node_type": null, "metadata": {"file_path": "docs/config.md", "file_name": "config.md"}, "hash": "8847079eb331e85ef73b38aefeee43dae1d4f4e09a7d20e4f059d4a72a73f105"}, "2": {"node_id": "599dbdfb-3049-4a7c-afa1-88d0372625ab", "node_type": null, "metadata": {"file_path": "docs/config.md", "file_name": "config.md"}, "hash": "9660a6ca008fb66d7f804d1dc0cb08bb9a21b62873883580b99ca0b7b75abe0b"}, "3": {"node_id": "32b1f477-b3f8-4542-9321-b617259b3a13", "node_type": null, "metadata": {"file_path": "docs/config.md", "file_name": "config.md"}, "hash": "4532ffe558e0fef0f6dc844885ba29a93a85a293c9864606faf7f1b0fc9597b2"}}, "hash": "5c32a37e1a1742e2ef32b4097cd2af28edbed6667d60867a22e7fdc748c13484", "text": "The Google Cloud project ID to use for pipeline execution\n\n`google.region`\n: *Available only for Google Life Sciences*\n: The Google Cloud region where jobs are executed. Multiple regions can be provided as a comma-separated list. Cannot be used with the `google.zone` option. See the [Google Cloud documentation](https://cloud.google.com/compute/docs/regions-zones/) for a list of available regions and zones.\n\n`google.zone`\n: *Available only for Google Life Sciences*\n: The Google Cloud zone where jobs are executed. Multiple zones can be provided as a comma-separated list. Cannot be used with the `google.region` option. See the [Google Cloud documentation](https://cloud.google.com/compute/docs/regions-zones/) for a list of available regions and zones.\n\n`google.batch.allowedLocations`\n: :::{versionadded} 22.12.0-edge\n  :::\n: Define the set of allowed locations for VMs to be provisioned. See [Google documentation](https://cloud.google.com/batch/docs/reference/rest/v1/projects.locations.jobs#locationpolicy) for details (default: no restriction).\n\n`google.batch.bootDiskSize`\n: Set the size of the virtual machine boot disk, e.g `50.GB` (default: none).\n\n`google.batch.cpuPlatform`\n: Set the minimum CPU Platform, e.g. `'Intel Skylake'`. See [Specifying a minimum CPU Platform for VM instances](https://cloud.google.com/compute/docs/instances/specify-min-cpu-platform#specifications) (default: none).\n\n`google.batch.network`\n: Set network name to attach the VM's network interface to. The value will be prefixed with `global/networks/` unless it contains a `/`, in which case it is assumed to be a fully specified network resource URL. If unspecified, the global default network is used.\n\n`google.batch.serviceAccountEmail`\n: Define the Google service account email to use for the pipeline execution. If not specified, the default Compute Engine service account for the project will be used.\n\n`google.batch.spot`\n: When `true` enables the usage of *spot* virtual machines or `false` otherwise (default: `false`).\n\n`google.batch.subnetwork`\n: Define the name of the subnetwork to attach the instance to must be specified here, when the specified network is configured for custom subnet creation. The value is prefixed with `regions/subnetworks/` unless it contains a `/`, in which case it is assumed to be a fully specified subnetwork resource URL.\n\n`google.batch.usePrivateAddress`\n: When `true` the VM will NOT be provided with a public IP address, and only contain an internal IP. If this option is enabled, the associated job can only load docker images from Google Container Registry, and the job executable cannot use external services other than Google APIs (default: `false`).\n\n`google.lifeSciences.bootDiskSize`\n: Set the size of the virtual machine boot disk e.g `50.GB` (default: none).\n\n`google.lifeSciences.copyImage`\n: The container image run to copy input and output files. It must include the `gsutil` tool (default: `google/cloud-sdk:alpine`).\n\n`google.lifeSciences.cpuPlatform`\n: Set the minimum CPU Platform e.g. `'Intel Skylake'`. See [Specifying a minimum CPU Platform for VM instances](https://cloud.google.com/compute/docs/instances/specify-min-cpu-platform#specifications) (default: none).\n\n`google.lifeSciences.debug`\n: When `true` copies the `/google` debug directory in that task bucket directory (default: `false`).\n\n`google.lifeSciences.keepAliveOnFailure`\n: :::{versionadded} 21.06.0-edge\n  :::\n: When `true` and a task complete with an unexpected exit status the associated compute node is kept up for 1 hour. This options implies", "start_char_idx": 28636, "end_char_idx": 32204, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "32b1f477-b3f8-4542-9321-b617259b3a13": {"__data__": {"id_": "32b1f477-b3f8-4542-9321-b617259b3a13", "embedding": null, "metadata": {"file_path": "docs/config.md", "file_name": "config.md"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "5f43dd876f21e8b1a4c175d44180bc9a99b46e3d", "node_type": null, "metadata": {"file_path": "docs/config.md", "file_name": "config.md"}, "hash": "8847079eb331e85ef73b38aefeee43dae1d4f4e09a7d20e4f059d4a72a73f105"}, "2": {"node_id": "40da5828-2dba-4cbb-acda-83c02bdd8b63", "node_type": null, "metadata": {"file_path": "docs/config.md", "file_name": "config.md"}, "hash": "5c32a37e1a1742e2ef32b4097cd2af28edbed6667d60867a22e7fdc748c13484"}, "3": {"node_id": "ec8cbbc1-3390-4794-a203-10cce7f1113f", "node_type": null, "metadata": {"file_path": "docs/config.md", "file_name": "config.md"}, "hash": "f5c9c2749bdf7f4334aec1845f988d3c12a6d20a2959098984fe9968cd6fc10a"}}, "hash": "4532ffe558e0fef0f6dc844885ba29a93a85a293c9864606faf7f1b0fc9597b2", "text": "unexpected exit status the associated compute node is kept up for 1 hour. This options implies `sshDaemon=true` (default: `false`).\n\n`google.lifeSciences.network`\n: :::{versionadded} 21.03.0-edge\n  :::\n: Set network name to attach the VM's network interface to. The value will be prefixed with `global/networks/` unless it contains a `/`, in which case it is assumed to be a fully specified network resource URL. If unspecified, the global default network is used.\n\n`google.lifeSciences.preemptible`\n: When `true` enables the usage of *preemptible* virtual machines or `false` otherwise (default: `true`).\n\n`google.lifeSciences.serviceAccountEmail`\n: :::{versionadded} 20.05.0-edge\n  :::\n: Define the Google service account email to use for the pipeline execution. If not specified, the default Compute Engine service account for the project will be used.\n\n`google.lifeSciences.subnetwork`\n: :::{versionadded} 21.03.0-edge\n  :::\n: Define the name of the subnetwork to attach the instance to must be specified here, when the specified network is configured for custom subnet creation. The value is prefixed with `regions/subnetworks/` unless it contains a `/`, in which case it is assumed to be a fully specified subnetwork resource URL.\n\n`google.lifeSciences.sshDaemon`\n: When `true` runs SSH daemon in the VM carrying out the job to which it's possible to connect for debugging purposes (default: `false`).\n\n`google.lifeSciences.sshImage`\n: The container image used to run the SSH daemon (default: `gcr.io/cloud-genomics-pipelines/tools`).\n\n`google.lifeSciences.usePrivateAddress`\n: :::{versionadded} 20.03.0-edge\n  :::\n: When `true` the VM will NOT be provided with a public IP address, and only contain an internal IP. If this option is enabled, the associated job can only load docker images from Google Container Registry, and the job executable cannot use external services other than Google APIs (default: `false`).\n\n`google.storage.delayBetweenAttempts`\n: :::{versionadded} 21.06.0-edge\n  :::\n: Delay between download attempts from Google Storage (default `10 sec`).\n\n`google.storage.downloadMaxComponents`\n: :::{versionadded} 21.06.0-edge\n  :::\n: Defines the value for the option `GSUtil:sliced_object_download_max_components` used by `gsutil` for transfer input and output data (default: `8`).\n\n`google.storage.maxParallelTransfers`\n: :::{versionadded} 21.06.0-edge\n  :::\n: Max parallel upload/download transfer operations *per job* (default: `4`).\n\n`google.storage.maxTransferAttempts`\n: :::{versionadded} 21.06.0-edge\n  :::\n: Max number of downloads attempts from Google Storage (default: `1`).\n\n`google.storage.parallelThreadCount`\n: :::{versionadded} 21.06.0-edge\n  :::\n: Defines the value for the option `GSUtil:parallel_thread_count` used by `gsutil` for transfer input and output data (default: `1`).\n\n(config-k8s)=\n\n### Scope `k8s`\n\nThe `k8s` scope controls the deployment and execution of workflow applications in a Kubernetes cluster.\n\nThe following settings are available:\n\n`k8s.autoMountHostPaths`\n: Automatically mounts host paths in the job pods. Only for development purpose when using a single node cluster (default: `false`).\n\n`k8s.computeResourceType`\n: :::{versionadded} 22.05.0-edge\n  :::\n: Define whether use Kubernetes `Pod` or `Job` resource type to carry out Nextflow", "start_char_idx": 32165, "end_char_idx": 35466, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "ec8cbbc1-3390-4794-a203-10cce7f1113f": {"__data__": {"id_": "ec8cbbc1-3390-4794-a203-10cce7f1113f", "embedding": null, "metadata": {"file_path": "docs/config.md", "file_name": "config.md"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "5f43dd876f21e8b1a4c175d44180bc9a99b46e3d", "node_type": null, "metadata": {"file_path": "docs/config.md", "file_name": "config.md"}, "hash": "8847079eb331e85ef73b38aefeee43dae1d4f4e09a7d20e4f059d4a72a73f105"}, "2": {"node_id": "32b1f477-b3f8-4542-9321-b617259b3a13", "node_type": null, "metadata": {"file_path": "docs/config.md", "file_name": "config.md"}, "hash": "4532ffe558e0fef0f6dc844885ba29a93a85a293c9864606faf7f1b0fc9597b2"}, "3": {"node_id": "0a39cbf1-a298-4f1e-9d18-bb82cf940edc", "node_type": null, "metadata": {"file_path": "docs/config.md", "file_name": "config.md"}, "hash": "df323c988df1e518bfd973ca3949bc016fe10c7cb856485ee9883f5587c6071c"}}, "hash": "f5c9c2749bdf7f4334aec1845f988d3c12a6d20a2959098984fe9968cd6fc10a", "text": "whether use Kubernetes `Pod` or `Job` resource type to carry out Nextflow tasks (default: `Pod`).\n\n`k8s.context`\n: Defines the Kubernetes [configuration context name](https://kubernetes.io/docs/tasks/access-application-cluster/configure-access-multiple-clusters/) to use.\n\n`k8s.fetchNodeName`\n: :::{versionadded} 22.05.0-edge\n  :::\n: If you trace the hostname, activate this option (default: `false`).\n\n`k8s.httpConnectTimeout`\n: :::{versionadded} 22.10.0\n  :::\n: Defines the Kubernetes client request HTTP connection timeout e.g. `'60s'`.\n\n`k8s.httpReadTimeout`\n: :::{versionadded} 22.10.0\n  :::\n: Defines the Kubernetes client request HTTP connection read timeout e.g. `'60s'`.\n\n`k8s.launchDir`\n: Defines the path where the workflow is launched and the user data is stored. This must be a path in a shared K8s persistent volume (default: `<volume-claim-mount-path>/<user-name>`).\n\n`k8s.maxErrorRetry`\n: :::{versionadded} 22.09.6-edge\n  :::\n: Defines the Kubernetes API max request retries (default: 4).\n\n`k8s.namespace`\n: Defines the Kubernetes namespace to use (default: `default`).\n\n`k8s.pod`\n: Allows the definition of one or more pod configuration options such as environment variables, config maps, secrets, etc. It allows the same settings as the {ref}`process-pod` process directive.\n\n`k8s.projectDir`\n: Defines the path where Nextflow projects are downloaded. This must be a path in a shared K8s persistent volume (default: `<volume-claim-mount-path>/projects`).\n\n`k8s.pullPolicy`\n: Defines the strategy to be used to pull the container image e.g. `pullPolicy: 'Always'`.\n\n`k8s.runAsUser`\n: Defines the user ID to be used to run the containers. Shortcut for the `securityContext` option.\n\n`k8s.securityContext`\n: Defines the [security context](https://kubernetes.io/docs/tasks/configure-pod-container/security-context/) for all pods.\n\n`k8s.serviceAccount`\n: Defines the Kubernetes [service account name](https://kubernetes.io/docs/tasks/configure-pod-container/configure-service-account/) to use.\n\n`k8s.storageClaimName`\n: The name of the persistent volume claim where store workflow result data.\n\n`k8s.storageMountPath`\n: The path location used to mount the persistent volume claim (default: `/workspace`).\n\n`k8s.storageSubPath`\n: The path in the persistent volume to be mounted (default: `/`).\n\n`k8s.workDir`\n: Defines the path where the workflow temporary data is stored. This must be a path in a shared K8s persistent volume (default: `<user-dir>/work`).\n\nSee the {ref}`k8s-page` page for more details.\n\n(config-mail)=\n\n### Scope `mail`\n\nThe `mail` scope controls the mail server used to send email notifications.\n\nThe following settings are available:\n\n`mail.debug`\n: When `true` enables Java Mail logging for debugging purpose.\n\n`mail.from`\n: Default email sender address.\n\n`mail.smtp.host`\n: Host name of the mail server.\n\n`mail.smtp.port`\n: Port number of the mail server.\n\n`mail.smtp.user`\n: User name to connect to the mail server.\n\n`mail.smtp.password`\n: User password to connect to the mail", "start_char_idx": 35484, "end_char_idx": 38496, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "0a39cbf1-a298-4f1e-9d18-bb82cf940edc": {"__data__": {"id_": "0a39cbf1-a298-4f1e-9d18-bb82cf940edc", "embedding": null, "metadata": {"file_path": "docs/config.md", "file_name": "config.md"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "5f43dd876f21e8b1a4c175d44180bc9a99b46e3d", "node_type": null, "metadata": {"file_path": "docs/config.md", "file_name": "config.md"}, "hash": "8847079eb331e85ef73b38aefeee43dae1d4f4e09a7d20e4f059d4a72a73f105"}, "2": {"node_id": "ec8cbbc1-3390-4794-a203-10cce7f1113f", "node_type": null, "metadata": {"file_path": "docs/config.md", "file_name": "config.md"}, "hash": "f5c9c2749bdf7f4334aec1845f988d3c12a6d20a2959098984fe9968cd6fc10a"}, "3": {"node_id": "fa74fa51-e7eb-41d5-afab-6c96c0e0033c", "node_type": null, "metadata": {"file_path": "docs/config.md", "file_name": "config.md"}, "hash": "444cc817a54d660db71ec0b628c164a5940a99494cee0c2ac588142890f2059d"}}, "hash": "df323c988df1e518bfd973ca3949bc016fe10c7cb856485ee9883f5587c6071c", "text": "User password to connect to the mail server.\n\n`mail.smtp.proxy.host`\n: Host name of an HTTP web proxy server that will be used for connections to the mail server.\n\n`mail.smtp.proxy.port`\n: Port number for the HTTP web proxy server.\n\n`mail.smtp.*`\n: Any SMTP configuration property supported by the [Java Mail API](https://javaee.github.io/javamail/), which Nextflow uses to send emails. See the table of available properties [here](https://javaee.github.io/javamail/docs/api/com/sun/mail/smtp/package-summary.html#properties).\n\nFor example, the following snippet shows how to configure Nextflow to send emails through the [AWS Simple Email Service](https://aws.amazon.com/ses/):\n\n```groovy\nmail {\n    smtp.host = 'email-smtp.us-east-1.amazonaws.com'\n    smtp.port = 587\n    smtp.user = '<Your AWS SES access key>'\n    smtp.password = '<Your AWS SES secret key>'\n    smtp.auth = true\n    smtp.starttls.enable = true\n    smtp.starttls.required = true\n}\n```\n\n:::{note}\nSome versions of Java (e.g. Java 11 Corretto) do not default to TLS v1.2, and as a result may have issues with 3rd party integrations that enforce TLS v1.2 (e.g. Azure Active Directory OIDC). This problem can be addressed by setting the following config option:\n\n```groovy\nmail {\n    smtp.ssl.protocols = 'TLSv1.2'\n}\n```\n:::\n\n(config-manifest)=\n\n### Scope `manifest`\n\nThe `manifest` scope allows you to define some meta-data information needed when publishing or running your pipeline.\n\nThe following settings are available:\n\n`manifest.author`\n: Project author name (use a comma to separate multiple names).\n\n`manifest.defaultBranch`\n: Git repository default branch (default: `master`).\n\n`manifest.description`\n: Free text describing the workflow project.\n\n`manifest.doi`\n: Project related publication DOI identifier.\n\n`manifest.homePage`\n: Project home page URL.\n\n`manifest.mainScript`\n: Project main script (default: `main.nf`).\n\n`manifest.name`\n: Project short name.\n\n`manifest.nextflowVersion`\n: Minimum required Nextflow version.\n\n  This setting may be useful to ensure that a specific version is used:\n\n  ```groovy\n  manifest.nextflowVersion = '1.2.3'        // exact match\n  manifest.nextflowVersion = '1.2+'         // 1.2 or later (excluding 2 and later)\n  manifest.nextflowVersion = '>=1.2'        // 1.2 or later\n  manifest.nextflowVersion = '>=1.2, <=1.5' // any version in the 1.2 .. 1.5 range\n  manifest.nextflowVersion = '!>=1.2'       // with ! prefix, stop execution if current version does not match required version.\n  ```\n\n`manifest.recurseSubmodules`\n: Pull submodules recursively from the Git repository.\n\n`manifest.version`\n: Project version number.\n\nThe above options can also be specified in a `manifest` block, for example:\n\n```groovy\nmanifest {\n    homePage = 'http://foo.com'\n    description = 'Pipeline does this and that'\n    mainScript = 'foo.nf'\n    version = '1.0.0'\n}\n```\n\nRead the {ref}`sharing-page` page to learn how to publish", "start_char_idx": 38529, "end_char_idx": 41459, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "fa74fa51-e7eb-41d5-afab-6c96c0e0033c": {"__data__": {"id_": "fa74fa51-e7eb-41d5-afab-6c96c0e0033c", "embedding": null, "metadata": {"file_path": "docs/config.md", "file_name": "config.md"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "5f43dd876f21e8b1a4c175d44180bc9a99b46e3d", "node_type": null, "metadata": {"file_path": "docs/config.md", "file_name": "config.md"}, "hash": "8847079eb331e85ef73b38aefeee43dae1d4f4e09a7d20e4f059d4a72a73f105"}, "2": {"node_id": "0a39cbf1-a298-4f1e-9d18-bb82cf940edc", "node_type": null, "metadata": {"file_path": "docs/config.md", "file_name": "config.md"}, "hash": "df323c988df1e518bfd973ca3949bc016fe10c7cb856485ee9883f5587c6071c"}, "3": {"node_id": "885ba2dc-e05b-4055-b45a-841a7adf9bc9", "node_type": null, "metadata": {"file_path": "docs/config.md", "file_name": "config.md"}, "hash": "ff220ace9f44fed9033348bc56066dbc47b6a5b6a3df171dcd95f7ef3eb6fd10"}}, "hash": "444cc817a54d660db71ec0b628c164a5940a99494cee0c2ac588142890f2059d", "text": "the {ref}`sharing-page` page to learn how to publish your pipeline to GitHub, BitBucket or GitLab.\n\n(config-notification)=\n\n### Scope `notification`\n\nThe `notification` scope allows you to define the automatic sending of a notification email message when the workflow execution terminates.\n\n`notification.binding`\n: An associative array modelling the variables in the template file.\n\n`notification.enabled`\n: Enables the sending of a notification message when the workflow execution completes.\n\n`notification.from`\n: Sender address for the notification email message.\n\n`notification.template`\n: Path of a template file which provides the content of the notification message.\n\n`notification.to`\n: Recipient address for the notification email. Multiple addresses can be specified separating them with a comma.\n\nThe notification message is sent my using the STMP server defined in the configuration {ref}`mail scope<config-mail>`.\n\nIf no mail configuration is provided, it tries to send the notification message by using the external mail command eventually provided by the underlying system (e.g. `sendmail` or `mail`).\n\n(config-params)=\n\n### Scope `params`\n\nThe `params` scope allows you to define parameters that will be accessible in the pipeline script. Simply prefix the parameter names with the `params` scope or surround them by curly brackets, as shown below:\n\n```groovy\nparams.custom_param = 123\nparams.another_param = 'string value .. '\n\nparams {\n    alpha_1 = true\n    beta_2 = 'another string ..'\n}\n```\n\n(config-podman)=\n\n### Scope `podman`\n\nThe `podman` scope controls how [Podman](https://podman.io/) containers are executed by Nextflow.\n\nThe following settings are available:\n\n`podman.enabled`\n: Enable Podman execution (default: `false`).\n\n`podman.engineOptions`\n: This attribute can be used to provide any option supported by the Podman engine i.e. `podman [OPTIONS]`.\n\n`podman.envWhitelist`\n: Comma separated list of environment variable names to be included in the container environment.\n\n`podman.mountFlags`\n: Add the specified flags to the volume mounts e.g. `mountFlags = 'ro,Z'`.\n\n`podman.registry`\n: The registry from where container images are pulled. It should be only used to specify a private registry server. It should NOT include the protocol prefix i.e. `http://`.\n\n`podman.remove`\n: Clean-up the container after the execution (default: `true`).\n\n`podman.runOptions`\n: This attribute can be used to provide any extra command line options supported by the `podman run` command.\n\n`podman.temp`\n: Mounts a path of your choice as the `/tmp` directory in the container. Use the special value `auto` to create a temporary directory each time a container is created.\n\nRead the {ref}`container-podman` page to learn more about how to use Podman containers with Nextflow.\n\n(config-process)=\n\n### Scope `process`\n\nThe `process` scope allows you to specify default {ref}`directives <process-directives>` for processes in your pipeline.\n\nFor example:\n\n```groovy\nprocess {\n    executor = 'sge'\n    queue = 'long'\n    clusterOptions = '-pe smp 10 -l virtual_free=64G,h_rt=30:00:00'\n}\n```\n\nBy using this configuration, all processes in your pipeline will be executed through the SGE cluster, with the specified settings.\n\n(config-process-selectors)=\n\n#### Process selectors\n\nThe `withLabel` selectors allow the configuration of all processes annotated with a {ref}`process-label` directive as shown below:\n\n```groovy\nprocess {\n    withLabel:", "start_char_idx": 41445, "end_char_idx": 44900, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "885ba2dc-e05b-4055-b45a-841a7adf9bc9": {"__data__": {"id_": "885ba2dc-e05b-4055-b45a-841a7adf9bc9", "embedding": null, "metadata": {"file_path": "docs/config.md", "file_name": "config.md"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "5f43dd876f21e8b1a4c175d44180bc9a99b46e3d", "node_type": null, "metadata": {"file_path": "docs/config.md", "file_name": "config.md"}, "hash": "8847079eb331e85ef73b38aefeee43dae1d4f4e09a7d20e4f059d4a72a73f105"}, "2": {"node_id": "fa74fa51-e7eb-41d5-afab-6c96c0e0033c", "node_type": null, "metadata": {"file_path": "docs/config.md", "file_name": "config.md"}, "hash": "444cc817a54d660db71ec0b628c164a5940a99494cee0c2ac588142890f2059d"}, "3": {"node_id": "6066287c-e78e-46b4-9747-13d1ac9291ca", "node_type": null, "metadata": {"file_path": "docs/config.md", "file_name": "config.md"}, "hash": "e9c4bc6c0186d91d3453ebc2ec106d71a8dbd268045b62f39ba834662ba05009"}}, "hash": "ff220ace9f44fed9033348bc56066dbc47b6a5b6a3df171dcd95f7ef3eb6fd10", "text": "as shown below:\n\n```groovy\nprocess {\n    withLabel: big_mem {\n        cpus = 16\n        memory = 64.GB\n        queue = 'long'\n    }\n}\n```\n\nThe above configuration example assigns 16 cpus, 64 Gb of memory and the `long` queue to all processes annotated with the `big_mem` label.\n\nIn the same manner, the `withName` selector allows the configuration of a specific process in your pipeline by its name. For example:\n\n```groovy\nprocess {\n    withName: hello {\n        cpus = 4\n        memory = 8.GB\n        queue = 'short'\n    }\n}\n```\n\n:::{tip}\nLabel and process names do not need to be enclosed with quotes, provided the name does not include special characters (`-`, `!`, etc) and is not a keyword or a built-in type identifier. When in doubt, you can enclose the label name or process name with single or double quotes.\n:::\n\n(config-selector-expressions)=\n\n#### Selector expressions\n\nBoth label and process name selectors allow the use of a regular expression in order to apply the same configuration to all processes matching the specified pattern condition. For example:\n\n```groovy\nprocess {\n    withLabel: 'foo|bar' {\n        cpus = 2\n        memory = 4.GB\n    }\n}\n```\n\nThe above configuration snippet sets 2 cpus and 4 GB of memory to the processes annotated with a label `foo` and `bar`.\n\nA process selector can be negated prefixing it with the special character `!`. For example:\n\n```groovy\nprocess {\n    withLabel: 'foo' { cpus = 2 }\n    withLabel: '!foo' { cpus = 4 }\n    withName: '!align.*' { queue = 'long' }\n}\n```\n\nThe above configuration snippet sets 2 cpus for the processes annotated with the `foo` label and 4 cpus to all processes *not* annotated with that label. Finally it sets the use of `long` queue to all process whose name does *not* start with `align`.\n\n(config-selector-priority)=\n\n#### Selector priority\n\nWhen mixing generic process configuration and selectors the following priority rules are applied (from lower to higher):\n\n1. Process generic configuration.\n2. Process specific directive defined in the workflow script.\n3. `withLabel` selector definition.\n4. `withName` selector definition.\n\nFor example:\n\n```groovy\nprocess {\n    cpus = 4\n    withLabel: foo { cpus = 8 }\n    withName: bar { cpus = 32 }\n}\n```\n\nUsing the above configuration snippet, all workflow processes use 4 cpus if not otherwise specified in the workflow script. Moreover processes annotated with the `foo` label use 8 cpus. Finally the process named `bar` uses 32 cpus.\n\n(config-report)=\n\n### Scope `report`\n\nThe `report` scope allows you to configure the workflow {ref}`execution-report`.\n\nThe following settings are available:\n\n`report.enabled`\n: If `true` it create the workflow execution report.\n\n`report.file`\n: The path of the created execution report file (default: `report-<timestamp>.html`).\n\n`report.overwrite`\n: When `true` overwrites any existing report file with the same name.\n\n(config-sarus)=\n\n### Scope `sarus`\n\nThe ``sarus`` scope controls how [Sarus](https://sarus.readthedocs.io) containers are executed by Nextflow.\n\nThe following settings are available:\n\n`sarus.enabled`\n: Enable Sarus execution (default:", "start_char_idx": 44902, "end_char_idx": 48029, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "6066287c-e78e-46b4-9747-13d1ac9291ca": {"__data__": {"id_": "6066287c-e78e-46b4-9747-13d1ac9291ca", "embedding": null, "metadata": {"file_path": "docs/config.md", "file_name": "config.md"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "5f43dd876f21e8b1a4c175d44180bc9a99b46e3d", "node_type": null, "metadata": {"file_path": "docs/config.md", "file_name": "config.md"}, "hash": "8847079eb331e85ef73b38aefeee43dae1d4f4e09a7d20e4f059d4a72a73f105"}, "2": {"node_id": "885ba2dc-e05b-4055-b45a-841a7adf9bc9", "node_type": null, "metadata": {"file_path": "docs/config.md", "file_name": "config.md"}, "hash": "ff220ace9f44fed9033348bc56066dbc47b6a5b6a3df171dcd95f7ef3eb6fd10"}, "3": {"node_id": "fd81bc34-c92f-414c-9813-a0ce486af0bd", "node_type": null, "metadata": {"file_path": "docs/config.md", "file_name": "config.md"}, "hash": "562631c832b885e3e58b31db95b52199971b33cda6996245d275c8ec4f13189e"}}, "hash": "e9c4bc6c0186d91d3453ebc2ec106d71a8dbd268045b62f39ba834662ba05009", "text": "available:\n\n`sarus.enabled`\n: Enable Sarus execution (default: `false`).\n\n`sarus.envWhitelist`\n: Comma separated list of environment variable names to be included in the container environment.\n\n`sarus.runOptions`\n: This attribute can be used to provide any extra command line options supported by the `sarus run` command. For details see the [Sarus user guide](https://sarus.readthedocs.io/en/stable/user/user_guide.html).\n\n`sarus.tty`\n: Allocates a pseudo-tty (default: `false`).\n\nRead the {ref}`container-sarus` page to learn more about how to use Sarus containers with Nextflow.\n\n(config-shifter)=\n\n### Scope `shifter`\n\nThe `shifter` scope controls how [Shifter](https://docs.nersc.gov/programming/shifter/overview/) containers are executed by Nextflow.\n\nThe following settings are available:\n\n`shifter.enabled`\n: Enable Shifter execution (default: `false`).\n\nRead the {ref}`container-shifter` page to learn more about how to use Shifter containers with Nextflow.\n\n(config-singularity)=\n\n### Scope `singularity`\n\nThe `singularity` scope controls how [Singularity](https://sylabs.io/singularity/) containers are executed by Nextflow.\n\nThe following settings are available:\n\n`singularity.autoMounts`\n: When `true` Nextflow automatically mounts host paths in the executed container. It requires the `user bind control` feature to be enabled in your Singularity installation (default: `false`).\n\n`singularity.cacheDir`\n: The directory where remote Singularity images are stored. When using a computing cluster it must be a shared folder accessible to all compute nodes.\n\n`singularity.enabled`\n: Enable Singularity execution (default: `false`).\n\n`singularity.engineOptions`\n: This attribute can be used to provide any option supported by the Singularity engine i.e. `singularity [OPTIONS]`.\n\n`singularity.envWhitelist`\n: Comma separated list of environment variable names to be included in the container environment.\n\n`singularity.noHttps`\n: Pull the Singularity image with http protocol (default: `false`).\n\n`singularity.pullTimeout`\n: The amount of time the Singularity pull can last, exceeding which the process is terminated (default: `20 min`).\n\n`singularity.registry`\n: :::{versionadded} 22.12.0-edge\n  :::\n: The registry from where Docker images are pulled. It should be only used to specify a private registry server. It should NOT include the protocol prefix i.e. `http://`.\n\n`singularity.runOptions`\n: This attribute can be used to provide any extra command line options supported by `singularity exec`.\n\nRead the {ref}`container-singularity` page to learn more about how to use Singularity containers with Nextflow.\n\n(config-spack)=\n\n### Scope `spack`\n\nThe `spack` scope controls the creation of a Spack environment by the Spack package manager.\n\nThe following settings are available:\n\n`spack.cacheDir`\n: Defines the path where Spack environments are stored. When using a compute cluster make sure to provide a shared file system path accessible from all compute nodes.\n\n`spack.checksum`\n: Enables checksum verification for source tarballs (default: `true`). Only disable when requesting a package version not yet encoded in the corresponding Spack recipe.\n\n`spack.createTimeout`\n: Defines the amount of time the Spack environment creation can last. The creation process is terminated when the timeout is exceeded (default: `60 min`).\n\n`spack.parallelBuilds`\n: Sets number of parallel package builds (Spack default: coincides with number of available CPU cores).\n\nNextflow does not allow for fine-grained configuration of the Spack package manager. Instead, this has to be", "start_char_idx": 48016, "end_char_idx": 51597, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "fd81bc34-c92f-414c-9813-a0ce486af0bd": {"__data__": {"id_": "fd81bc34-c92f-414c-9813-a0ce486af0bd", "embedding": null, "metadata": {"file_path": "docs/config.md", "file_name": "config.md"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "5f43dd876f21e8b1a4c175d44180bc9a99b46e3d", "node_type": null, "metadata": {"file_path": "docs/config.md", "file_name": "config.md"}, "hash": "8847079eb331e85ef73b38aefeee43dae1d4f4e09a7d20e4f059d4a72a73f105"}, "2": {"node_id": "6066287c-e78e-46b4-9747-13d1ac9291ca", "node_type": null, "metadata": {"file_path": "docs/config.md", "file_name": "config.md"}, "hash": "e9c4bc6c0186d91d3453ebc2ec106d71a8dbd268045b62f39ba834662ba05009"}, "3": {"node_id": "89329e42-8078-42e9-85d4-002d0aa6cbbd", "node_type": null, "metadata": {"file_path": "docs/config.md", "file_name": "config.md"}, "hash": "39d479766f12d635b2476e1cdddefc0482a7b8a8f52b3b16b859dc86017a2edf"}}, "hash": "562631c832b885e3e58b31db95b52199971b33cda6996245d275c8ec4f13189e", "text": "for fine-grained configuration of the Spack package manager. Instead, this has to be performed directly on the host Spack installation. For more information see the [Spack documentation](https://spack.readthedocs.io).\n\n(config-timeline)=\n\n### Scope `timeline`\n\nThe `timeline` scope controls the execution timeline report generated by Nextflow.\n\nThe following settings are available:\n\n`timeline.enabled`\n: When `true` enables the generation of the timeline report file (default: `false`).\n\n`timeline.file`\n: Timeline file name (default: `timeline-<timestamp>.html`).\n\n`timeline.overwrite`\n: When `true` overwrites any existing timeline file with the same name.\n\n(config-tower)=\n\n### Scope `tower`\n\nThe `tower` scope controls the settings for the [Nextflow Tower](https://tower.nf) monitoring and tracing service.\n\nThe following settings are available:\n\n`tower.accessToken`\n: The unique access token specific to your account on an instance of Tower.\n\n  Your `accessToken` can be obtained from your Tower instance in the [Tokens page](https://tower.nf/tokens).\n\n`tower.enabled`\n: When `true` Nextflow sends the workflow tracing and execution metrics to the Nextflow Tower service (default: `false`).\n\n`tower.endpoint`\n: The endpoint of your Tower deployment (default: `https://tower.nf`).\n\n`tower.workspaceId`\n: The ID of the Tower workspace where the run should be added (default: the launching user personal workspace).\n\n  The Tower workspace ID can also be specified using the environment variable `TOWER_WORKSPACE_ID` (config file has priority over the environment variable).\n\n(config-trace)=\n\n### Scope `trace`\n\nThe `trace` scope controls the layout of the execution trace file generated by Nextflow.\n\nThe following settings are available:\n\n`trace.enabled`\n: When `true` turns on the generation of the execution trace report file (default: `false`).\n\n`trace.fields`\n: Comma separated list of fields to be included in the report. The available fields are listed at {ref}`this page <trace-fields>`.\n\n`trace.file`\n: Trace file name (default: `trace-<timestamp>.txt`).\n\n`trace.overwrite`\n: When `true` overwrites any existing trace file with the same name.\n\n`trace.raw`\n: When `true` turns on raw number report generation i.e. date and time are reported as milliseconds and memory as number of bytes.\n\n`trace.sep`\n: Character used to separate values in each row (default: `\\t`).\n\nThe above options can also be specified in a `trace` block, for example:\n\n```groovy\ntrace {\n    enabled = true\n    file = 'pipeline_trace.txt'\n    fields = 'task_id,name,status,exit,realtime,%cpu,rss'\n}\n```\n\nRead the {ref}`trace-report` page to learn more about the execution report that can be generated by Nextflow.\n\n(config-weblog)=\n\n### Scope `weblog`\n\nThe `weblog` scope allows you to send detailed {ref}`trace <trace-fields>` information as HTTP POST requests to a webserver, shipped as a JSON object.\n\nDetailed information about the JSON fields can be found in the {ref}`weblog description<weblog-service>`.\n\n`weblog.enabled`\n: If `true` it will send HTTP POST requests to a given url.\n\n`weblog.url`\n: The url where to send HTTP POST requests (default: `http:localhost`).\n\n(config-miscellaneous)=\n\n### Miscellaneous\n\nThere are additional variables that can be defined within a configuration file that do not have a dedicated scope.\n\n`cleanup`\n: If `true`, on a successful completion of a run all files in *work* directory are", "start_char_idx": 51584, "end_char_idx": 54994, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "89329e42-8078-42e9-85d4-002d0aa6cbbd": {"__data__": {"id_": "89329e42-8078-42e9-85d4-002d0aa6cbbd", "embedding": null, "metadata": {"file_path": "docs/config.md", "file_name": "config.md"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "5f43dd876f21e8b1a4c175d44180bc9a99b46e3d", "node_type": null, "metadata": {"file_path": "docs/config.md", "file_name": "config.md"}, "hash": "8847079eb331e85ef73b38aefeee43dae1d4f4e09a7d20e4f059d4a72a73f105"}, "2": {"node_id": "fd81bc34-c92f-414c-9813-a0ce486af0bd", "node_type": null, "metadata": {"file_path": "docs/config.md", "file_name": "config.md"}, "hash": "562631c832b885e3e58b31db95b52199971b33cda6996245d275c8ec4f13189e"}, "3": {"node_id": "c1a3b427-4784-48b4-ae4d-655dd4e3ea08", "node_type": null, "metadata": {"file_path": "docs/config.md", "file_name": "config.md"}, "hash": "c6d140c9b3d2d5646c4c1c1518012432e88939bde15835989f9830b28b4db0b8"}}, "hash": "39d479766f12d635b2476e1cdddefc0482a7b8a8f52b3b16b859dc86017a2edf", "text": "If `true`, on a successful completion of a run all files in *work* directory are automatically deleted.\n\n  :::{warning}\n  The use of the `cleanup` option will prevent the use of the *resume* feature on subsequent executions of that pipeline run. Also, be aware that deleting all scratch files can take a lot of time, especially when using a shared file system or remote cloud storage.\n  :::\n\n`dumpHashes`\n: If `true`, dump task hash keys in the log file, for debugging purposes.\n\n(config-profiles)=\n\n## Config profiles\n\nConfiguration files can contain the definition of one or more *profiles*. A profile is a set of configuration attributes that can be selected during pipeline execution by using the `-profile` command line option.\n\nConfiguration profiles are defined by using the special scope `profiles`, which group the attributes that belong to the same profile using a common prefix. For example:\n\n```groovy\nprofiles {\n\n    standard {\n        process.executor = 'local'\n    }\n\n    cluster {\n        process.executor = 'sge'\n        process.queue = 'long'\n        process.memory = '10GB'\n    }\n\n    cloud {\n        process.executor = 'cirrus'\n        process.container = 'cbcrg/imagex'\n        docker.enabled = true\n    }\n\n}\n```\n\nThis configuration defines three different profiles: `standard`, `cluster`, and `cloud`, that each set different process\nconfiguration strategies depending on the target runtime platform. The `standard` profile is used by default when no profile is specified.\n\n:::{tip}\nMultiple configuration profiles can be specified by separating the profile names with a comma, for example:\n\n```bash\nnextflow run <your script> -profile standard,cloud\n```\n:::\n\n:::{danger}\nWhen using the `profiles` feature in your config file, do NOT set attributes in the same scope both inside and outside a `profiles` context. For example:\n\n```groovy\nprocess.cpus = 1\n\nprofiles {\n  foo {\n    process.memory = '2 GB'\n  }\n\n  bar {\n    process.memory = '4 GB'\n  }\n}\n```\n\nIn the above example, the `process.cpus` attribute is not correctly applied because the `process` scope is also used in the `foo` and `bar` profiles.\n:::\n\n(config-env-vars)=\n\n## Environment variables\n\nThe following environment variables control the configuration of the Nextflow runtime and the underlying Java virtual machine.\n\n`NXF_ANSI_LOG`\n: Enables/disables ANSI console output (default `true` when ANSI terminal is detected).\n\n`NXF_ANSI_SUMMARY`\n: Enables/disables ANSI completion summary: `true\\|false` (default: print summary if execution last more than 1 minute).\n\n`NXF_ASSETS`\n: Defines the directory where downloaded pipeline repositories are stored (default: `$NXF_HOME/assets`)\n\n`NXF_CHARLIECLOUD_CACHEDIR`\n: Directory where remote Charliecloud images are stored. When using a computing cluster it must be a shared folder accessible from all compute nodes.\n\n`NXF_CLASSPATH`\n: Allows the extension of the Java runtime classpath with extra JAR files or class folders.\n\n`NXF_CLOUD_DRIVER`\n: Defines the default cloud driver to be used if not specified in the config file or as command line option, either `aws` or `google`.\n\n`NXF_CONDA_CACHEDIR`\n: Directory where Conda environments are stored. When using a computing cluster it must be a shared folder accessible from all compute", "start_char_idx": 55001, "end_char_idx": 58267, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "c1a3b427-4784-48b4-ae4d-655dd4e3ea08": {"__data__": {"id_": "c1a3b427-4784-48b4-ae4d-655dd4e3ea08", "embedding": null, "metadata": {"file_path": "docs/config.md", "file_name": "config.md"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "5f43dd876f21e8b1a4c175d44180bc9a99b46e3d", "node_type": null, "metadata": {"file_path": "docs/config.md", "file_name": "config.md"}, "hash": "8847079eb331e85ef73b38aefeee43dae1d4f4e09a7d20e4f059d4a72a73f105"}, "2": {"node_id": "89329e42-8078-42e9-85d4-002d0aa6cbbd", "node_type": null, "metadata": {"file_path": "docs/config.md", "file_name": "config.md"}, "hash": "39d479766f12d635b2476e1cdddefc0482a7b8a8f52b3b16b859dc86017a2edf"}, "3": {"node_id": "018f7e7c-995c-4587-bc2b-7c4ad1d6a62b", "node_type": null, "metadata": {"file_path": "docs/config.md", "file_name": "config.md"}, "hash": "9112ef855430c436538cb94332e2e2af2e7c1f169d6eb47aaf0df510ac7dc6b5"}}, "hash": "c6d140c9b3d2d5646c4c1c1518012432e88939bde15835989f9830b28b4db0b8", "text": "When using a computing cluster it must be a shared folder accessible from all compute nodes.\n\n`NXF_CONDA_ENABLED`\n: :::{versionadded} 22.08.0-edge\n  :::\n: Enable the use of Conda recipes defined by using the {ref}`process-conda` directive. (default: `false`).\n\n`NXF_DEBUG`\n: Defines scripts debugging level: `1` dump task environment variables in the task log file; `2` enables command script execution tracing; `3` enables command wrapper execution tracing.\n\n`NXF_DEFAULT_DSL`\n: :::{versionadded} 22.03.0-edge\n  :::\n: Defines the DSL version that should be used in not specified otherwise in the script of config file (default: `2`)\n\n`NXF_DISABLE_JOBS_CANCELLATION`\n: :::{versionadded} 21.12.0-edge\n  :::\n: Disables the cancellation of child jobs on workflow execution termination.\n\n`NXF_ENABLE_SECRETS`\n: :::{versionadded} 21.09.0-edge\n  :::\n: Enable Nextflow secrets features (default: `true`)\n\n`NXF_ENABLE_STRICT`\n: :::{versionadded} 22.05.0-edge\n  :::\n: Enable Nextflow *strict* execution mode (default: `false`)\n\n`NXF_EXECUTOR`\n: Defines the default process executor e.g. `sge`\n\n`NXF_HOME`\n: Nextflow home directory (default: `$HOME/.nextflow`).\n\n`NXF_JAVA_HOME`\n: Defines the path location of the Java VM installation used to run Nextflow. This variable overrides the `JAVA_HOME` variable if defined.\n\n`NXF_JVM_ARGS`\n: :::{versionadded} 21.12.1-edge\n  :::\n: Allows the setting Java VM options. This is similar to `NXF_OPTS` however it's only applied the JVM running Nextflow and not to any java pre-launching commands.\n\n`NXF_OFFLINE`\n: When `true` disables the project automatic download and update from remote repositories (default: `false`).\n\n`NXF_OPTS`\n: Provides extra options for the Java and Nextflow runtime. It must be a blank separated list of `-Dkey[=value]` properties.\n\n`NXF_ORG`\n: Default `organization` prefix when looking for a hosted repository (default: `nextflow-io`).\n\n`NXF_PARAMS_FILE`\n: :::{versionadded} 20.10.0\n  :::\n: Defines the path location of the pipeline parameters file .\n\n`NXF_PID_FILE`\n: Name of the file where the process PID is saved when Nextflow is launched in background.\n\n`NXF_SCM_FILE`\n: :::{versionadded} 20.10.0\n  :::\n: Defines the path location of the SCM config file .\n\n`NXF_SINGULARITY_CACHEDIR`\n: Directory where remote Singularity images are stored. When using a computing cluster it must be a shared folder accessible from all compute nodes.\n\n`NXF_SINGULARITY_LIBRARYDIR`\n: :::{versionadded} 21.09.0-edge\n  :::\n: Directory where remote Singularity images are retrieved. It should be a directory accessible to all compute nodes.\n\n`NXF_SPACK_CACHEDIR`\n: Directory where Spack environments are stored. When using a computing cluster it must be a shared folder accessible from all compute nodes.\n\n`NXF_SPACK_ENABLED`\n: :::{versionadded} 23.02.0-edge\n  :::\n: Enable the use of Spack recipes defined by using the {ref}`process-spack` directive. (default:", "start_char_idx": 58263, "end_char_idx": 61165, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "018f7e7c-995c-4587-bc2b-7c4ad1d6a62b": {"__data__": {"id_": "018f7e7c-995c-4587-bc2b-7c4ad1d6a62b", "embedding": null, "metadata": {"file_path": "docs/config.md", "file_name": "config.md"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "5f43dd876f21e8b1a4c175d44180bc9a99b46e3d", "node_type": null, "metadata": {"file_path": "docs/config.md", "file_name": "config.md"}, "hash": "8847079eb331e85ef73b38aefeee43dae1d4f4e09a7d20e4f059d4a72a73f105"}, "2": {"node_id": "c1a3b427-4784-48b4-ae4d-655dd4e3ea08", "node_type": null, "metadata": {"file_path": "docs/config.md", "file_name": "config.md"}, "hash": "c6d140c9b3d2d5646c4c1c1518012432e88939bde15835989f9830b28b4db0b8"}, "3": {"node_id": "b416f012-a8c7-44f1-b5ed-1cf906e8a631", "node_type": null, "metadata": {"file_path": "docs/config.md", "file_name": "config.md"}, "hash": "5344b34126295092e8713b6a7a6e610d6b6d71441e88ae68ce2eb05097de54d8"}}, "hash": "9112ef855430c436538cb94332e2e2af2e7c1f169d6eb47aaf0df510ac7dc6b5", "text": "defined by using the {ref}`process-spack` directive. (default: `false`).\n\n`NXF_TEMP`\n: Directory where temporary files are stored\n\n`NXF_VER`\n: Defines which version of Nextflow to use.\n\n`NXF_WORK`\n: Directory where working files are stored (usually your *scratch* directory)\n\n`NXF_FILE_ROOT`\n: :::{versionadded} 23.05.0-edge\n  :::\n: The file storage path against which relative file paths are resolved.\n: For example, with `NXF_FILE_ROOT=/some/root/path`, the use of `file('foo')` will be resolved to the absolute path `/some/root/path/foo`. A remote root path can be specified using the usual protocol prefix, e.g. `NXF_FILE_ROOT=s3://my-bucket/data`. Files defined using an absolute path are not affected by this setting.\n\n`JAVA_HOME`\n: Defines the path location of the Java VM installation used to run Nextflow.\n\n`JAVA_CMD`\n: Defines the path location of the Java binary command used to launch Nextflow.\n\n`HTTP_PROXY`\n: Defines the HTTP proxy server.\n: :::{versionadded} 21.06.0-edge\n  Proxy authentication is supported by providing the credentials in the proxy URL, e.g. `http://user:password@proxy-host.com:port`.\n  :::\n\n`HTTPS_PROXY`\n: Defines the HTTPS proxy server.\n: :::{versionadded} 21.06.0-edge\n  Proxy authentication is supported by providing the credentials in the proxy URL, e.g. `https://user:password@proxy-host.com:port`.\n  :::\n\n`FTP_PROXY`\n: :::{versionadded} 21.06.0-edge\n  :::\n: Defines the FTP proxy server. Proxy authentication is supported by providing the credentials in the proxy URL, e.g. `ftp://user:password@proxy-host.com:port`.\n\n`NO_PROXY`\n: Defines one or more host names that should not use the proxy server. Separate multiple names using a comma character.\n\n(config-feature-flags)=\n\n## Feature flags\n\nSome features can be enabled using the `nextflow.enable` and `nextflow.preview` flags. These flags can be specified in the pipeline script or the configuration file, and they are generally used to introduce experimental or other opt-in features.\n\n`nextflow.enable.configProcessNamesValidation`\n\n: When `true`, prints a warning for every `withName:` process selector that doesn't match a process in the pipeline (default: `true`).\n\n`nextflow.enable.dsl`\n\n: Defines the DSL version to use (`1` or `2`).\n\n: :::{versionadded} 22.03.0-edge\n  DSL2 is the default DSL version.\n  :::\n\n: :::{versionadded} 22.12.0-edge\n  DSL1 is no longer supported.\n  :::\n\n`nextflow.enable.moduleBinaries`\n\n: When `true`, enables the use of modules with binary scripts. See {ref}`module-binaries` for more information.\n\n`nextflow.enable.strict`\n\n: :::{versionadded} 22.05.0-edge\n  :::\n\n: When `true`, the pipeline is executed in \"strict\" mode, which introduces the following rules:\n\n  - When reading a params file, Nextflow will fail if a dynamic param value references an undefined variable\n\n  - When merging params from a config file with params from the command line, Nextflow will fail if a param is specified from both sources but with different types\n\n  - When using the `join` operator, the `failOnDuplicate` option is `true` by default\n\n  - When using the `join` operator, the `failOnMismatch` option is `true` by default (unless `remainder` is also `true`)\n\n  - When using the", "start_char_idx": 61181, "end_char_idx": 64376, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "b416f012-a8c7-44f1-b5ed-1cf906e8a631": {"__data__": {"id_": "b416f012-a8c7-44f1-b5ed-1cf906e8a631", "embedding": null, "metadata": {"file_path": "docs/config.md", "file_name": "config.md"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "5f43dd876f21e8b1a4c175d44180bc9a99b46e3d", "node_type": null, "metadata": {"file_path": "docs/config.md", "file_name": "config.md"}, "hash": "8847079eb331e85ef73b38aefeee43dae1d4f4e09a7d20e4f059d4a72a73f105"}, "2": {"node_id": "018f7e7c-995c-4587-bc2b-7c4ad1d6a62b", "node_type": null, "metadata": {"file_path": "docs/config.md", "file_name": "config.md"}, "hash": "9112ef855430c436538cb94332e2e2af2e7c1f169d6eb47aaf0df510ac7dc6b5"}}, "hash": "5344b34126295092e8713b6a7a6e610d6b6d71441e88ae68ce2eb05097de54d8", "text": "default (unless `remainder` is also `true`)\n\n  - When using the `publishDir` process directive, the `failOnError` option is `true` by default\n\n  - In a process definition, Nextflow will fail if an input or output tuple has only one element\n\n  - In a process definition, Nextflow will fail if an output emit name is not a valid identifier (i.e. it should match the pattern `/[A-Za-z_][A-Za-z0-9_]*/`)\n\n  - During a process execution, Nextflow will fail if a received input tuple does not have the same number of elements as was declared\n\n  - During a process execution, Nextflow will fail if the `storeDir` directive is used with non-file outputs\n\n  - Nextflow will fail if a pipeline param is referenced before it is defined\n\n  - Nextflow will fail if multiple functions and/or processes with the same name are defined in a module script\n\n`nextflow.preview.recursion`\n\n: :::{versionadded} 21.11.0-edge\n  :::\n\n: *Experimental: may change in a future release.*\n\n: When `true`, enables process and workflow recursion. See [this GitHub discussion](https://github.com/nextflow-io/nextflow/discussions/2521) for more information.", "start_char_idx": 64369, "end_char_idx": 65492, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "7832a54c-1260-4b79-a2fc-2633e6c02f48": {"__data__": {"id_": "7832a54c-1260-4b79-a2fc-2633e6c02f48", "embedding": null, "metadata": {"file_path": "docs/container.md", "file_name": "container.md"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "5f43db1a3196c695750e930895ca2c989594a5d9", "node_type": null, "metadata": {"file_path": "docs/container.md", "file_name": "container.md"}, "hash": "22b5f27b1b9d7b1576687eadfa92ecb87e83de57dc5be6b59eaa41685ee6e053"}, "3": {"node_id": "cb545bda-e589-4c66-9fc5-ad49f436e370", "node_type": null, "metadata": {"file_path": "docs/container.md", "file_name": "container.md"}, "hash": "386df0313fdb837ba705fa929c9bbc0d65c78418bdca6bd7ece4e5c4716f11ed"}}, "hash": "c7ca9ce5052f7a8fb66747bec7c9685ed48036543bfe0c0bd579720b08df7cc0", "text": "(container-page)=\n\n# Containers\n\nNextflow supports a variety of container runtimes. Containerization allows you to write self-contained and truly reproducible computational pipelines, by packaging the binary dependencies of a script into a standard and portable format that can be executed on any platform that supports a container runtime. Furthermore, the same pipeline can be transparently executed with any of the supported container runtimes, depending on which runtimes are available in the target compute environment.\n\n:::{note}\nWhen creating your container image to use with Nextflow, make sure that Bash (3.x or later) and `ps` are installed in your image, along with other tools required for collecting metrics (See  {ref}`this section <execution-report-tasks>`). Also, Bash should be available on the path `/bin/bash` and it should be the container entrypoint.\n:::\n\n(container-apptainer)=\n\n## Apptainer\n\n:::{versionadded} 22.11.0-edge\n:::\n\n[Apptainer](https://apptainer.org) is an alternative container runtime to Docker and an open source fork of Singularity. The main advantages of Apptainer are that it can be used without root privileges and it doesn't require a separate daemon process. These, along with other features such as support for autofs mounts, makes Apptainer better suited to the requirements of HPC workloads. Apptainer is able to use existing Docker images and can pull from Docker registries.\n\n### Prerequisites\n\nYou will need Apptainer installed on your execution environment e.g. your computer or a distributed cluster, depending on where you want to run your pipeline.\n\n### Images\n\nApptainer makes use of a container image file, which physically contains the container. Refer to the [Apptainer documentation](https://apptainer.org/docs) to learn how create Apptainer images.\n\nApptainer allows paths that do not currently exist within the container to be created and mounted dynamically by specifying them on the command line. However this feature is only supported on hosts that support the [Overlay file system](https://en.wikipedia.org/wiki/OverlayFS) and is not enabled by default.\n\n:::{note}\nNextflow expects that data paths are defined system wide, and your Apptainer images need to be created having the mount paths defined in the container file system.\n:::\n\nIf your Apptainer installation support the \"user bind control\" feature, enable the Nextflow support for that by defining the `apptainer.autoMounts = true` setting in the Nextflow configuration file.\n\n### How it works\n\nThe integration for Apptainer follows the same execution model implemented for Docker. You won't need to modify your Nextflow script in order to run it with Apptainer. Simply specify the Apptainer image file from where the containers are started by using the `-with-apptainer` command line option. For example::\n\n```bash\nnextflow run <your script> -with-apptainer [apptainer image file]\n```\n\nEvery time your script launches a process execution, Nextflow will run it into a Apptainer container created by using the specified image. In practice Nextflow will automatically wrap your processes and launch them by running the `apptainer exec` command with the image you have provided.\n\n:::{note}\nA Apptainer image can contain any tool or piece of software you may need to carry out a process execution. Moreover, the container is run in such a way that the process result files are created in the host file system, thus it behaves in a completely transparent manner without requiring extra steps or affecting the flow in your pipeline.\n:::\n\nIf you want to avoid entering the Apptainer image as a command line parameter, you can define it in the Nextflow configuration file. For example you can add the following lines in the `nextflow.config` file:\n\n```groovy\nprocess.container = '/path/to/apptainer.img'\napptainer.enabled = true\n```\n\nIn the above example replace `/path/to/apptainer.img` with any Apptainer image of your", "start_char_idx": 0, "end_char_idx": 3934, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "cb545bda-e589-4c66-9fc5-ad49f436e370": {"__data__": {"id_": "cb545bda-e589-4c66-9fc5-ad49f436e370", "embedding": null, "metadata": {"file_path": "docs/container.md", "file_name": "container.md"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "5f43db1a3196c695750e930895ca2c989594a5d9", "node_type": null, "metadata": {"file_path": "docs/container.md", "file_name": "container.md"}, "hash": "22b5f27b1b9d7b1576687eadfa92ecb87e83de57dc5be6b59eaa41685ee6e053"}, "2": {"node_id": "7832a54c-1260-4b79-a2fc-2633e6c02f48", "node_type": null, "metadata": {"file_path": "docs/container.md", "file_name": "container.md"}, "hash": "c7ca9ce5052f7a8fb66747bec7c9685ed48036543bfe0c0bd579720b08df7cc0"}, "3": {"node_id": "c04ef79d-dd51-402e-9087-9c4534b341ed", "node_type": null, "metadata": {"file_path": "docs/container.md", "file_name": "container.md"}, "hash": "900751bc5ca56f2d6a3962c4f809490045dede93546c432daebafa36aad53d24"}}, "hash": "386df0313fdb837ba705fa929c9bbc0d65c78418bdca6bd7ece4e5c4716f11ed", "text": "`/path/to/apptainer.img` with any Apptainer image of your choice.\n\nRead the {ref}`config-page` page to learn more about the `nextflow.config` file and how to use it to configure your pipeline execution.\n\n:::{note}\nUnlike Docker, Nextflow does not automatically mount host paths in the container when using Apptainer. It expects that the paths are configure and mounted system wide by the Apptainer runtime. If your Apptainer installation allows user defined bind points, read the {ref}`Apptainer configuration <config-apptainer>` section to learn how to enable Nextflow auto mounts.\n:::\n\n:::{warning}\nWhen a process input is a *symbolic link* file, make sure the linked file is stored in a host folder that is accessible from a bind path defined in your Apptainer installation. Otherwise the process execution will fail because the launched container won't be able to access the linked file.\n:::\n\n:::{versionchanged} 23.07.0-edge\nNextflow no longer mounts the home directory when launching an Apptainer container. To re-enable the old behavior, set the environment variable `NXF_APPTAINER_HOME_MOUNT` to `true`.\n:::\n\n### Multiple containers\n\nIt is possible to specify a different Apptainer image for each process definition in your pipeline script. For example, let's suppose you have two processes named `foo` and `bar`. You can specify two different Apptainer images specifying them in the `nextflow.config` file as shown below::\n\n```groovy\nprocess {\n    withName:foo {\n        container = 'image_name_1'\n    }\n    withName:bar {\n        container = 'image_name_2'\n    }\n}\napptainer {\n    enabled = true\n}\n```\n\nRead the {ref}`Process scope <config-process>` section to learn more about processes configuration.\n\n### Apptainer & Docker Hub\n\nNextflow is able to transparently pull remote container images stored in any Docker compatible registry.\n\nBy default when a container name is specified, Nextflow checks if an image file with that name exists in the local file system. If that image file exists, it's used to execute the container. If a matching file does not exist, Nextflow automatically tries to pull an image with the specified name from the container registry.\n\nIf you want Nextflow to check only for local file images, prefix the container name with the `file://` pseudo-protocol. For example:\n\n```groovy\nprocess.container = 'file:///path/to/apptainer.img'\napptainer.enabled = true\n```\n\n:::{warning}\nUse three `/` slashes to specify an **absolute** file path, otherwise the path will be interpreted as relative to the workflow launch directory.\n:::\n\nTo pull images from Apptainer Hub or a third party Docker registry, simply prefix the image name with the `shub://`, `docker://` or `docker-daemon://` pseudo-protocol as required by Apptainer. For example:\n\n```groovy\nprocess.container = 'docker://quay.io/biocontainers/multiqc:1.3--py35_2'\napptainer.enabled = true\n```\n\nYou do not need to specify `docker://` to pull from a Docker repository. Nextflow will automatically prepend it to your image name when Apptainer is enabled. Additionally, the Docker engine will not work with containers specified as `docker://`.\n\n:::{note}\nThis feature requires the `apptainer` tool to be installed where the workflow execution is launched (as opposed to the compute nodes).\n:::\n\nNextflow caches those images in the `apptainer` directory in the pipeline work directory by default. However it is suggested to provide a centralised cache directory by using either the `NXF_APPTAINER_CACHEDIR` environment variable or the `apptainer.cacheDir` setting in the Nextflow config", "start_char_idx": 3884, "end_char_idx": 7454, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "c04ef79d-dd51-402e-9087-9c4534b341ed": {"__data__": {"id_": "c04ef79d-dd51-402e-9087-9c4534b341ed", "embedding": null, "metadata": {"file_path": "docs/container.md", "file_name": "container.md"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "5f43db1a3196c695750e930895ca2c989594a5d9", "node_type": null, "metadata": {"file_path": "docs/container.md", "file_name": "container.md"}, "hash": "22b5f27b1b9d7b1576687eadfa92ecb87e83de57dc5be6b59eaa41685ee6e053"}, "2": {"node_id": "cb545bda-e589-4c66-9fc5-ad49f436e370", "node_type": null, "metadata": {"file_path": "docs/container.md", "file_name": "container.md"}, "hash": "386df0313fdb837ba705fa929c9bbc0d65c78418bdca6bd7ece4e5c4716f11ed"}, "3": {"node_id": "51cebf8b-6391-4eb6-950f-9178e8a441ec", "node_type": null, "metadata": {"file_path": "docs/container.md", "file_name": "container.md"}, "hash": "7e0a9fa3308da655dd288c6d4f5e9ff6c6904dc35b155f48ff706a7bd85ee115"}}, "hash": "900751bc5ca56f2d6a3962c4f809490045dede93546c432daebafa36aad53d24", "text": "environment variable or the `apptainer.cacheDir` setting in the Nextflow config file.\n\n:::{versionadded} 21.09.0-edge\nWhen looking for a Apptainer image file, Nextflow first checks the *library* directory, and if the image file is not found, the *cache* directory is used s usual. The library directory can be defined either using the `NXF_APPTAINER_LIBRARYDIR` environment variable or the `apptainer.libraryDir` configuration setting (the latter overrides the former).\n:::\n\n:::{warning}\nWhen using a compute cluster, the Apptainer cache directory must reside in a shared filesystem accessible to all compute nodes.\n:::\n\n:::{danger}\nWhen pulling Docker images, Apptainer may be unable to determine the container size if the image was stored using an old Docker format, resulting in a pipeline execution error. See the Apptainer documentation for details.\n:::\n\n### Advanced settings\n\nApptainer advanced configuration settings are described in {ref}`config-apptainer` section in the Nextflow configuration page.\n\n(container-charliecloud)=\n\n## Charliecloud\n\n:::{versionadded} 20.12.0-edge\n:::\n\n:::{versionchanged} 21.03.0-edge\nRequires Charliecloud 0.22 to 0.27.\n:::\n\n:::{versionchanged} 22.09.0-edge\nRequires Charliecloud 0.28 or later.\n:::\n\n:::{warning} *Experimental: not recommended for production environments.*\n:::\n\n[Charliecloud](https://hpc.github.io/charliecloud) is an alternative container runtime to Docker, that is better suited for use in HPC environments. Its main advantage is that it can be used without root privileges, making use of user namespaces in the Linux kernel. Charliecloud is able to pull from Docker registries.\n\n### Prerequisites\n\nYou will need Charliecloud installed in your execution environment e.g. on your computer or a distributed cluster, depending on where you want to run your pipeline.\n\n### How it works\n\nYou won't need to modify your Nextflow script in order to run it with Charliecloud. Simply specify the docker image from where the containers are started by using the `-with-charliecloud` command line option. For example:\n\n```bash\nnextflow run <your script> -with-charliecloud [container]\n```\n\nEvery time your script launches a process execution, Nextflow will run it into a charliecloud container created by using the specified container image. In practice Nextflow will automatically wrap your processes and run them by executing the `ch-run` command with the container you have provided.\n\n:::{note}\nA container image can contain any tool or piece of software you may need to carry out a process execution. Moreover, the container is run in such a way that the process result files are created in the host file system, thus it behaves in a completely transparent manner without requiring extra steps or affecting the flow in your pipeline.\n:::\n\nIf you want to avoid entering the Container image as a command line parameter, you can define it in the Nextflow configuration file. For example you can add the following lines in the `nextflow.config` file:\n\n```groovy\nprocess.container = '/path/to/container'\ncharliecloud.enabled = true\n```\n\n:::{warning}\nIf an absolute path is provided, the container needs to be in the Charliecloud flat directory format. See the section below on compatibility with Docker registries.\n:::\n\nRead the {ref}`config-page` page to learn more about the `nextflow.config` file and how to use it to configure your pipeline execution.\n\n:::{warning}\nNextflow automatically manages the file system mounts whenever a container is launched depending on the process input files. However, when a process input is a *symbolic link*, the linked file **must** be stored in the same folder where the symlink is located, or a sub-folder of it. Otherwise the process execution will fail because the launched container won't be able to access the linked file.\n:::\n\n###", "start_char_idx": 7436, "end_char_idx": 11258, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "51cebf8b-6391-4eb6-950f-9178e8a441ec": {"__data__": {"id_": "51cebf8b-6391-4eb6-950f-9178e8a441ec", "embedding": null, "metadata": {"file_path": "docs/container.md", "file_name": "container.md"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "5f43db1a3196c695750e930895ca2c989594a5d9", "node_type": null, "metadata": {"file_path": "docs/container.md", "file_name": "container.md"}, "hash": "22b5f27b1b9d7b1576687eadfa92ecb87e83de57dc5be6b59eaa41685ee6e053"}, "2": {"node_id": "c04ef79d-dd51-402e-9087-9c4534b341ed", "node_type": null, "metadata": {"file_path": "docs/container.md", "file_name": "container.md"}, "hash": "900751bc5ca56f2d6a3962c4f809490045dede93546c432daebafa36aad53d24"}, "3": {"node_id": "6cfda5e0-7219-423e-9372-b61d44ed1b87", "node_type": null, "metadata": {"file_path": "docs/container.md", "file_name": "container.md"}, "hash": "faa87b31b494ec036fe6d0136a456a6e1ab7ac0ea02a44204b4086c65a1e25e8"}}, "hash": "7e0a9fa3308da655dd288c6d4f5e9ff6c6904dc35b155f48ff706a7bd85ee115", "text": "the launched container won't be able to access the linked file.\n:::\n\n### Charliecloud & Docker Hub\n\nNextflow is able to transparently pull remote container images stored in any Docker compatible registry and converts them to the Charliecloud compatible format.\n\nBy default when a container name is specified, Nextflow checks if a container with that name exists in the local file system. If it exists, it's used to execute the container. If a matching file does not exist, Nextflow automatically tries to pull an image with the specified name from the Docker Hub.\n\nTo pull images from a third party Docker registry simply provide the URL to the image. If no URL is provided, Docker Hub is assumed. For example this can be used to pull an image from quay.io and convert it automatically to the Charliecloud container format:\n\n```groovy\nprocess.container = 'https://quay.io/biocontainers/multiqc:1.3--py35_2'\ncharliecloud.enabled = true\n```\n\nWhereas this would pull from Docker Hub:\n\n```groovy\nprocess.container = 'nextflow/examples:latest'\ncharliecloud.enabled = true\n```\n\n### Multiple containers\n\nIt is possible to specify a different Docker image for each process definition in your pipeline script. However, this can't be done with `-with-docker` command line option, since it doesn't support process selectors in the `nextflow.config` file, and doesn't check for the `container` process directive in the pipeline script file. Let's suppose you have two processes named `foo` and `bar`. You can specify two different Docker images for them in the Nextflow script as shown below:\n\n```groovy\nprocess {\n    withName:foo {\n        container = 'image_name_1'\n    }\n    withName:bar {\n        container = 'image_name_2'\n    }\n}\n\ncharliecloud {\n    enabled = true\n}\n```\n\nRead the {ref}`Process scope <config-process>` section to learn more about processes configuration.\n\nAfter running your pipeline, you can easily query the container image that each process used with the following command:\n\n```bash\nnextflow log last -f name,container\n```\n\n### Advanced settings\n\nCharliecloud advanced configuration settings are described in {ref}`config-charliecloud` section in the Nextflow configuration page.\n\n(container-docker)=\n\n## Docker\n\n[Docker](http://www.docker.io) is the industry standard container runtime.\n\n### Prerequisites\n\nYou will need Docker installed on your execution environment e.g. your computer or a distributed cluster, depending on where you want to run your pipeline.\n\nIf you are running Docker on Mac OSX make sure you are mounting your local `/Users` directory into the Docker VM as explained in this excellent tutorial: [How to use Docker on OSX](http://viget.com/extend/how-to-use-docker-on-os-x-the-missing-guide).\n\n### How it works\n\nYou won't need to modify your Nextflow script in order to run it with Docker. Simply specify the Docker image from where the containers are started by using the `-with-docker` command line option. For example:\n\n```bash\nnextflow run <your script> -with-docker [docker image]\n```\n\nEvery time your script launches a process execution, Nextflow will run it into a Docker container created by using the specified image. In practice Nextflow will automatically wrap your processes and run them by executing the `docker run` command with the image you have provided.\n\n:::{note}\nA Docker image can contain any tool or piece of software you may need to carry out a process execution. Moreover, the container is run in such a way that the process result files are created in the host file system, thus it behaves in a completely transparent manner without requiring extra steps or affecting the flow in your pipeline.\n:::\n\nIf you want to avoid", "start_char_idx": 11267, "end_char_idx": 14949, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "6cfda5e0-7219-423e-9372-b61d44ed1b87": {"__data__": {"id_": "6cfda5e0-7219-423e-9372-b61d44ed1b87", "embedding": null, "metadata": {"file_path": "docs/container.md", "file_name": "container.md"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "5f43db1a3196c695750e930895ca2c989594a5d9", "node_type": null, "metadata": {"file_path": "docs/container.md", "file_name": "container.md"}, "hash": "22b5f27b1b9d7b1576687eadfa92ecb87e83de57dc5be6b59eaa41685ee6e053"}, "2": {"node_id": "51cebf8b-6391-4eb6-950f-9178e8a441ec", "node_type": null, "metadata": {"file_path": "docs/container.md", "file_name": "container.md"}, "hash": "7e0a9fa3308da655dd288c6d4f5e9ff6c6904dc35b155f48ff706a7bd85ee115"}, "3": {"node_id": "354d5665-b2a8-4bc4-b831-ea2e21a34727", "node_type": null, "metadata": {"file_path": "docs/container.md", "file_name": "container.md"}, "hash": "ee9562f1f6721a355552add1cf43955a328e712a2d6d2e940c74cc88578e2c0b"}}, "hash": "faa87b31b494ec036fe6d0136a456a6e1ab7ac0ea02a44204b4086c65a1e25e8", "text": "the flow in your pipeline.\n:::\n\nIf you want to avoid entering the Docker image as a command line parameter, you can define it in the Nextflow configuration file. For example you can add the following lines in the `nextflow.config` file:\n\n```groovy\nprocess.container = 'nextflow/examples:latest'\ndocker.enabled = true\n```\n\nIn the above example replace `nextflow/examples:latest` with any Docker image of your choice.\n\nRead the {ref}`config-page` page to learn more about the `nextflow.config` file and how to use it to configure your pipeline execution.\n\n:::{warning}\nNextflow automatically manages the file system mounts whenever a container is launched depending on the process input files. However, when a process input is a *symbolic link*, the linked file **must** be stored in the same folder where the symlink is located, or a sub-folder of it. Otherwise the process execution will fail because the launched container won't be able to access the linked file.\n:::\n\n### Multiple containers\n\nIt is possible to specify a different Docker image for each process definition in your pipeline script. Let's suppose you have two processes named `foo` and `bar`. You can specify two different Docker images for them in the Nextflow script as shown below:\n\n```groovy\nprocess foo {\n  container 'image_name_1'\n\n  '''\n  do this\n  '''\n}\n\nprocess bar {\n  container 'image_name_2'\n\n  '''\n  do that\n  '''\n}\n```\n\nAlternatively, the same containers definitions can be provided by using the `nextflow.config` file as shown below:\n\n```groovy\nprocess {\n    withName:foo {\n        container = 'image_name_1'\n    }\n    withName:bar {\n        container = 'image_name_2'\n    }\n}\n\ndocker {\n    enabled = true\n}\n```\n\nRead the {ref}`Process scope <config-process>` section to learn more about processes configuration.\n\n### Advanced settings\n\nDocker advanced configuration settings are described in {ref}`config-docker` section in the Nextflow configuration page.\n\n(container-podman)=\n\n## Podman\n\n:::{versionadded} 20.01.0\n:::\n\n:::{warning} *Experimental: not recommended for production environments.*\n:::\n\n[Podman](http://www.podman.io) is a drop-in replacement for Docker that can run containers with or without root privileges.\n\n### Prerequisites\n\nYou will need Podman installed on your execution environment e.g. your computer or a distributed cluster, depending on where you want to run your pipeline. Running in rootless mode requires appropriate OS configuration. Due to current Podman limits using cpuset for cpus and memory such is only possible using sudo.\n\n### How it works\n\nYou won't need to modify your Nextflow script in order to run it with Podman. Simply specify the Podman image from where the containers are started by using the `-with-podman` command line option. For example:\n\n```bash\nnextflow run <your script> -with-podman [OCI container image]\n```\n\nEvery time your script launches a process execution, Nextflow will run it into a Podman container created by using the specified image. In practice Nextflow will automatically wrap your processes and run them by executing the `podman run` command with the image you have provided.\n\n:::{note}\nAn OCI container image can contain any tool or piece of software you may need to carry out a process execution. Moreover, the container is run in such a way that the process result files are created in the host file system, thus it behaves in a completely transparent manner without requiring extra steps or affecting the flow in your pipeline.\n:::\n\nIf you want to avoid entering the Podman image as a command line parameter, you can define it in the Nextflow configuration file. For example you can add the", "start_char_idx": 14968, "end_char_idx": 18614, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "354d5665-b2a8-4bc4-b831-ea2e21a34727": {"__data__": {"id_": "354d5665-b2a8-4bc4-b831-ea2e21a34727", "embedding": null, "metadata": {"file_path": "docs/container.md", "file_name": "container.md"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "5f43db1a3196c695750e930895ca2c989594a5d9", "node_type": null, "metadata": {"file_path": "docs/container.md", "file_name": "container.md"}, "hash": "22b5f27b1b9d7b1576687eadfa92ecb87e83de57dc5be6b59eaa41685ee6e053"}, "2": {"node_id": "6cfda5e0-7219-423e-9372-b61d44ed1b87", "node_type": null, "metadata": {"file_path": "docs/container.md", "file_name": "container.md"}, "hash": "faa87b31b494ec036fe6d0136a456a6e1ab7ac0ea02a44204b4086c65a1e25e8"}, "3": {"node_id": "94e0c2a4-9d84-4aaf-b1e9-d35b8bf5285b", "node_type": null, "metadata": {"file_path": "docs/container.md", "file_name": "container.md"}, "hash": "4a806d064f7d44d924697165ad7682d9a0dca0f86884ef1eef0348b1eb05ca8b"}}, "hash": "ee9562f1f6721a355552add1cf43955a328e712a2d6d2e940c74cc88578e2c0b", "text": "you can define it in the Nextflow configuration file. For example you can add the following lines in the `nextflow.config` file:\n\n```groovy\nprocess.container = 'nextflow/examples:latest'\npodman.enabled = true\n```\n\nIn the above example replace `nextflow/examples:latest` with any Podman image of your choice.\n\nRead the {ref}`config-page` page to learn more about the `nextflow.config` file and how to use it to configure your pipeline execution.\n\n:::{warning}\nNextflow automatically manages the file system mounts whenever a container is launched depending on the process input files. However, when a process input is a *symbolic link*, the linked file **must** be stored in the same folder where the symlink is located, or a sub-folder of it. Otherwise the process execution will fail because the launched container won't be able to access the linked file.\n:::\n\n### Multiple containers\n\nIt is possible to specify a different container image for each process definition in your pipeline script. Let's suppose you have two processes named `foo` and `bar`. You can specify two different container images for them in the Nextflow script as shown below:\n\n```groovy\nprocess foo {\n  container 'image_name_1'\n\n  '''\n  do this\n  '''\n}\n\nprocess bar {\n  container 'image_name_2'\n\n  '''\n  do that\n  '''\n}\n```\n\nAlternatively, the same containers definitions can be provided by using the `nextflow.config` file as shown below:\n\n```groovy\nprocess {\n    withName:foo {\n        container = 'image_name_1'\n    }\n    withName:bar {\n        container = 'image_name_2'\n    }\n}\n\npodman {\n    enabled = true\n}\n```\n\nRead the {ref}`Process scope <config-process>` section to learn more about processes configuration.\n\n### Advanced settings\n\nPodman advanced configuration settings are described in {ref}`config-podman` section in the Nextflow configuration page.\n\n(container-sarus)=\n\n## Sarus\n\n:::{versionadded} 22.12.0-edge\nRequires Sarus 1.5.1 or later.\n:::\n\n[Sarus](https://sarus.readthedocs.io) is an alternative container runtime to Docker. Sarus works by converting Docker images to a common format that can then be distributed and launched on HPC systems. The user interface to Sarus enables a user to select an image from [Docker Hub](https://hub.docker.com/) and then submit jobs which run entirely within the container.\n\n### Prerequisites\n\nYou need Sarus installed in your execution environment, i.e. your personal computer or a distributed cluster, depending on where you want to run your pipeline.\n\n### Images\n\nSarus converts a docker image to Squashfs layers which are distributed and launched in the cluster. For more information on how to build Sarus images see the [official documentation](https://sarus.readthedocs.io/en/stable/user/user_guide.html#develop-the-docker-image).\n\n### How it works\n\nThe integration for Sarus, at this time, requires you to set up the following parameters in your config file:\n\n```groovy\nprocess.container = \"dockerhub_user/image_name:image_tag\"\nsarus.enabled = true\n```\n\nand it will always try to search the Docker Hub registry for the images.\n\n:::{note}\nif you do not specify an image tag, the `latest` tag will be fetched by default.\n:::\n\n### Multiple containers\n\nIt is possible to specify a different Sarus image for each process definition in your pipeline script. For example, let's suppose you have two processes named `foo` and `bar`. You can specify two different Sarus images specifying them in the `nextflow.config` file as shown", "start_char_idx": 18592, "end_char_idx": 22051, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "94e0c2a4-9d84-4aaf-b1e9-d35b8bf5285b": {"__data__": {"id_": "94e0c2a4-9d84-4aaf-b1e9-d35b8bf5285b", "embedding": null, "metadata": {"file_path": "docs/container.md", "file_name": "container.md"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "5f43db1a3196c695750e930895ca2c989594a5d9", "node_type": null, "metadata": {"file_path": "docs/container.md", "file_name": "container.md"}, "hash": "22b5f27b1b9d7b1576687eadfa92ecb87e83de57dc5be6b59eaa41685ee6e053"}, "2": {"node_id": "354d5665-b2a8-4bc4-b831-ea2e21a34727", "node_type": null, "metadata": {"file_path": "docs/container.md", "file_name": "container.md"}, "hash": "ee9562f1f6721a355552add1cf43955a328e712a2d6d2e940c74cc88578e2c0b"}, "3": {"node_id": "f81d8bc5-ff42-4de1-b8eb-508b8112b90c", "node_type": null, "metadata": {"file_path": "docs/container.md", "file_name": "container.md"}, "hash": "3bb59f8b7b401e5a44a4625d47f573b744ac554a6d805a2d9d9e259f54648540"}}, "hash": "4a806d064f7d44d924697165ad7682d9a0dca0f86884ef1eef0348b1eb05ca8b", "text": "two different Sarus images specifying them in the `nextflow.config` file as shown below:\n\n```groovy\nprocess {\n    withName:foo {\n        container = 'image_name_1'\n    }\n    withName:bar {\n        container = 'image_name_2'\n    }\n}\n\nsarus {\n    enabled = true\n}\n```\n\nRead the {ref}`Process scope <config-process>` section to learn more about processes configuration.\n\n(container-shifter)=\n\n## Shifter\n\n:::{versionadded} 19.10.0\nRequires Shifter 18.03 or later.\n:::\n\n[Shifter](https://docs.nersc.gov/programming/shifter/overview/) is an alternative container runtime to Docker. Shifter works by converting Docker images to a common format that can then be distributed and launched on HPC systems. The user interface to Shifter enables a user to select an image from [Docker Hub](https://hub.docker.com/) and then submit jobs which run entirely within the container.\n\n### Prerequisites\n\nYou need Shifter and Shifter image gateway installed in your execution environment, i.e: your personal computer or the entry node of a distributed cluster. In the case of a distributed cluster, you should have Shifter installed on all of the compute nodes and the `shifterimg` command should also be available and Shifter properly setup to access the Image gateway, for more information see the [official documentation](https://github.com/NERSC/shifter/tree/master/doc).\n\n### Images\n\nShifter converts a Docker image to squashfs layers which are distributed and launched in the cluster. For more information on how to build Shifter images see the [official documentation](https://docs.nersc.gov/programming/shifter/how-to-use/#building-shifter-images).\n\n### How it works\n\nThe integration for Shifter requires you to set up the following parameters in your config file:\n\n```groovy\nprocess.container = \"dockerhub_user/image_name:image_tag\"\nshifter.enabled = true\n```\n\nShifter will search the Docker Hub registry for the images. If you do not specify an image tag, the `latest` tag will be fetched by default.\n\n### Multiple containers\n\nIt is possible to specify a different Shifter image for each process definition in your pipeline script. For example, let's suppose you have two processes named `foo` and `bar`. You can specify two different Shifter images specifying them in the `nextflow.config` file as shown below:\n\n```groovy\nprocess {\n    withName:foo {\n        container = 'image_name_1'\n    }\n    withName:bar {\n        container = 'image_name_2'\n    }\n}\n\nshifter {\n    enabled = true\n}\n```\n\nRead the {ref}`Process scope <config-process>` section to learn more about processes configuration.\n\n(container-singularity)=\n\n## Singularity\n\n[Singularity](http://singularity.lbl.gov/) is an alternative container runtime to Docker. The main advantages of Singularity are that it can be used without root privileges and it doesn't require a separate daemon process. These, along with other features such as support for autofs mounts, makes Singularity better suited to the requirements of HPC workloads. Singularity is able to use existing Docker images and can pull from Docker registries.\n\n### Prerequisites\n\nYou will need Singularity installed on your execution environment e.g. your computer or a distributed cluster, depending on where you want to run your pipeline.\n\n### Images\n\nSingularity makes use of a container image file, which physically contains the container. Refer to the [Singularity documentation](https://www.sylabs.io/docs/) to learn how create Singularity images.\n\nSingularity allows paths that do not currently exist within the container to be created and mounted", "start_char_idx": 22049, "end_char_idx": 25616, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "f81d8bc5-ff42-4de1-b8eb-508b8112b90c": {"__data__": {"id_": "f81d8bc5-ff42-4de1-b8eb-508b8112b90c", "embedding": null, "metadata": {"file_path": "docs/container.md", "file_name": "container.md"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "5f43db1a3196c695750e930895ca2c989594a5d9", "node_type": null, "metadata": {"file_path": "docs/container.md", "file_name": "container.md"}, "hash": "22b5f27b1b9d7b1576687eadfa92ecb87e83de57dc5be6b59eaa41685ee6e053"}, "2": {"node_id": "94e0c2a4-9d84-4aaf-b1e9-d35b8bf5285b", "node_type": null, "metadata": {"file_path": "docs/container.md", "file_name": "container.md"}, "hash": "4a806d064f7d44d924697165ad7682d9a0dca0f86884ef1eef0348b1eb05ca8b"}, "3": {"node_id": "6bece696-695d-4d03-8972-baf9d18a4319", "node_type": null, "metadata": {"file_path": "docs/container.md", "file_name": "container.md"}, "hash": "6bee223137644827c3d0b78074f79543f3de4578df5e5432d31325ef1644f139"}}, "hash": "3bb59f8b7b401e5a44a4625d47f573b744ac554a6d805a2d9d9e259f54648540", "text": "allows paths that do not currently exist within the container to be created and mounted dynamically by specifying them on the command line. However this feature is only supported on hosts that support the [Overlay file system](https://en.wikipedia.org/wiki/OverlayFS) and is not enabled by default.\n\n:::{note}\nNextflow expects that data paths are defined system wide, and your Singularity images need to be created having the mount paths defined in the container file system.\n:::\n\nIf your Singularity installation support the \"user bind control\" feature, enable the Nextflow support for that by defining the `singularity.autoMounts = true` setting in the Nextflow configuration file.\n\n### How it works\n\nThe integration for Singularity follows the same execution model implemented for Docker. You won't need to modify your Nextflow script in order to run it with Singularity. Simply specify the Singularity image file from where the containers are started by using the `-with-singularity` command line option. For example:\n\n```bash\nnextflow run <your script> -with-singularity [singularity image file]\n```\n\nEvery time your script launches a process execution, Nextflow will run it into a Singularity container created by using the specified image. In practice Nextflow will automatically wrap your processes and launch them by running the `singularity exec` command with the image you have provided.\n\n:::{note}\nA Singularity image can contain any tool or piece of software you may need to carry out a process execution. Moreover, the container is run in such a way that the process result files are created in the host file system, thus it behaves in a completely transparent manner without requiring extra steps or affecting the flow in your pipeline.\n:::\n\nIf you want to avoid entering the Singularity image as a command line parameter, you can define it in the Nextflow configuration file. For example you can add the following lines in the `nextflow.config` file:\n\n```groovy\nprocess.container = '/path/to/singularity.img'\nsingularity.enabled = true\n```\n\nIn the above example replace `/path/to/singularity.img` with any Singularity image of your choice.\n\nRead the {ref}`config-page` page to learn more about the `nextflow.config` file and how to use it to configure your pipeline execution.\n\n:::{note}\nUnlike Docker, Nextflow does not automatically mount host paths in the container when using Singularity. It expects that the paths are configure and mounted system wide by the Singularity runtime. If your Singularity installation allows user defined bind points, read the {ref}`Singularity configuration <config-singularity>` section to learn how to enable Nextflow auto mounts.\n:::\n\n:::{warning}\nWhen a process input is a *symbolic link* file, make sure the linked file is stored in a host folder that is accessible from a bind path defined in your Singularity installation. Otherwise the process execution will fail because the launched container won't be able to access the linked file.\n:::\n\n:::{versionchanged} 23.07.0-edge\nNextflow no longer mounts the home directory when launching a Singularity container. To re-enable the old behavior, set the environment variable `NXF_SINGULARITY_HOME_MOUNT` to `true`.\n:::\n\n### Multiple containers\n\nIt is possible to specify a different Singularity image for each process definition in your pipeline script. For example, let's suppose you have two processes named `foo` and `bar`. You can specify two different Singularity images specifying them in the `nextflow.config` file as shown below:\n\n```groovy\nprocess {\n    withName:foo {\n        container = 'image_name_1'\n    }\n    withName:bar {\n        container = 'image_name_2'\n    }\n}\n\nsingularity {\n    enabled = true\n}\n```\n\nRead the {ref}`Process scope <config-process>` section to learn more about processes configuration.\n\n### Singularity & Docker Hub\n\n*Requires Singularity 2.3 or", "start_char_idx": 25614, "end_char_idx": 29497, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "6bece696-695d-4d03-8972-baf9d18a4319": {"__data__": {"id_": "6bece696-695d-4d03-8972-baf9d18a4319", "embedding": null, "metadata": {"file_path": "docs/container.md", "file_name": "container.md"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "5f43db1a3196c695750e930895ca2c989594a5d9", "node_type": null, "metadata": {"file_path": "docs/container.md", "file_name": "container.md"}, "hash": "22b5f27b1b9d7b1576687eadfa92ecb87e83de57dc5be6b59eaa41685ee6e053"}, "2": {"node_id": "f81d8bc5-ff42-4de1-b8eb-508b8112b90c", "node_type": null, "metadata": {"file_path": "docs/container.md", "file_name": "container.md"}, "hash": "3bb59f8b7b401e5a44a4625d47f573b744ac554a6d805a2d9d9e259f54648540"}}, "hash": "6bee223137644827c3d0b78074f79543f3de4578df5e5432d31325ef1644f139", "text": "Singularity & Docker Hub\n\n*Requires Singularity 2.3 or later*\n\nNextflow is able to transparently pull remote container images stored in the [Singularity-Hub](https://singularity-hub.org/), [Singularity Library](https://cloud.sylabs.io/library/), or any Docker compatible registry.\n\nBy default when a container name is specified, Nextflow checks if an image file with that name exists in the local file system. If that image file exists, it's used to execute the container. If a matching file does not exist, Nextflow automatically tries to pull an image with the specified name from the Docker Hub.\n\nIf you want Nextflow to check only for local file images, prefix the container name with the `file://` pseudo-protocol. For example:\n\n```groovy\nprocess.container = 'file:///path/to/singularity.img'\nsingularity.enabled = true\n```\n\n:::{warning}\nUse three `/` slashes to specify an **absolute** file path, otherwise the path will be interpreted as relative to the workflow launch directory.\n:::\n\nTo pull images from the Singularity Hub or a third party Docker registry simply prefix the image name with the `shub://`, `docker://` or `docker-daemon://` pseudo-protocol as required by the Singularity tool. For example:\n\n```groovy\nprocess.container = 'docker://quay.io/biocontainers/multiqc:1.3--py35_2'\nsingularity.enabled = true\n```\n\nYou do not need to specify `docker://` to pull from a Docker repository. Nextflow will automatically prepend it to your image name when Singularity is enabled. Additionally, the Docker engine will not work with containers specified as `docker://`.\n\n:::{versionadded} 19.04.0\nRequires Singularity 3.0.3 or later.\n:::\n\nNextflow supports the [Singularity Library](https://cloud.sylabs.io/library/) repository:\n\n```groovy\nprocess.container = 'library://library/default/alpine:3.8'\n```\n\nThe `library://` pseudo-protocol allows you to import Singularity images from a local Docker installation instead of downloading them from a Docker registry. This feature requires the `singularity` tool to be installed where the workflow execution is launched (as opposed to the compute nodes).\n\nNextflow caches the images in `${NXF_WORK}/singularity` by default. However, it is recommended to define a centralised cache directory using either the `NXF_SINGULARITY_CACHEDIR` environment variable or the `singularity.cacheDir` setting in the Nextflow config file.\n\n:::{versionadded} 21.09.0-edge\nWhen looking for a Singularity image file, Nextflow first checks the *library* directory, and if the image file is not found, the *cache* directory is used as usual. The library directory can be defined either using the `NXF_SINGULARITY_LIBRARYDIR` environment variable or the `singularity.libraryDir` configuration setting (the latter overrides the former).\n:::\n\n:::{warning}\nWhen using a compute cluster, the Singularity cache directory must reside in a shared filesystem accessible to all compute nodes.\n:::\n\n:::{danger}\nWhen pulling Docker images, Singularity may be unable to determine the container size if the image was stored using an old Docker format, resulting in a pipeline execution error. See the Singularity documentation for details.\n:::\n\n### Advanced settings\n\nSingularity advanced configuration settings are described in {ref}`config-singularity` section in the Nextflow configuration page.", "start_char_idx": 29516, "end_char_idx": 32832, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "e84dfd76-1081-4a09-bd5b-24de0f2ecb54": {"__data__": {"id_": "e84dfd76-1081-4a09-bd5b-24de0f2ecb54", "embedding": null, "metadata": {"file_path": "docs/dsl2.md", "file_name": "dsl2.md"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "036c5d8ecf903500763d02d18d3f5317bfed1923", "node_type": null, "metadata": {"file_path": "docs/dsl2.md", "file_name": "dsl2.md"}, "hash": "4ab7ce336e1659ddde8c196b60a3e586c5cbc582d106bfccc3978c30fad4ac69"}, "3": {"node_id": "a8016ff5-8f66-42f7-b4da-fefc410bbf11", "node_type": null, "metadata": {"file_path": "docs/dsl2.md", "file_name": "dsl2.md"}, "hash": "368a085cf737a5be65f244f77a106ac39b475e6584d6f300bd9a8213e3359680"}}, "hash": "447eaaa63f6d3304defa815e53cec32ef8b7f6279206f5205020b343801bda31", "text": "(dsl2-page)=\n\n# DSL 2\n\nNextflow provides a syntax extension that allows the definition of module libraries and simplifies the writing of complex data analysis pipelines.\n\nTo enable this feature you need to define the following directive at the beginning of your workflow script:\n\n```groovy\nnextflow.enable.dsl=2\n```\n\n:::{versionchanged} 22.03.0-edge\nNextflow uses DSL2 if no version is specified explicitly. You can restore the previous behavior by setting the following environment variable:\n\n```bash\nexport NXF_DEFAULT_DSL=1\n```\n:::\n\n:::{versionchanged} 22.03.0-edge\nThe DSL version specification (either 1 or 2) can also be specified in the Nextflow configuration file using the same notation shown above.\n:::\n\n:::{versionchanged} 22.11.0-edge\nSupport for DSL1 was removed from Nextflow.\n:::\n\n## Function\n\nNextflow allows the definition of custom functions in the workflow script using the following syntax:\n\n```groovy\ndef <function name> ( arg1, arg, .. ) {\n    <function body>\n}\n```\n\nFor example:\n\n```groovy\ndef foo() {\n    'Hello world'\n}\n\ndef bar(alpha, omega) {\n    alpha + omega\n}\n```\n\nThe above snippet defines two simple functions, that can be invoked in the workflow script as `foo()` which returns the `Hello world` string and `bar(10,20)` which returns the sum of two parameters (`30` in this case).\n\n:::{note}\nFunctions implicitly return the result of the last evaluated statement.\n:::\n\nThe keyword `return` can be used to explicitly exit from a function and return the specified value. For example:\n\n```groovy\ndef fib( x ) {\n    if( x <= 1 )\n        return x\n    else\n        fib(x-1) + fib(x-2)\n}\n```\n\n## Process\n\n### Process definition\n\nThe new DSL separates the definition of a process from its invocation. The process definition follows the usual syntax as described in the {ref}`process documentation <process-page>`. The only difference is that the `from` and `into` channel declarations have to be omitted.\n\nThen a process can be invoked as a function in the `workflow` scope, passing the expected input channels as parameters as if it were a custom function. For example:\n\n```groovy\nnextflow.enable.dsl=2\n\nprocess foo {\n    output:\n      path 'foo.txt'\n\n    script:\n      \"\"\"\n      your_command > foo.txt\n      \"\"\"\n}\n\nprocess bar {\n    input:\n      path x\n\n    output:\n      path 'bar.txt'\n\n    script:\n      \"\"\"\n      another_command $x > bar.txt\n      \"\"\"\n}\n\nworkflow {\n    data = channel.fromPath('/some/path/*.txt')\n    foo()\n    bar(data)\n}\n```\n\n:::{warning}\nA process component can be invoked only once in the same workflow context.\n:::\n\n### Process composition\n\nProcesses having matching *input-output* declaration can be composed so that the output of the first process is passed as input to the next process. Taking in consideration the previous example, it's possible to write the following:\n\n```groovy\nworkflow {\n    bar(foo())\n}\n```\n\n### Process output\n\nA process output can also be accessed using the `out` attribute on the corresponding process object. For example:\n\n```groovy\nworkflow {\n    foo()\n", "start_char_idx": 0, "end_char_idx": 3036, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "a8016ff5-8f66-42f7-b4da-fefc410bbf11": {"__data__": {"id_": "a8016ff5-8f66-42f7-b4da-fefc410bbf11", "embedding": null, "metadata": {"file_path": "docs/dsl2.md", "file_name": "dsl2.md"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "036c5d8ecf903500763d02d18d3f5317bfed1923", "node_type": null, "metadata": {"file_path": "docs/dsl2.md", "file_name": "dsl2.md"}, "hash": "4ab7ce336e1659ddde8c196b60a3e586c5cbc582d106bfccc3978c30fad4ac69"}, "2": {"node_id": "e84dfd76-1081-4a09-bd5b-24de0f2ecb54", "node_type": null, "metadata": {"file_path": "docs/dsl2.md", "file_name": "dsl2.md"}, "hash": "447eaaa63f6d3304defa815e53cec32ef8b7f6279206f5205020b343801bda31"}, "3": {"node_id": "0a5284a0-9f00-401d-bfd8-f3a1e54ba28c", "node_type": null, "metadata": {"file_path": "docs/dsl2.md", "file_name": "dsl2.md"}, "hash": "1689e3468d66bbf3288e0d716c9117b316b2b87a3ed2c5f09b11f7e45755d968"}}, "hash": "368a085cf737a5be65f244f77a106ac39b475e6584d6f300bd9a8213e3359680", "text": "For example:\n\n```groovy\nworkflow {\n    foo()\n    bar(foo.out)\n    bar.out.view()\n}\n```\n\nWhen a process defines two or more output channels, each of them can be accessed using the array element operator e.g. `out[0]`, `out[1]`, etc. or using *named outputs* (see below).\n\n### Process named output\n\nThe `emit` option can be added to the process output definition to assign a name identifier. This name can be used to reference the channel within the caller scope. For example:\n\n```groovy\nprocess foo {\n  output:\n    path '*.bam', emit: samples_bam\n\n  '''\n  your_command --here\n  '''\n}\n\nworkflow {\n    foo()\n    foo.out.samples_bam.view()\n}\n```\n\n### Process named stdout\n\nThe `emit` option can be used also to name the stdout:\n\n```groovy\nprocess sayHello {\n    input:\n        val cheers\n\n    output:\n        stdout emit: verbiage\n\n    script:\n    \"\"\"\n    echo -n $cheers\n    \"\"\"\n}\n\nworkflow {\n    things = channel.of('Hello world!', 'Yo, dude!', 'Duck!')\n    sayHello(things)\n    sayHello.out.verbiage.view()\n}\n```\n\n:::{note}\nOptional params for a process input/output are always prefixed with a comma, except for `stdout`. Because `stdout` does not have an associated name or value like other types, the first param should not be prefixed.\n:::\n\n## Workflow\n\n### Workflow definition\n\nThe `workflow` keyword allows the definition of sub-workflow components that enclose the invocation of one or more processes and operators:\n\n```groovy\nworkflow my_pipeline {\n    foo()\n    bar( foo.out.collect() )\n}\n```\n\nFor example, the above snippet defines a workflow component, named `my_pipeline`, that can be invoked from another workflow component definition as any other function or process with `my_pipeline()`.\n\n### Workflow parameters\n\nA workflow component can access any variable and parameter defined in the outer scope:\n\n```groovy\nparams.data = '/some/data/file'\n\nworkflow my_pipeline {\n    if( params.data )\n        bar(params.data)\n    else\n        bar(foo())\n}\n```\n\n### Workflow input\n\nA workflow component can declare one or more input channels using the `take` keyword. For example:\n\n```groovy\nworkflow my_pipeline {\n    take: data\n    main:\n        foo(data)\n        bar(foo.out)\n}\n```\n\n:::{warning}\nWhen the `take` keyword is used, the beginning of the workflow body must be identified with the `main` keyword.\n:::\n\nThen, the input can be specified as an argument in the workflow invocation statement:\n\n```groovy\nworkflow {\n    my_pipeline( channel.from('/some/data') )\n}\n```\n\n:::{note}\nWorkflow inputs are always channels by definition. If a basic data type is provided instead, such as a number, string, list, etc, it is implicitly converted to a {ref}`value channel <channel-type-value>`.\n:::\n\n### Workflow output\n\nA workflow component can declare one or more output channels using the `emit` keyword. For example:\n\n```groovy\nworkflow my_pipeline {\n    main:\n      foo(data)\n ", "start_char_idx": 2998, "end_char_idx": 5878, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "0a5284a0-9f00-401d-bfd8-f3a1e54ba28c": {"__data__": {"id_": "0a5284a0-9f00-401d-bfd8-f3a1e54ba28c", "embedding": null, "metadata": {"file_path": "docs/dsl2.md", "file_name": "dsl2.md"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "036c5d8ecf903500763d02d18d3f5317bfed1923", "node_type": null, "metadata": {"file_path": "docs/dsl2.md", "file_name": "dsl2.md"}, "hash": "4ab7ce336e1659ddde8c196b60a3e586c5cbc582d106bfccc3978c30fad4ac69"}, "2": {"node_id": "a8016ff5-8f66-42f7-b4da-fefc410bbf11", "node_type": null, "metadata": {"file_path": "docs/dsl2.md", "file_name": "dsl2.md"}, "hash": "368a085cf737a5be65f244f77a106ac39b475e6584d6f300bd9a8213e3359680"}, "3": {"node_id": "94b62fd8-02be-4783-92dd-78ea1b7d99d4", "node_type": null, "metadata": {"file_path": "docs/dsl2.md", "file_name": "dsl2.md"}, "hash": "319c89cd9c236b813b390bff12c323ece7702db96496d56b09923a35e4a8300e"}}, "hash": "1689e3468d66bbf3288e0d716c9117b316b2b87a3ed2c5f09b11f7e45755d968", "text": "{\n    main:\n      foo(data)\n      bar(foo.out)\n    emit:\n      bar.out\n}\n```\n\nThen, the result of the `my_pipeline` execution can be accessed using the `out` property, i.e. `my_pipeline.out`. When multiple output channels are declared, use the array bracket notation to access each output channel as described for the [Process output](#process-output) definition.\n\n### Workflow named output\n\nIf the output channel is assigned to an identifier in the `emit` declaration, such identifier can be used to reference the channel within the caller scope. For example:\n\n```groovy\nworkflow my_pipeline {\n   main:\n     foo(data)\n     bar(foo.out)\n   emit:\n     my_data = bar.out\n}\n```\n\nThen, the result of the above snippet can accessed using `my_pipeline.out.my_data`.\n\n### Workflow entrypoint\n\nA workflow definition which does not declare any name (also known as *implicit workflow*) is the entry point of execution for the workflow application.\n\n:::{note}\nImplicit workflow definition is ignored when a script is included as a module. This allows the writing of a workflow script that can be used either as a library module or as an application script.\n:::\n\n:::{tip}\nA different workflow entrypoint can be specified using the `-entry` command line option.\n:::\n\n### Workflow composition\n\nWorkflows defined in your script or imported with [Module inclusion](#module-inclusion) can be invoked and composed as any other process in your application.\n\n```groovy\nworkflow flow1 {\n    take: data\n    main:\n        foo(data)\n        bar(foo.out)\n    emit:\n        bar.out\n}\n\nworkflow flow2 {\n    take: data\n    main:\n        foo(data)\n        baz(foo.out)\n    emit:\n        baz.out\n}\n\nworkflow {\n    take: data\n    main:\n        flow1(data)\n        flow2(flow1.out)\n}\n```\n\n:::{note}\nNested workflow execution determines an implicit scope. Therefore the same process can be invoked in two different workflow scopes, like for example `foo` in the above snippet that is used both in `flow1` and `flow2`. The workflow execution path, along with the process names, determines the *fully qualified process name* that is used to distinguish the two different process invocations, i.e. `flow1:foo` and `flow2:foo` in the above example.\n:::\n\n:::{tip}\nThe fully qualified process name can be used as a valid {ref}`process selector <config-process-selectors>` in the `nextflow.config` file and it has priority over the simple process name.\n:::\n\n## Modules\n\nThe new DSL allows the definition of *module scripts* that can be included and shared across workflow applications.\n\nA module script (or simply, module) can contain the definition of functions, processes and workflows as described in the previous sections.\n\n:::{note}\nFunctions, processes and workflows are globally referred to as *components*.\n:::\n\n### Module inclusion\n\nA component defined in a module script can be imported into another Nextflow script using the `include` keyword.\n\nFor example:\n\n```groovy\ninclude { foo } from './some/module'\n\nworkflow {\n    data = channel.fromPath('/some/data/*.txt')\n    foo(data)\n}\n```\n\nThe above", "start_char_idx": 5900, "end_char_idx": 8967, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "94b62fd8-02be-4783-92dd-78ea1b7d99d4": {"__data__": {"id_": "94b62fd8-02be-4783-92dd-78ea1b7d99d4", "embedding": null, "metadata": {"file_path": "docs/dsl2.md", "file_name": "dsl2.md"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "036c5d8ecf903500763d02d18d3f5317bfed1923", "node_type": null, "metadata": {"file_path": "docs/dsl2.md", "file_name": "dsl2.md"}, "hash": "4ab7ce336e1659ddde8c196b60a3e586c5cbc582d106bfccc3978c30fad4ac69"}, "2": {"node_id": "0a5284a0-9f00-401d-bfd8-f3a1e54ba28c", "node_type": null, "metadata": {"file_path": "docs/dsl2.md", "file_name": "dsl2.md"}, "hash": "1689e3468d66bbf3288e0d716c9117b316b2b87a3ed2c5f09b11f7e45755d968"}, "3": {"node_id": "6920ea8f-5c14-44f2-9ea4-addbc435c4ba", "node_type": null, "metadata": {"file_path": "docs/dsl2.md", "file_name": "dsl2.md"}, "hash": "a01dbb8b5d8f62767dacf7948e56cec5c0afc4e280670460969cd6a14cd1caf7"}}, "hash": "319c89cd9c236b813b390bff12c323ece7702db96496d56b09923a35e4a8300e", "text": "   foo(data)\n}\n```\n\nThe above snippet includes a process with name `foo` defined in the module script in the main execution context. This way, `foo` can be invoked in the `workflow` scope.\n\nNextflow implicitly looks for the script file `./some/module.nf` resolving the path against the *including* script location.\n\n:::{note}\nRelative paths must begin with the `./` prefix. Also, the `include` statement must be defined **outside** of the workflow definition.\n:::\n\n(dsl2-module-directory)=\n\n### Module directory\n\n:::{versionadded} 22.10.0\n:::\n\nThe module can be defined as a directory whose name matches the module name and contains a script named `main.nf`. For example:\n\n```\nsome\n\u2514-module\n  \u2514-main.nf\n```\n\nWhen defined as a directory the module needs to be included specifying the module directory path:\n\n```groovy\ninclude { foo } from './some/module'\n```\n\nModule directories allows the use of module scoped binaries scripts. See [Module binaries](#module-binaries) for details.\n\n### Multiple inclusions\n\nA Nextflow script allows the inclusion of an arbitrary number of modules and components. When multiple components need to be included from the same module script, the component names can be specified in the same inclusion using the curly brackets notation as shown below:\n\n```groovy\ninclude { foo; bar } from './some/module'\n\nworkflow {\n    data = channel.fromPath('/some/data/*.txt')\n    foo(data)\n    bar(data)\n}\n```\n\n### Module aliases\n\nWhen including a module component, it's possible to specify an *alias* with the `as` keyword. This allows the inclusion and the invocation of components with the same name in your script using different names. For example:\n\n```groovy\ninclude { foo } from './some/module'\ninclude { foo as bar } from './other/module'\n\nworkflow {\n    foo(some_data)\n    bar(other_data)\n}\n```\n\nThe same is possible when including the same component multiple times from the same module script as shown below:\n\n```groovy\ninclude { foo; foo as bar } from './some/module'\n\nworkflow {\n    foo(some_data)\n    bar(other_data)\n}\n```\n\n### Module parameters\n\nA module script can define one or more parameters using the same syntax of a Nextflow workflow script:\n\n```groovy\nparams.foo = 'Hello'\nparams.bar = 'world!'\n\ndef sayHello() {\n    println \"$params.foo $params.bar\"\n}\n```\n\nThen, parameters are inherited from the including context. For example:\n\n```groovy\nparams.foo = 'Hola'\nparams.bar = 'Mundo'\n\ninclude {sayHello} from './some/module'\n\nworkflow {\n    sayHello()\n}\n```\n\nThe above snippet prints:\n\n```\nHola Mundo\n```\n\n:::{note}\nThe module inherits the parameters defined *before* the `include` statement, therefore any further parameter set later is ignored.\n:::\n\n:::{tip}\nDefine all pipeline parameters at the beginning of the script *before* any `include` declaration.\n:::\n\nThe option `addParams` can be used to extend the module parameters without affecting the external scope. For example:\n\n```groovy\ninclude {sayHello} from './some/module' addParams(foo: 'Ciao')\n\nworkflow {\n    sayHello()\n}\n```\n\nThe above snippet prints:\n\n```\nCiao world!\n```\n\nFinally, the include option `params` allows the specification of one or more parameters without inheriting any value from the external", "start_char_idx": 8961, "end_char_idx": 12169, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "6920ea8f-5c14-44f2-9ea4-addbc435c4ba": {"__data__": {"id_": "6920ea8f-5c14-44f2-9ea4-addbc435c4ba", "embedding": null, "metadata": {"file_path": "docs/dsl2.md", "file_name": "dsl2.md"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "036c5d8ecf903500763d02d18d3f5317bfed1923", "node_type": null, "metadata": {"file_path": "docs/dsl2.md", "file_name": "dsl2.md"}, "hash": "4ab7ce336e1659ddde8c196b60a3e586c5cbc582d106bfccc3978c30fad4ac69"}, "2": {"node_id": "94b62fd8-02be-4783-92dd-78ea1b7d99d4", "node_type": null, "metadata": {"file_path": "docs/dsl2.md", "file_name": "dsl2.md"}, "hash": "319c89cd9c236b813b390bff12c323ece7702db96496d56b09923a35e4a8300e"}, "3": {"node_id": "dd9c496f-c281-4cec-8c23-233a046ad400", "node_type": null, "metadata": {"file_path": "docs/dsl2.md", "file_name": "dsl2.md"}, "hash": "52327e9df20254c6e9244735c93bc66b327ef5644c3242ee6e8728a6e4f748fb"}}, "hash": "a01dbb8b5d8f62767dacf7948e56cec5c0afc4e280670460969cd6a14cd1caf7", "text": "allows the specification of one or more parameters without inheriting any value from the external environment.\n\n(module-templates)=\n\n### Module templates\n\nThe module script can be defined in an external {ref}`template <process-template>` file. With DSL2 the template file can be placed under the `templates` directory where the module script is located.\n\nFor example, let's suppose to have a project L with a module script defining 2 processes (P1 and P2) and both use templates. The template files can be made available under the local `templates` directory:\n\n```\nProject L\n|\u2500myModules.nf\n\u2514\u2500templates\n  |\u2500P1-template.sh\n  \u2514\u2500P2-template.sh\n```\n\nThen, we have a second project A with a workflow that includes P1 and P2:\n\n```\nPipeline A\n\u2514-main.nf\n```\n\nFinally, we have a third project B with a workflow that includes again P1 and P2:\n\n```\nPipeline B\n\u2514-main.nf\n```\n\nWith the possibility to keep the template files inside the project L, A and B can use the modules defined in L without any changes. A future project C would do the same, just cloning L (if not available on the system) and including its module script.\n\nBeside promoting sharing modules across pipelines, there are several advantages in keeping the module template under the script path:\n\n1. module components are *self-contained*,\n2. module components can be tested independently from the pipeline(s) importing them,\n3. it is possible to create libraries of module components.\n\nUltimately, having multiple template locations allows a more structured organization within the same project. If a project has several module components, and all them use templates, the project could group module scripts and their templates as needed. For example:\n\n```\nbaseDir\n|\u2500main.nf\n|\u2500Phase0-Modules\n  |\u2500mymodules1.nf\n  |\u2500mymodules2.nf\n  \u2514\u2500templates\n    |\u2500P1-template.sh\n    \u2514\u2500P2-template.sh\n|\u2500Phase1-Modules\n  |\u2500mymodules3.nf\n  |\u2500mymodules4.nf\n  \u2514\u2500templates\n    |\u2500P3-template.sh\n    \u2514\u2500P4-template.sh\n\u2514\u2500Phase2-Modules\n  |\u2500mymodules5.nf\n  |\u2500mymodules6.nf\n  \u2514\u2500templates\n    |\u2500P5-template.sh\n    |\u2500P6-template.sh\n    \u2514\u2500P7-template.sh\n```\n\n(module-binaries)=\n\n### Module binaries\n\n:::{versionadded} 22.10.0\n:::\n\nModules can define binary scripts that are locally scoped to the processes defined by the tasks.\n\nTo enable this feature add the following setting in pipeline configuration file:\n\n```groovy\nnextflow.enable.moduleBinaries = true\n```\n\nThe binary scripts must be placed in the module directory names `<module-dir>/resources/usr/bin`:\n\n```\n<module-dir>\n|\u2500main.nf\n\u2514\u2500resources\n  \u2514\u2500usr\n    \u2514\u2500bin\n      |\u2500your-module-script1.sh\n      \u2514\u2500another-module-script2.py\n```\n\nThose scripts will be accessible as any other command in the tasks environment, provided they have been granted the Linux execute permissions.\n\n:::{note}\nThis feature requires the use of a local or shared file system as the pipeline work directory or {ref}`wave-page` when using cloud based executors.\n:::\n\n## Channel forking\n\nUsing the new DSL, Nextflow channels are automatically forked when connecting two or more consumers.\n\nFor example:\n\n```groovy\nchannel\n    .from('Hello','Hola','Ciao')\n", "start_char_idx": 12112, "end_char_idx": 15218, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "dd9c496f-c281-4cec-8c23-233a046ad400": {"__data__": {"id_": "dd9c496f-c281-4cec-8c23-233a046ad400", "embedding": null, "metadata": {"file_path": "docs/dsl2.md", "file_name": "dsl2.md"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "036c5d8ecf903500763d02d18d3f5317bfed1923", "node_type": null, "metadata": {"file_path": "docs/dsl2.md", "file_name": "dsl2.md"}, "hash": "4ab7ce336e1659ddde8c196b60a3e586c5cbc582d106bfccc3978c30fad4ac69"}, "2": {"node_id": "6920ea8f-5c14-44f2-9ea4-addbc435c4ba", "node_type": null, "metadata": {"file_path": "docs/dsl2.md", "file_name": "dsl2.md"}, "hash": "a01dbb8b5d8f62767dacf7948e56cec5c0afc4e280670460969cd6a14cd1caf7"}, "3": {"node_id": "16e76ad7-03e5-433f-a8f3-a891665536a3", "node_type": null, "metadata": {"file_path": "docs/dsl2.md", "file_name": "dsl2.md"}, "hash": "c460a640b5aa03b313c723b49eca12a0ddffb9b8321df0462b9f190533fe125d"}}, "hash": "52327e9df20254c6e9244735c93bc66b327ef5644c3242ee6e8728a6e4f748fb", "text": "   .from('Hello','Hola','Ciao')\n    .set{ cheers }\n\ncheers\n    .map{ it.toUpperCase() }\n    .view()\n\ncheers\n    .map{ it.reverse() }\n    .view()\n```\n\nThe same is valid for the result (channel) of a process execution. Therefore a process output can be consumed by two or more processes without the need to fork it using the `into` operator, making the writing of workflow scripts more fluent and readable.\n\n## Pipes\n\n### The *pipe* operator\n\nNextflow processes and operators can be composed using the `|` *pipe* operator. For example:\n\n```groovy\nprocess foo {\n    input:\n    val data\n\n    output:\n    val result\n\n    exec:\n    result = \"$data world\"\n}\n\nworkflow {\n   channel.from('Hello','Hola','Ciao') | foo | map { it.toUpperCase() } | view\n}\n```\n\nThe above snippet defines a process named `foo` and invokes it passing the content of the `data` channel. The result is then piped to the {ref}`operator-map` operator which converts each string to uppercase and finally, the last {ref}`operator-view` operator prints it.\n\n### The *and* operator\n\nThe `&` *and* operator allows feeding of two or more processes with the content of the same channel(s). For example:\n\n```groovy\nprocess foo {\n    input:\n    val data\n\n    output:\n    val result\n\n    exec:\n    result = \"$data world\"\n}\n\nprocess bar {\n    input:\n    val data\n\n    output:\n    val result\n\n    exec:\n    result = data.toUpperCase()\n}\n\nworkflow {\n    channel.from('Hello') | map { it.reverse() } | (foo & bar) | mix | view\n}\n```\n\nIn the above snippet the channel emitting the `Hello` string is piped with the {ref}`operator-map` which reverses the string value. Then, the result is passed to both `foo` and `bar` processes which are executed in parallel. Each process outputs a channel, and the two channels are merged into a single channel using the {ref}`operator-mix` operator. Finally the result is printed using the {ref}`operator-view` operator.\n\n:::{tip}\nThe break-line operator `\\` can be used to split long statements over multiple lines. The above snippet can also be written as:\n\n```groovy\nworkflow {\n    channel.from('Hello') \\\n      | map { it.reverse() } \\\n      | (foo & bar) \\\n      | mix \\\n      | view\n}\n```\n:::\n\n## DSL2 migration notes\n\n- DSL2 final version is activated using the declaration `nextflow.enable.dsl=2` in place of `nextflow.preview.dsl=2`.\n\n- Process inputs of type `set` have to be replaced with {ref}`tuple <process-input-tuple>`.\n\n- Process outputs of type `set` have to be replaced with {ref}`tuple <process-out-tuple>`.\n\n- Process output option `mode flatten` is no longer available. Replace it using the {ref}`operator-flatten` operator on the corresponding output channel.\n\n- Anonymous and unwrapped includes are not supported anymore. Replace them with an explicit module inclusion. For example:\n\n  ```groovy\n  include './some/library'\n  include bar from './other/library'\n\n  workflow {\n    foo()\n    bar()\n  }\n  ```\n\n  Should be replaced with:\n\n  ```groovy\n  include { foo } from", "start_char_idx": 15273, "end_char_idx": 18250, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "16e76ad7-03e5-433f-a8f3-a891665536a3": {"__data__": {"id_": "16e76ad7-03e5-433f-a8f3-a891665536a3", "embedding": null, "metadata": {"file_path": "docs/dsl2.md", "file_name": "dsl2.md"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "036c5d8ecf903500763d02d18d3f5317bfed1923", "node_type": null, "metadata": {"file_path": "docs/dsl2.md", "file_name": "dsl2.md"}, "hash": "4ab7ce336e1659ddde8c196b60a3e586c5cbc582d106bfccc3978c30fad4ac69"}, "2": {"node_id": "dd9c496f-c281-4cec-8c23-233a046ad400", "node_type": null, "metadata": {"file_path": "docs/dsl2.md", "file_name": "dsl2.md"}, "hash": "52327e9df20254c6e9244735c93bc66b327ef5644c3242ee6e8728a6e4f748fb"}}, "hash": "c460a640b5aa03b313c723b49eca12a0ddffb9b8321df0462b9f190533fe125d", "text": " Should be replaced with:\n\n  ```groovy\n  include { foo } from './some/library'\n  include { bar } from './other/library'\n\n  workflow {\n    foo()\n    bar()\n  }\n  ```\n\n- The use of unqualified value and file elements into input tuples is not allowed anymore. Replace them with a corresponding\n  `val` or `path` qualifier:\n\n  ```groovy\n  process foo {\n  input:\n    tuple X, 'some-file.bam'\n\n  script:\n    '''\n    your_command --in $X some-file.bam\n    '''\n  }\n  ```\n\n  Use:\n\n  ```groovy\n  process foo {\n  input:\n    tuple val(X), path('some-file.bam')\n\n  script:\n    '''\n    your_command --in $X some-file.bam\n    '''\n  }\n  ```\n\n- The use of unqualified value and file elements into output tuples is not allowed anymore. Replace them with a corresponding\n  `val` or `path` qualifier:\n\n  ```groovy\n  process foo {\n  output:\n    tuple X, 'some-file.bam'\n\n  script:\n    X = 'some value'\n    '''\n    your_command > some-file.bam\n    '''\n  }\n  ```\n\n  Use:\n\n  ```groovy\n  process foo {\n  output:\n    tuple val(X), path('some-file.bam')\n\n  script:\n    X = 'some value'\n    '''\n    your_command > some-file.bam\n    '''\n  }\n  ```\n\n- The `bind` channel method (and corresponding `<<` operator) has been deprecated in DSL2.\n\n- The `choice` operator has been deprecated in DSL2. Use {ref}`operator-branch` instead.\n\n- The `close` operator has been deprecated in DSL2.\n\n- The `countBy` operator has been deprecated in DSL2.\n\n- The `create` channel factory has been deprecated in DSL2.\n\n- The `fork` operator has been renamed to {ref}`operator-multimap`.\n\n- The `groupBy` operator has been deprecated in DSL2. Replace it with {ref}`operator-grouptuple`\n\n- The `into` operator has been deprecated in DSL2 since it's not needed anymore.\n\n- The `print` and `println` operators have been deprecated in DSL2. Use {ref}`operator-view` instead.\n\n- The `route` operator has been deprecated in DSL2.\n\n- The `separate` operator has been deprecated in DSL2.\n\n- The `spread` operator has been deprecated in DSL2. Replace it with {ref}`operator-combine`.", "start_char_idx": 18218, "end_char_idx": 20241, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "6629031a-a105-4430-8302-a16a24a7e7fc": {"__data__": {"id_": "6629031a-a105-4430-8302-a16a24a7e7fc", "embedding": null, "metadata": {"file_path": "docs/executor.md", "file_name": "executor.md"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "4adc9c0d92b6708aee44a652137dbf24403c7944", "node_type": null, "metadata": {"file_path": "docs/executor.md", "file_name": "executor.md"}, "hash": "9860c25624910502bd3060b99103d914c1662a23298f102c2ff46182f8e5dfa2"}, "3": {"node_id": "6d3586ab-8145-4b14-9ed8-6e3cd8b5596f", "node_type": null, "metadata": {"file_path": "docs/executor.md", "file_name": "executor.md"}, "hash": "6d4961310c3084ba5a4187b5ec37f1c1cf139cf8e537e5f89ad4808c3c2c8ae8"}}, "hash": "954257115ff52ae2c5c978c4e61194909a203b6c0670376ffdf07726ace38b52", "text": "(executor-page)=\n\n# Executors\n\nIn the Nextflow framework architecture, the *executor* is the component that determines the system where a pipeline process is run and supervises its execution.\n\nThe executor provides an abstraction between the pipeline processes and the underlying execution system. This allows you to write the pipeline functional logic independently from the actual processing platform.\n\nIn other words, you can write your pipeline script once and have it running on your computer, a cluster resource manager, or the cloud \u2014 simply change the executor definition in the Nextflow configuration file.\n\n(awsbatch-executor)=\n\n## AWS Batch\n\nNextflow supports the [AWS Batch](https://aws.amazon.com/batch/) service that allows job submission in the cloud without having to spin out and manage a cluster of virtual machines. AWS Batch uses Docker containers to run tasks, which greatly simplifies pipeline deployment.\n\nThe pipeline processes must specify the Docker image to use by defining the `container` directive, either in the pipeline script or the `nextflow.config` file.\n\nTo enable this executor, set `process.executor = 'awsbatch'` in the `nextflow.config` file.\n\nThe pipeline can be launched either in a local computer, or an EC2 instance. EC2 is suggested for heavy or long-running workloads. Additionally, an S3 bucket must be used as the pipeline work directory.\n\nResource requests and other job characteristics can be controlled via the following process directives:\n\n- {ref}`process-accelerator`\n- {ref}`process-container`\n- {ref}`process-containerOptions`\n- {ref}`process-cpus`\n- {ref}`process-memory`\n- {ref}`process-queue`\n- {ref}`process-time`\n\nSee the {ref}`AWS Batch<aws-batch>` page for further configuration details.\n\n(azurebatch-executor)=\n\n## Azure Batch\n\n:::{versionadded} 21.04.0\n:::\n\nNextflow supports the [Azure Batch](https://azure.microsoft.com/en-us/services/batch/) service that allows job submission in the cloud without having to spin out and manage a cluster of virtual machines. Azure Batch uses Docker containers to run tasks, which greatly simplifies pipeline deployment.\n\nThe pipeline processes must specify the Docker image to use by defining the `container` directive, either in the pipeline script or the `nextflow.config` file.\n\nTo enable this executor, set `process.executor = 'azurebatch'` in the `nextflow.config` file.\n\nThe pipeline can be launched either in a local computer, or a cloud virtual machine. The cloud VM is suggested for heavy or long-running workloads. Additionally, an Azure Blob storage container must be used as the pipeline work directory.\n\nResource requests and other job characteristics can be controlled via the following process directives:\n\n- {ref}`process-container`\n- {ref}`process-containerOptions`\n- {ref}`process-cpus`\n- {ref}`process-machineType`\n- {ref}`process-memory`\n- {ref}`process-queue`\n- {ref}`process-time`\n\nSee the {ref}`Azure Batch <azure-batch>` page for further configuration details.\n\n(bridge-executor)=\n\n## Bridge\n\n:::{versionadded} 22.09.1-edge\n:::\n\n[Bridge](https://github.com/cea-hpc/bridge) is an abstraction layer to ease batch system and resource manager usage in heterogeneous HPC environments.\n\nIt is open source software that can be installed on top of existing classical job schedulers such as Slurm, LSF, or other schedulers. Bridge allows you to submit jobs, get information on running jobs, stop jobs, get information on the cluster system, etc.\n\nFor more details on how to install", "start_char_idx": 0, "end_char_idx": 3497, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "6d3586ab-8145-4b14-9ed8-6e3cd8b5596f": {"__data__": {"id_": "6d3586ab-8145-4b14-9ed8-6e3cd8b5596f", "embedding": null, "metadata": {"file_path": "docs/executor.md", "file_name": "executor.md"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "4adc9c0d92b6708aee44a652137dbf24403c7944", "node_type": null, "metadata": {"file_path": "docs/executor.md", "file_name": "executor.md"}, "hash": "9860c25624910502bd3060b99103d914c1662a23298f102c2ff46182f8e5dfa2"}, "2": {"node_id": "6629031a-a105-4430-8302-a16a24a7e7fc", "node_type": null, "metadata": {"file_path": "docs/executor.md", "file_name": "executor.md"}, "hash": "954257115ff52ae2c5c978c4e61194909a203b6c0670376ffdf07726ace38b52"}, "3": {"node_id": "205e4873-6821-4621-a7c1-5741886f1e91", "node_type": null, "metadata": {"file_path": "docs/executor.md", "file_name": "executor.md"}, "hash": "c11c6531ec9ff19d6b3277bd981e1a6a246e34f0fceb34bda8bc72e00c17ccbc"}}, "hash": "6d4961310c3084ba5a4187b5ec37f1c1cf139cf8e537e5f89ad4808c3c2c8ae8", "text": "get information on the cluster system, etc.\n\nFor more details on how to install the Bridge system, see the [documentation](https://github.com/cea-hpc/bridge).\n\nTo enable the Bridge executor, set `process.executor = 'bridge'` in the `nextflow.config` file.\n\nResource requests and other job characteristics can be controlled via the following process directives:\n\n- {ref}`process-clusterOptions`\n- {ref}`process-cpus`\n- {ref}`process-memory`\n- {ref}`process-queue`\n- {ref}`process-time`\n\n(flux-executor)=\n\n## Flux Executor\n\n:::{versionadded} 22.11.0-edge\n:::\n\nThe `flux` executor allows you to run your pipeline script using the [Flux Framework](https://flux-framework.org).\n\nNextflow manages each process as a separate job that is submitted to the cluster by using the `flux mini submit` command.\n\nTo enable the Flux executor, set `process.executor = 'flux'` in the `nextflow.config` file.\n\nResource requests and other job characteristics can be controlled via the following process directives:\n\n- {ref}`process-clusterOptions`\n- {ref}`process-cpus`\n- {ref}`process-queue`\n- {ref}`process-time`\n\n:::{note}\nFlux does not support the `memory` directive.\n:::\n\n:::{note}\nBy default, Flux will send all output to the `.command.log` file. To send this output to stdout and stderr instead, set `flux.terminalOutput = true` in your config file.\n:::\n\n(ga4ghtes-executor)=\n\n## GA4GH TES\n\n:::{warning} *Experimental: may change in a future release.*\n:::\n\nThe [Task Execution Schema](https://github.com/ga4gh/task-execution-schemas) (TES) project by the [GA4GH](https://www.ga4gh.org) standardization initiative is an effort to define a standardized schema and API for describing batch execution tasks in a portable manner.\n\nNextflow supports the TES API via the `tes` executor, which allows the submission of workflow tasks to a remote execution backend exposing a TES API endpoint.\n\nTo use this feature, define the following variables in the workflow launching environment:\n\n```bash\nexport NXF_MODE=ga4gh\nexport NXF_EXECUTOR=tes\nexport NXF_EXECUTOR_TES_ENDPOINT='http://back.end.com'\n```\n\nIt is important that the endpoint is specified without the trailing slash; otherwise, the resulting URLs will not be normalized and the requests to TES will fail.\n\nYou will then be able to run your workflow over TES using the usual Nextflow command line. Be sure to specify the Docker image to use, i.e.:\n\n```bash\nnextflow run rnaseq-nf -with-docker alpine\n```\n\n:::{note}\nIf the variable `NXF_EXECUTOR_TES_ENDPOINT` is omitted, the default endpoint is `http://localhost:8000`.\n:::\n\n:::{tip}\nYou can use a local [Funnel](https://ohsu-comp-bio.github.io/funnel/) server using the following launch command line:\n\n```bash\n./funnel server --Server.HTTPPort 8000 --LocalStorage.AllowedDirs $HOME run\n```\n\n(tested with version 0.8.0 on macOS)\n:::\n\n:::{warning}\nMake sure the TES backend can access the Nextflow work directory when data is exchanged using a local or shared file system.\n:::\n\n### Known Limitations\n\n- Automatic deployment of workflow scripts in the `bin` folder is not supported.\n\n  :::{versionchanged} 23.07.0-edge\n  Automatic upload of the `bin` directory is now", "start_char_idx": 3431, "end_char_idx": 6581, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "205e4873-6821-4621-a7c1-5741886f1e91": {"__data__": {"id_": "205e4873-6821-4621-a7c1-5741886f1e91", "embedding": null, "metadata": {"file_path": "docs/executor.md", "file_name": "executor.md"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "4adc9c0d92b6708aee44a652137dbf24403c7944", "node_type": null, "metadata": {"file_path": "docs/executor.md", "file_name": "executor.md"}, "hash": "9860c25624910502bd3060b99103d914c1662a23298f102c2ff46182f8e5dfa2"}, "2": {"node_id": "6d3586ab-8145-4b14-9ed8-6e3cd8b5596f", "node_type": null, "metadata": {"file_path": "docs/executor.md", "file_name": "executor.md"}, "hash": "6d4961310c3084ba5a4187b5ec37f1c1cf139cf8e537e5f89ad4808c3c2c8ae8"}, "3": {"node_id": "8788ca26-5277-4dd0-bacc-8a14aad5cfc5", "node_type": null, "metadata": {"file_path": "docs/executor.md", "file_name": "executor.md"}, "hash": "25af124efd3ff711eda69f98326ebc45d54b2160b6b097dce3535e37c86b40d6"}}, "hash": "c11c6531ec9ff19d6b3277bd981e1a6a246e34f0fceb34bda8bc72e00c17ccbc", "text": "23.07.0-edge\n  Automatic upload of the `bin` directory is now supported.\n  :::\n\n- Process output directories are not supported. For details see [#76](https://github.com/ga4gh/task-execution-schemas/issues/76).\n\n- Glob patterns in process output declarations are not supported. For details see [#77](https://github.com/ga4gh/task-execution-schemas/issues/77).\n\n(google-batch-executor)=\n\n## Google Cloud Batch\n\n:::{versionadded} 22.07.1-edge\n:::\n\n[Google Cloud Batch](https://cloud.google.com/batch) is a managed computing service that allows the execution of containerized workloads in the Google Cloud Platform infrastructure.\n\nNextflow provides built-in support for the Cloud Batch API, which allows the seamless deployment of a Nextflow pipeline in the cloud, offloading the process executions as pipelines.\n\nThe pipeline processes must specify the Docker image to use by defining the `container` directive, either in the pipeline script or the `nextflow.config` file. Additionally, the pipeline work directory must be located in a Google Storage bucket.\n\nTo enable this executor, set `process.executor = 'google-batch'` in the `nextflow.config` file.\n\nResource requests and other job characteristics can be controlled via the following process directives:\n\n- {ref}`process-accelerator`\n- {ref}`process-container`\n- {ref}`process-containerOptions`\n- {ref}`process-cpus`\n- {ref}`process-disk`\n- {ref}`process-machineType`\n- {ref}`process-memory`\n- {ref}`process-time`\n- {ref}`process-resourcelabels`\n\nSee the {ref}`Google Cloud Batch <google-batch>` page for further configuration details.\n\n(google-lifesciences-executor)=\n\n## Google Life Sciences\n\n:::{versionadded} 20.01.0\n:::\n\n[Google Cloud Life Sciences](https://cloud.google.com/life-sciences) is a managed computing service that allows the execution of containerized workloads in the Google Cloud Platform infrastructure.\n\nNextflow provides built-in support for the Life Sciences API, which allows the seamless deployment of a Nextflow pipeline in the cloud, offloading the process executions as pipelines.\n\nThe pipeline processes must specify the Docker image to use by defining the `container` directive, either in the pipeline script or the `nextflow.config` file. Additionally, the pipeline work directory must be located in a Google Storage bucket.\n\nTo enable this executor, set `process.executor = 'google-lifesciences'` in the `nextflow.config` file.\n\nResource requests and other job characteristics can be controlled via the following process directives:\n\n- {ref}`process-accelerator`\n- {ref}`process-cpus`\n- {ref}`process-disk`\n- {ref}`process-machineType`\n- {ref}`process-memory`\n- {ref}`process-time`\n\nSee the {ref}`Google Life Sciences <google-lifesciences>` page for further configuration details.\n\n(htcondor-executor)=\n\n## HTCondor\n\n:::{warning} *Experimental: may change in a future release.*\n:::\n\nThe `condor` executor allows you to run your pipeline script by using the [HTCondor](https://research.cs.wisc.edu/htcondor/) resource manager.\n\nNextflow manages each process as a separate job that is submitted to the cluster using the `condor_submit` command.\n\nThe pipeline must be launched from a node where the `condor_submit` command is available, which is typically the cluster login node.\n\n:::{note}\nThe HTCondor executor for Nextflow does not currently support HTCondor's ability to transfer input/output data to the", "start_char_idx": 6597, "end_char_idx": 9989, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "8788ca26-5277-4dd0-bacc-8a14aad5cfc5": {"__data__": {"id_": "8788ca26-5277-4dd0-bacc-8a14aad5cfc5", "embedding": null, "metadata": {"file_path": "docs/executor.md", "file_name": "executor.md"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "4adc9c0d92b6708aee44a652137dbf24403c7944", "node_type": null, "metadata": {"file_path": "docs/executor.md", "file_name": "executor.md"}, "hash": "9860c25624910502bd3060b99103d914c1662a23298f102c2ff46182f8e5dfa2"}, "2": {"node_id": "205e4873-6821-4621-a7c1-5741886f1e91", "node_type": null, "metadata": {"file_path": "docs/executor.md", "file_name": "executor.md"}, "hash": "c11c6531ec9ff19d6b3277bd981e1a6a246e34f0fceb34bda8bc72e00c17ccbc"}, "3": {"node_id": "6b4a08a1-e6e7-4bab-8e55-e27203b74bbd", "node_type": null, "metadata": {"file_path": "docs/executor.md", "file_name": "executor.md"}, "hash": "9bb96e0f560d8079a9087b48c7a7e035976a2b0b7045c60660536d29604693a0"}}, "hash": "25af124efd3ff711eda69f98326ebc45d54b2160b6b097dce3535e37c86b40d6", "text": "Nextflow does not currently support HTCondor's ability to transfer input/output data to the corresponding job's compute node. Therefore, the data must be made accessible to the compute nodes through a shared file system directory from where the Nextflow workflow is executed (or specified via the `-w` option).\n:::\n\nTo enable the HTCondor executor, set `process.executor = 'condor'` in the `nextflow.config` file.\n\nResource requests and other job characteristics can be controlled via the following process directives:\n\n- {ref}`process-clusterOptions`\n- {ref}`process-cpus`\n- {ref}`process-disk`\n- {ref}`process-memory`\n- {ref}`process-time`\n\n(hyperqueue-executor)=\n\n## HyperQueue\n\n:::{versionadded} 22.05.0-edge\n:::\n\n:::{warning} *Experimental: may change in a future release.*\n:::\n\nThe `hyperqueue` executor allows you to run your pipeline script by using the [HyperQueue](https://github.com/It4innovations/hyperqueue) job scheduler.\n\nNextflow manages each process as a separate job that is submitted to the cluster using the `hq` command line tool.\n\nThe pipeline must be launched from a node where the `hq` command is available, which is typically the cluster login node.\n\nTo enable the HTCondor executor, set `process.executor = 'hyperqueue'` in the `nextflow.config` file.\n\nResource requests and other job characteristics can be controlled via the following process directives:\n\n- {ref}`process-accelerator`\n- {ref}`process-clusterOptions`\n- {ref}`process-cpus`\n- {ref}`process-memory`\n- {ref}`process-time`\n\n(ignite-executor)=\n\n## Ignite\n\n:::{warning}\nThis feature is no longer maintained.\n:::\n\n:::{versionchanged} 22.01.0-edge\nThe `ignite` executor must be enabled via the `nf-ignite` plugin.\n:::\n\nThe `ignite` executor allows you to run a pipeline on an [Apache Ignite](https://ignite.apache.org/) cluster.\n\nTo enable this executor, set `process.executor = 'ignite'` in the `nextflow.config` file.\n\nResource requests and other job characteristics can be controlled via the following process directives:\n\n- {ref}`process-cpus`\n- {ref}`process-disk`\n- {ref}`process-memory`\n\nSee the {ref}`ignite-page` page to learn how to configure Nextflow to deploy and run an Ignite cluster in your infrastructure.\n\n(k8s-executor)=\n\n## Kubernetes\n\nThe `k8s` executor allows you to run a pipeline on a [Kubernetes](http://kubernetes.io/) cluster.\n\nResource requests and other job characteristics can be controlled via the following process directives:\n\n- {ref}`process-accelerator`\n- {ref}`process-cpus`\n- {ref}`process-disk`\n- {ref}`process-memory`\n- {ref}`process-pod`\n- {ref}`process-time`\n\nSee the {ref}`Kubernetes <k8s-page>` page to learn how to set up a Kubernetes cluster to run Nextflow pipelines.\n\n(local-executor)=\n\n## Local\n\nThe `local` executor is used by default. It runs the pipeline processes on the computer where Nextflow is launched. The processes are parallelised by spawning multiple threads, taking advantage of the multi-core architecture of the CPU.\n\nThe `local` executor is useful for developing and testing a pipeline script on your computer, before switching to a cluster or cloud environment with production data.\n\n(lsf-executor)=\n\n## LSF\n\nThe `lsf` executor allows you to run your", "start_char_idx": 9963, "end_char_idx": 13164, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "6b4a08a1-e6e7-4bab-8e55-e27203b74bbd": {"__data__": {"id_": "6b4a08a1-e6e7-4bab-8e55-e27203b74bbd", "embedding": null, "metadata": {"file_path": "docs/executor.md", "file_name": "executor.md"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "4adc9c0d92b6708aee44a652137dbf24403c7944", "node_type": null, "metadata": {"file_path": "docs/executor.md", "file_name": "executor.md"}, "hash": "9860c25624910502bd3060b99103d914c1662a23298f102c2ff46182f8e5dfa2"}, "2": {"node_id": "8788ca26-5277-4dd0-bacc-8a14aad5cfc5", "node_type": null, "metadata": {"file_path": "docs/executor.md", "file_name": "executor.md"}, "hash": "25af124efd3ff711eda69f98326ebc45d54b2160b6b097dce3535e37c86b40d6"}, "3": {"node_id": "7e0361c3-0921-4dbb-bab8-bebc722c3a68", "node_type": null, "metadata": {"file_path": "docs/executor.md", "file_name": "executor.md"}, "hash": "0246acd16b107281c672314b9f28ad921876cc8d453b86e4333a94e91ed57b9b"}}, "hash": "9bb96e0f560d8079a9087b48c7a7e035976a2b0b7045c60660536d29604693a0", "text": "LSF\n\nThe `lsf` executor allows you to run your pipeline script using a [Platform LSF](http://en.wikipedia.org/wiki/Platform_LSF) cluster.\n\nNextflow manages each process as a separate job that is submitted to the cluster using the `bsub` command.\n\nThe pipeline must be launched from a node where the `bsub` command is available, which is typically the cluster login node.\n\nTo enable the LSF executor, set `process.executor = 'lsf'` in the `nextflow.config` file.\n\nResource requests and other job characteristics can be controlled via the following process directives:\n\n- {ref}`process-clusterOptions`\n- {ref}`process-cpus`\n- {ref}`process-memory`\n- {ref}`process-queue`\n- {ref}`process-time`\n\n:::{note}\nLSF supports both *per-core* and *per-job* memory limits. Nextflow assumes that LSF works in the *per-core* mode, thus it divides the requested {ref}`process-memory` by the number of requested {ref}`process-cpus`.\n\nWhen LSF is configured to work in the *per-job* memory limit mode, you must specify this limit with the `perJobMemLimit` option in the {ref}`config-executor` scope of your Nextflow config file.\n\nSee also the [Platform LSF documentation](https://www.ibm.com/support/knowledgecenter/SSETD4_9.1.3/lsf_config_ref/lsf.conf.lsb_job_memlimit.5.dita).\n:::\n\n(moab-executor)=\n\n## Moab\n\n:::{versionadded} 19.07.0\n:::\n\n:::{warning} *Experimental: may change in a future release.*\n:::\n\nThe `moab` executor allows you to run your pipeline script using the [Moab](https://en.wikipedia.org/wiki/Moab_Cluster_Suite) resource manager by [Adaptive Computing](http://www.adaptivecomputing.com/).\n\nNextflow manages each process as a separate job that is submitted to the cluster using the `msub` command provided by the resource manager.\n\nThe pipeline must be launched from a node where the `msub` command is available, which is typically the cluster login node.\n\nTo enable the `Moab` executor, set `process.executor = 'moab'` in the `nextflow.config` file.\n\nResource requests and other job characteristics can be controlled via the following process directives:\n\n- {ref}`process-clusterOptions`\n- {ref}`process-cpus`\n- {ref}`process-memory`\n- {ref}`process-queue`\n- {ref}`process-time`\n\n(nqsii-executor)=\n\n## NQSII\n\nThe `nsqii` executor allows you to run your pipeline script using the [NQSII](https://www.rz.uni-kiel.de/en/our-portfolio/hiperf/nec-linux-cluster) resource manager.\n\nNextflow manages each process as a separate job that is submitted to the cluster using the `qsub` command provided by the scheduler.\n\nThe pipeline must be launched from a node where the `qsub` command is available, which is typically the cluster login node.\n\nTo enable the NQSII executor, set `process.executor = 'nqsii'` in the `nextflow.config` file.\n\nResource requests and other job characteristics can be controlled via the following process directives:\n\n- {ref}`process-clusterOptions`\n- {ref}`process-cpus`\n- {ref}`process-memory`\n- {ref}`process-queue`\n- {ref}`process-time`\n\n(oar-executor)=\n\n## OAR\n\n:::{versionadded} 19.11.0-edge\n:::\n\nThe `oar` executor allows you to run your pipeline script using the", "start_char_idx": 13205, "end_char_idx": 16296, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "7e0361c3-0921-4dbb-bab8-bebc722c3a68": {"__data__": {"id_": "7e0361c3-0921-4dbb-bab8-bebc722c3a68", "embedding": null, "metadata": {"file_path": "docs/executor.md", "file_name": "executor.md"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "4adc9c0d92b6708aee44a652137dbf24403c7944", "node_type": null, "metadata": {"file_path": "docs/executor.md", "file_name": "executor.md"}, "hash": "9860c25624910502bd3060b99103d914c1662a23298f102c2ff46182f8e5dfa2"}, "2": {"node_id": "6b4a08a1-e6e7-4bab-8e55-e27203b74bbd", "node_type": null, "metadata": {"file_path": "docs/executor.md", "file_name": "executor.md"}, "hash": "9bb96e0f560d8079a9087b48c7a7e035976a2b0b7045c60660536d29604693a0"}, "3": {"node_id": "e0bba44c-21fb-4c6b-89a2-17bed896ac35", "node_type": null, "metadata": {"file_path": "docs/executor.md", "file_name": "executor.md"}, "hash": "c582ef307f5d32698709ee7cbd1298dec8edfed3703ab5ec462dc335e1bd3f86"}}, "hash": "0246acd16b107281c672314b9f28ad921876cc8d453b86e4333a94e91ed57b9b", "text": "`oar` executor allows you to run your pipeline script using the [OAR](https://oar.imag.fr) resource manager.\n\nNextflow manages each process as a separate job that is submitted to the cluster using the `oarsub` command.\n\nThe pipeline must be launched from a node where the `oarsub` command is available, which is typically the cluster login node.\n\nTo enable the OAR executor set `process.executor = 'oar'` in the `nextflow.config` file.\n\nResource requests and other job characteristics can be controlled via the following process directives:\n\n- {ref}`process-clusterOptions`\n- {ref}`process-cpus`\n- {ref}`process-memory`\n- {ref}`process-queue`\n- {ref}`process-time`\n\n### Known Limitations\n\n- Multiple `clusterOptions` should be semicolon-separated to ensure that the OAR job script is accurately formatted:\n  ```groovy\n  clusterOptions = '-t besteffort;--project myproject'\n  ```\n\n(pbs-executor)=\n\n## PBS/Torque\n\nThe `pbs` executor allows you to run your pipeline script using a resource manager from the [PBS/Torque](http://en.wikipedia.org/wiki/Portable_Batch_System) family of batch schedulers.\n\nNextflow manages each process as a separate job that is submitted to the cluster using the `qsub` command provided by the scheduler.\n\nThe pipeline must be launched from a node where the `qsub` command is available, which is typically the cluster login node.\n\nTo enable the PBS executor, set `process.executor = 'pbs'` in the `nextflow.config` file.\n\nResource requests and other job characteristics can be controlled via the following process directives:\n\n- {ref}`process-clusterOptions`\n- {ref}`process-cpus`\n- {ref}`process-memory`\n- {ref}`process-queue`\n- {ref}`process-time`\n\n(pbspro-executor)=\n\n## PBS Pro\n\nThe `pbspro` executor allows you to run your pipeline script using the [PBS Pro](https://www.pbspro.org/) resource manager.\n\nNextflow manages each process as a separate job that is submitted to the cluster using the `qsub` command provided by the scheduler.\n\nThe pipeline must be launched from a node where the `qsub` command is available, which is typically the cluster login node.\n\nTo enable the PBS Pro executor, set `process.executor = 'pbspro'` in the `nextflow.config` file.\n\nResource requests and other job characteristics can be controlled via the following process directives:\n\n- {ref}`process-clusterOptions`\n- {ref}`process-cpus`\n- {ref}`process-memory`\n- {ref}`process-queue`\n- {ref}`process-time`\n\n(sge-executor)=\n\n## SGE\n\nThe `sge` executor allows you to run your pipeline script using a [Sun Grid Engine](http://en.wikipedia.org/wiki/Oracle_Grid_Engine) cluster or a compatible platform ([Open Grid Engine](http://gridscheduler.sourceforge.net/), [Univa Grid Engine](http://www.univa.com/products/grid-engine.php), etc).\n\nNextflow manages each process as a separate grid job that is submitted to the cluster using the `qsub` command.\n\nThe pipeline must be launched from a node where the `qsub` command is available, which is typically the cluster login node.\n\nTo enable the SGE executor, set `process.executor = 'sge'` in the `nextflow.config` file.\n\nResource requests and other job characteristics can be controlled via the following process directives:\n\n- {ref}`process-clusterOptions`\n- {ref}`process-cpus`\n- {ref}`process-memory`\n-", "start_char_idx": 16283, "end_char_idx": 19542, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "e0bba44c-21fb-4c6b-89a2-17bed896ac35": {"__data__": {"id_": "e0bba44c-21fb-4c6b-89a2-17bed896ac35", "embedding": null, "metadata": {"file_path": "docs/executor.md", "file_name": "executor.md"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "4adc9c0d92b6708aee44a652137dbf24403c7944", "node_type": null, "metadata": {"file_path": "docs/executor.md", "file_name": "executor.md"}, "hash": "9860c25624910502bd3060b99103d914c1662a23298f102c2ff46182f8e5dfa2"}, "2": {"node_id": "7e0361c3-0921-4dbb-bab8-bebc722c3a68", "node_type": null, "metadata": {"file_path": "docs/executor.md", "file_name": "executor.md"}, "hash": "0246acd16b107281c672314b9f28ad921876cc8d453b86e4333a94e91ed57b9b"}}, "hash": "c582ef307f5d32698709ee7cbd1298dec8edfed3703ab5ec462dc335e1bd3f86", "text": "{ref}`process-memory`\n- {ref}`process-penv`\n- {ref}`process-queue`\n- {ref}`process-time`\n\n(slurm-executor)=\n\n## SLURM\n\nThe `slurm` executor allows you to run your pipeline script using the [SLURM](https://slurm.schedmd.com/documentation.html) resource manager.\n\nNextflow manages each process as a separate job that is submitted to the cluster using the `sbatch` command.\n\nThe pipeline must be launched from a node where the `sbatch` command is available, which is typically the cluster login node.\n\nTo enable the SLURM executor, set `process.executor = 'slurm'` in the `nextflow.config` file.\n\nResource requests and other job characteristics can be controlled via the following process directives:\n\n- {ref}`process-clusterOptions`\n- {ref}`process-cpus`\n- {ref}`process-memory`\n- {ref}`process-queue`\n- {ref}`process-time`\n\n:::{note}\nSLURM partitions can be specified with the `queue` directive.\n:::\n\n:::{tip}\nNextflow does not provide direct support for SLURM multi-clusters. If you need to submit workflow executions to a cluster other than the current one, specify it with the `SLURM_CLUSTERS` variable in the launch environment.\n:::", "start_char_idx": 19572, "end_char_idx": 20707, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "5acbedd4-f8d3-4b50-b12b-1929fefd31dd": {"__data__": {"id_": "5acbedd4-f8d3-4b50-b12b-1929fefd31dd", "embedding": null, "metadata": {"file_path": "docs/flux.md", "file_name": "flux.md"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "6e16ac103b09cb71be3f33bbb68b2ef6bc7d3fcc", "node_type": null, "metadata": {"file_path": "docs/flux.md", "file_name": "flux.md"}, "hash": "1ae08974cb80653de0b3a7f66f3339e324ad61faa6e86152b0776a50a4007a2a"}, "3": {"node_id": "f474ca00-0583-43aa-8266-ce97fc931ce4", "node_type": null, "metadata": {"file_path": "docs/flux.md", "file_name": "flux.md"}, "hash": "552721246bdc649a20dd06d4c5c704bb4209f64e539ecaaadb9a91cabca33e6f"}}, "hash": "1b792a18fc77ef94e0742d825a7a1fba0afe75a484d98c8040a013202e329fea", "text": "(flux-page)=\n\n# Flux Framework\n\n:::{versionadded} 22.11.0-edge\n:::\n\nThe [Flux Framework](https://flux-framework.org/) is a modern resource manager that can span the space between cloud and HPC. If your center does not provide Flux for you, you can [build Flux on your own](https://flux-framework.readthedocs.io/en/latest/quickstart.html#building-the-code) and launch it as a job with your resource manager of choice (e.g. SLURM or a cloud provider).\n\n## Tutorial\n\nIn the [`docker/flux`](https://github.com/nextflow-io/nextflow/tree/master/docker/flux) directory we provide a [Dockerfile for interacting with Flux](https://github.com/nextflow-io/nextflow/tree/master/docker/flux/.devcontainer/Dockerfile) along with a [VSCode Developer Container](https://code.visualstudio.com/docs/devcontainers/containers) environment that you can put at the root of the project to be provided with a Flux agent and the dependencies needed to build Nextflow. There are two ways to use this:\n\n- Build a container from scratch and bind your code to it (e.g. for development or testing)\n- Use VSCode and DevContainers to create a more seamless environment\n\nBoth strategies are described below. For this tutorial, you will generally want to prepare a pipeline to use the `flux` executor, create an environment with Flux, start a Flux instance, and interact with it.\n\n### Prepare your pipeline\n\nTo run your pipeline with Flux, you'll want to specify it in your config. Here is an example `nextflow.config`:\n\n```groovy\nmanifest {\n    mainScript = 'demo.nf'\n    homePage = 'https://github.com/nextflow-io/nextflow/tree/master/docker/flux'\n    description = 'Demo using Nextflow with Flux'\n}\n\nprocess {\n    executor = 'flux'\n}\n```\n\nFor additional Flux settings, see the {ref}`flux-executor` section.\n\nHere is an example pipeline that we will use:\n\n```groovy\nworkflow {\n    breakfast = Channel.from '\ud83e\udd5e\ufe0f', '\ud83e\udd51\ufe0f', '\ud83e\udd67\ufe0f', '\ud83c\udf75\ufe0f', '\ud83c\udf5e\ufe0f'\n    haveMeal(breakfast)\n}\n\nprocess haveMeal {\n    debug true\n    input:\n    val food\n    script:\n    \"\"\"\n    printf '$food for breakfast!'\n    \"\"\"\n}\n```\n\n### Container Environment\n\nYou can either build the Docker image from the root of the Nextflow repository:\n\n```console\n$ docker build -f docker/flux/.devcontainer/Dockerfile --platform linux/amd64 -o type=docker -t nextflow-flux .\n```\n\nAnd then shell into the container for a development environment. You'll need to bind the present working directory to `/code` to see your local changes in the container:\n\n```console\n$ docker run -it -v $PWD:/code nextflow-flux\n```\n\nYou can also move the `.devcontainer` directory to the root of your repository, and open it in VSCode:\n\n```console\n$ cp -R docker/flux/.devcontainer .devcontainer\n```\n\nThen open in VSCode, and select **Re-open in container**:\n\n```console\n$ code .\n```\n\nThen you should be able to open a terminal (**Terminal** -> **New Terminal**) to interact with the command line. Try running `make` again! Whichever of these two approaches you take, you should be in a container environment with the `flux` command available.\n\n### Start a Flux Instance\n\nOnce in your container, you can start an interactive Flux instance (from which you can submit jobs on the command line to test with", "start_char_idx": 0, "end_char_idx": 3195, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "f474ca00-0583-43aa-8266-ce97fc931ce4": {"__data__": {"id_": "f474ca00-0583-43aa-8266-ce97fc931ce4", "embedding": null, "metadata": {"file_path": "docs/flux.md", "file_name": "flux.md"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "6e16ac103b09cb71be3f33bbb68b2ef6bc7d3fcc", "node_type": null, "metadata": {"file_path": "docs/flux.md", "file_name": "flux.md"}, "hash": "1ae08974cb80653de0b3a7f66f3339e324ad61faa6e86152b0776a50a4007a2a"}, "2": {"node_id": "5acbedd4-f8d3-4b50-b12b-1929fefd31dd", "node_type": null, "metadata": {"file_path": "docs/flux.md", "file_name": "flux.md"}, "hash": "1b792a18fc77ef94e0742d825a7a1fba0afe75a484d98c8040a013202e329fea"}}, "hash": "552721246bdc649a20dd06d4c5c704bb4209f64e539ecaaadb9a91cabca33e6f", "text": "an interactive Flux instance (from which you can submit jobs on the command line to test with Nextflow) as follows:\n\n```console\n$ flux start --test-size=4\n```\n\n#### Getting Familiar with Flux\n\n:::{note}\nThis step is optional!\n:::\n\nHere is an example of submitting a job and getting the log for it.\n\nFirst submit the job:\n\n```console\n$ flux mini submit echo \"HELLO MOTO\"\n\u0192EzWqspb\n```\n\nThen get the log for it:\n\n```console\n$ flux job attach \u0192EzWqspb\nHELLO MOTO\n```\n\nTry submitting a longer job:\n\n```console\n$ flux mini submit sleep 60\n```\n\nAnd then seeing it in the jobs listing.\n\n```console\n$ flux jobs\n       JOBID USER     NAME       ST NTASKS NNODES     TIME INFO\n   \u01924tkMUAAT root     sleep       R      1      1   2.546s ab6634a491bb\n```\n\n### Submitting with Nextflow\n\nPrepare your `nextflow.config` and `demo.nf` in the same directory.\n\n```console\n$ ls .\ndemo.nf    nextflow.config\n```\n\nIf you've installed Nextflow already, you are good to go! If you are working with development code and need to build Nextflow:\n\n```console\n$ make assemble\n```\n\nMake sure `nextflow` is on your PATH (here we are in the root of the Nextflow repository):\n\n```console\n$ export PATH=$PWD:$PATH\n$ which nextflow\n/workspaces/nextflow/nextflow\n```\n\nThen change to the directory with your config and demo file:\n\n```console\n$ cd docker/flux\n```\n\nAnd then run the pipeline with Flux!\n\n```console\n$ nextflow -c nextflow.config run demo.nf\n\nN E X T F L O W  ~  version 22.10.0\nLaunching `demo.nf` [clever_blackwell] DSL2 - revision: f8cda838cb\nexecutor >  flux (5)\n[4c/f162db] process > haveMeal (3) [100%] 5 of 5 \u2714\n\ud83e\udd5e\ufe0f for breakfast!\n\ud83c\udf5e\ufe0f for breakfast!\n\ud83c\udf75\ufe0f for breakfast!\n\ud83e\udd51\ufe0f for breakfast!\n\ud83e\udd67\ufe0f for breakfast!\n```\n\nAnd that's it! You've just run a pipeline using nextflow and Flux.", "start_char_idx": 3102, "end_char_idx": 4857, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "f26eb4d3-0dcf-4a8f-8cac-9e45f11fb3fc": {"__data__": {"id_": "f26eb4d3-0dcf-4a8f-8cac-9e45f11fb3fc", "embedding": null, "metadata": {"file_path": "docs/fusion.md", "file_name": "fusion.md"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "1aeaeda99d3d4d605e1e5a50c24221a24ec14545", "node_type": null, "metadata": {"file_path": "docs/fusion.md", "file_name": "fusion.md"}, "hash": "9ce7c1d4a80b92c917e8d695a450d9fbf6b51aadd2e0a4a87a59cb803a067300"}, "3": {"node_id": "d003939d-e855-4f7c-aea9-3657ed918f27", "node_type": null, "metadata": {"file_path": "docs/fusion.md", "file_name": "fusion.md"}, "hash": "1bb5d5b20777625e54d13110072034cb30bf59fc7163576a6e472bae3045fc84"}}, "hash": "62c95678b35f23b34e6594fcebd121a3200c536b873a7f344175ab6a6fc5754b", "text": "(fusion-page)=\n\n# Fusion file system\n\n:::{versionadded} 22.10.0\n:::\n\n:::{versionadded} 23.02.0-edge\nSupport for Google Cloud Storage.\n:::\n\n## Introduction\n\nFusion is a distributed virtual file system for cloud-native data pipeline and optimised for Nextflow workloads.\n\nIt bridges the gap between cloud-native storage and data analysis workflow by implementing a thin client that allows any existing application to access object storage using the standard POSIX interface, thus simplifying and speeding up most operations. Currently it supports AWS S3 and Google Cloud Storage.\n\n## Getting started\n\n### Requirements\n\nFusion file system is designed to work with containerised workloads, therefore it requires the use of a container engine such as Docker or a container native platform for the execution of your pipeline e.g. AWS Batch or Kubernetes. It also requires the use of {ref}`Wave containers<wave-page>`.\n\n### AWS S3 configuration\n\nThe AWS S3 bucket should be configured with the following IAM permissions:\n\n```json\n{\n    \"Version\": \"2012-10-17\",\n    \"Statement\": [\n        {\n            \"Effect\": \"Allow\",\n            \"Action\": [\n                \"s3:ListBucket\"\n            ],\n            \"Resource\": [\n                \"arn:aws:s3:::YOUR-BUCKET-NAME\"\n            ]\n        },\n        {\n            \"Action\": [\n                \"s3:GetObject\",\n                \"s3:PutObject\",\n                \"s3:PutObjectTagging\",\n                \"s3:DeleteObject\"\n            ],\n            \"Resource\": [\n                \"arn:aws:s3:::YOUR-BUCKET-NAME/*\"\n            ],\n            \"Effect\": \"Allow\"\n        }\n    ]\n}\n```\n\n## Use cases\n\n### Local execution with S3 bucket as work directory\n\nFusion file system allows the use of an S3 bucket as a pipeline work directory with the Nextflow local executor. This configuration requires the use of Docker (or similar container engine) for the execution of your pipeline tasks.\n\nThe AWS S3 bucket credentials should be made accessible via standard `AWS_ACCESS_KEY_ID` and `AWS_SECRET_ACCESS_KEY` environment variables.\n\nThe following configuration should be added in your Nextflow configuration file:\n\n```groovy\ndocker {\n    enabled = true\n}\n\nfusion {\n    enabled = true\n    exportStorageCredentials = true\n}\n\nwave {\n    enabled = true\n}\n```\n\nThen you can run your pipeline using the following command:\n\n```bash\nnextflow run <YOUR PIPELINE> -work-dir s3://<YOUR BUCKET>/scratch\n```\n\nReplace `<YOUR PIPELINE>` and `<YOUR BUCKET>` with a pipeline script and bucket or your choice, for example:\n\n```bash\nnextflow run https://github.com/nextflow-io/rnaseq-nf -work-dir", "start_char_idx": 0, "end_char_idx": 2599, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "d003939d-e855-4f7c-aea9-3657ed918f27": {"__data__": {"id_": "d003939d-e855-4f7c-aea9-3657ed918f27", "embedding": null, "metadata": {"file_path": "docs/fusion.md", "file_name": "fusion.md"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "1aeaeda99d3d4d605e1e5a50c24221a24ec14545", "node_type": null, "metadata": {"file_path": "docs/fusion.md", "file_name": "fusion.md"}, "hash": "9ce7c1d4a80b92c917e8d695a450d9fbf6b51aadd2e0a4a87a59cb803a067300"}, "2": {"node_id": "f26eb4d3-0dcf-4a8f-8cac-9e45f11fb3fc", "node_type": null, "metadata": {"file_path": "docs/fusion.md", "file_name": "fusion.md"}, "hash": "62c95678b35f23b34e6594fcebd121a3200c536b873a7f344175ab6a6fc5754b"}, "3": {"node_id": "00f70469-9b34-4893-9a87-8f010cefe2a3", "node_type": null, "metadata": {"file_path": "docs/fusion.md", "file_name": "fusion.md"}, "hash": "8e0da0edbbe1833fbc4112c88e3c8d28ab4c91d589339dcbe18cdcca2da025d3"}}, "hash": "1bb5d5b20777625e54d13110072034cb30bf59fc7163576a6e472bae3045fc84", "text": "-work-dir s3://nextflow-ci/scratch\n```\n\n### AWS Batch execution with S3 bucket as work directory\n\nFusion file system allows the use of an S3 bucket as a pipeline work directory with the AWS Batch executor. The use of Fusion makes obsolete the need to create and configure a custom AMI that includes the `aws` command line tool, when setting up the AWS Batch compute environment.\n\nThe configuration for this deployment scenario looks like the following:\n\n```groovy\nfusion {\n    enabled = true\n}\n\nwave {\n    enabled = true\n}\n\nprocess {\n    executor = 'awsbatch'\n    queue = '<YOUR BATCH QUEUE>'\n}\n\naws {\n    region = '<YOUR AWS REGION>'\n}\n```\n\nThen you can run your pipeline using the following command:\n\n```bash\nnextflow run <YOUR PIPELINE> -work-dir s3://<YOUR BUCKET>/scratch\n```\n\n### Kubernetes execution with S3 bucket as work directory\n\nFusion file system allows the use of an S3 bucket as a pipeline work directory with the Kubernetes executor.\n\nThe use of Fusion makes obsolete the need to create and manage and separate persistent volume and shared file system in the Kubernetes cluster.\n\nThe configuration for this deployment scenario looks like the following:\n\n```groovy\nwave {\n    enabled = true\n}\n\nfusion {\n    enabled = true\n}\n\nprocess {\n    executor = 'k8s'\n}\n\nk8s {\n    context = '<YOUR K8S CONFIGURATION CONTEXT>'\n    namespace = '<YOUR K8S NAMESPACE>'\n    serviceAccount = '<YOUR K8S SERVICE ACCOUNT>'\n}\n```\n\nThe `k8s.context` represents the Kubernetes configuration context to be used for the pipeline execution. This setting can be omitted if Nextflow itself is run as a pod in the Kubernetes clusters.\n\nThe `k8s.namespace` represents the Kubernetes namespace where the jobs submitted by the pipeline execution should be executed.\n\nThe `k8s.serviceAccount` represents the Kubernetes service account that should be used to grant the execution permission to jobs launched by Nextflow. You can find more details how to configure it as the [following link](https://github.com/seqeralabs/wave-showcase/tree/master/example8).\n\nHaving the above configuration in place, you can run your pipeline using the following command:\n\n```bash\nnextflow run <YOUR PIPELINE> -work-dir s3://<YOUR BUCKET>/scratch\n```\n\n## NVMe storage\n\nThe Fusion file system implements a lazy download and upload algorithm that runs in the background to transfer files in parallel to and from object storage into a container-local temporary folder. This means that the performance of the temporary folder inside the container (`/tmp` in a default setup) is key to achieving maximum performance.\n\nThe temporary folder is used only as a temporary cache, so the size of the volume can be much lower than the actual needs of your pipeline processes. Fusion has a built-in garbage collector that constantly monitors remaining disk space on the temporary folder and immediately evicts old cached entries when necessary.\n\nThe recommended setup to get maximum performance is to mount a NVMe disk as the temporary folder and run the pipeline with the {ref}`scratch <process-scratch>` directive set to `false` to also avoid stage-out transfer time.\n\nExample configuration for using AWS Batch with [NVMe disks](https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ssd-instance-store.html) to maximize performance:\n\n```groovy\naws.batch.volumes", "start_char_idx": 2591, "end_char_idx": 5901, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "00f70469-9b34-4893-9a87-8f010cefe2a3": {"__data__": {"id_": "00f70469-9b34-4893-9a87-8f010cefe2a3", "embedding": null, "metadata": {"file_path": "docs/fusion.md", "file_name": "fusion.md"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "1aeaeda99d3d4d605e1e5a50c24221a24ec14545", "node_type": null, "metadata": {"file_path": "docs/fusion.md", "file_name": "fusion.md"}, "hash": "9ce7c1d4a80b92c917e8d695a450d9fbf6b51aadd2e0a4a87a59cb803a067300"}, "2": {"node_id": "d003939d-e855-4f7c-aea9-3657ed918f27", "node_type": null, "metadata": {"file_path": "docs/fusion.md", "file_name": "fusion.md"}, "hash": "1bb5d5b20777625e54d13110072034cb30bf59fc7163576a6e472bae3045fc84"}}, "hash": "8e0da0edbbe1833fbc4112c88e3c8d28ab4c91d589339dcbe18cdcca2da025d3", "text": "to maximize performance:\n\n```groovy\naws.batch.volumes = '/path/to/ec2/nvme:/tmp'\nprocess.scratch = false\n```\n\n## Advanced settings\n\nThe following configuration options are available:\n\n`fusion.enabled`\n: Enable/disable the use of Fusion file system.\n\n`fusion.exportStorageCredentials`\n: When `true` the access credentials required by the underlying object storage are exported the pipeline jobs execution environment\n(requires version `23.05.0-edge` or later).\n\n`fusion.containerConfigUrl`\n: The URL from where the container layer provisioning the Fusion client is downloaded. \n\n`fusion.logLevel`\n: The level of logging emitted by the Fusion client.\n\n`fusion.logOutput`\n: Where the logging output is written. \n\n`tagsEnabled`\n: Enable/disable the tagging of files created in the underlying object storage via the Fusion client (default: `true`).\n\n`tagsPattern`\n: The pattern that determines how tags are applied to files created via the Fusion client (default: `[.command.*|.exitcode|.fusion.*](nextflow.io/metadata=true),[*](nextflow.io/temporary=true)`)", "start_char_idx": 5857, "end_char_idx": 6910, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "c1bf8ab0-3b4a-4c6e-a4ae-386430078a30": {"__data__": {"id_": "c1bf8ab0-3b4a-4c6e-a4ae-386430078a30", "embedding": null, "metadata": {"file_path": "docs/getstarted.md", "file_name": "getstarted.md"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "d2ef3a5aebad2315dfabfe610a95a2148843b2f3", "node_type": null, "metadata": {"file_path": "docs/getstarted.md", "file_name": "getstarted.md"}, "hash": "52049d4dedb5af267c1d8058ab6409f1a1eee0799bbd6857baab529a2731e9df"}, "3": {"node_id": "50ec52c6-eb13-47d9-9789-198df925f274", "node_type": null, "metadata": {"file_path": "docs/getstarted.md", "file_name": "getstarted.md"}, "hash": "47d1fdf85f0b0530f94f9adc756c660a85379649ed8c7c6ebe768b9b23aa7ca9"}}, "hash": "69132994e28c45b15071363b5463e6e2c7cc044418a06351585c0bcc777c1bea", "text": "(getstarted-page)=\n\n# Get started\n\n(getstarted-requirement)=\n\n## Requirements\n\nNextflow can be used on any POSIX compatible system (Linux, OS X, etc). It requires Bash 3.2 (or later) and [Java 11 (or later, up to 20)](http://www.oracle.com/technetwork/java/javase/downloads/index.html) to be installed.\n\nFor the execution in a cluster of computers, the use of a shared file system is required to allow the sharing of tasks input/output files.\n\nNextflow can also be run on Windows through [WSL](https://en.wikipedia.org/wiki/Windows_Subsystem_for_Linux).\n\n:::{tip}\nWe recommend that you install Java through [SDKMAN!](https://sdkman.io/), and that you use the latest LTS version of Corretto or Temurin. See [this website](https://whichjdk.com/) for more information. While other Java distros may work at first or even most of the time, many users have experienced issues that are difficult to debug and are usually resolved by using one of the recommended distros.\n\nTo install Corretto 17:\n\n```bash\nsdk install java 17.0.6-amzn\n```\n\nTo install Temurin 17:\n\n```bash\nsdk install java 17.0.6-tem\n```\n:::\n\n(getstarted-install)=\n\n## Installation\n\nNextflow is distributed as a self-installing package, which means that it does not require any special installation procedure.\n\nIt only needs two easy steps:\n\n1. Download the executable package by copying and pasting either one of the following commands in your terminal window: `wget -qO- https://get.nextflow.io | bash`\n\n   Or, if you prefer `curl`: `curl -s https://get.nextflow.io | bash`\n\n   This will create the `nextflow` main executable file in the current directory.\n\n2. Make the binary executable on your system by running `chmod +x nextflow`.\n\n3. Optionally, move the `nextflow` file to a directory accessible by your `$PATH` variable (this is only required to avoid remembering and typing the full path to `nextflow` each time you need to run it).\n\n:::{tip}\nSet `export CAPSULE_LOG=none` to make the dependency installation logs less verbose.\n:::\n\n:::{tip}\nIf you don't have `curl` or `wget`, you can also download the Nextflow launcher script from the [project releases page](https://github.com/nextflow-io/nextflow/releases/latest) on GitHub, in lieu of step 1.\n:::\n\n:::{tip}\nTo avoid downloading the dependencies, you can also use the `nextflow-VERSION-all` distribution available for every Nextflow release on Github.\n\n1. Go to the [Github releases page](https://github.com/nextflow-io/nextflow/releases) and expand the `Assets` section for a specific release.\n2. Copy the URL of the `nextflow-VERSION-all` asset and enter the download command in your terminal, e.g. `wget -qO- ASSET-URL`. It will create the completely self-contained `nextflow-VERSION-all` executable file in the current directory.\n:::\n\n## Updates\n\nHaving Nextflow installed in your computer you can update to the latest version using the following command:\n\n```bash\nnextflow self-update\n```\n\n:::{tip}\nYou can temporarily switch to a specific version of Nextflow by prefixing the `nextflow` command with the `NXF_VER` environment variable. For example:\n\n```bash\nNXF_VER=20.04.0 nextflow run hello\n```\n:::\n\n## Stable and Edge releases\n\nA *stable* version of Nextflow is released on a six-months basic schedule, in the 1st and 3rd quarter of every year.\n\nAlong with the stable release, an *edge* version is released on a monthly basis. This", "start_char_idx": 0, "end_char_idx": 3363, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "50ec52c6-eb13-47d9-9789-198df925f274": {"__data__": {"id_": "50ec52c6-eb13-47d9-9789-198df925f274", "embedding": null, "metadata": {"file_path": "docs/getstarted.md", "file_name": "getstarted.md"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "d2ef3a5aebad2315dfabfe610a95a2148843b2f3", "node_type": null, "metadata": {"file_path": "docs/getstarted.md", "file_name": "getstarted.md"}, "hash": "52049d4dedb5af267c1d8058ab6409f1a1eee0799bbd6857baab529a2731e9df"}, "2": {"node_id": "c1bf8ab0-3b4a-4c6e-a4ae-386430078a30", "node_type": null, "metadata": {"file_path": "docs/getstarted.md", "file_name": "getstarted.md"}, "hash": "69132994e28c45b15071363b5463e6e2c7cc044418a06351585c0bcc777c1bea"}, "3": {"node_id": "392de5e3-6ee2-443c-a63d-0fd4b3e16985", "node_type": null, "metadata": {"file_path": "docs/getstarted.md", "file_name": "getstarted.md"}, "hash": "c0147b8560f22c4586db1107ddfa48705a5a773230bf604372a2722c47372902"}}, "hash": "47d1fdf85f0b0530f94f9adc756c660a85379649ed8c7c6ebe768b9b23aa7ca9", "text": "with the stable release, an *edge* version is released on a monthly basis. This version is useful to test and use most recent updates and experimental features.\n\nTo use the latest edge release run the following snippet in your shell terminal:\n\n```bash\nexport NXF_EDGE=1\nnextflow self-update\n```\n\n(getstarted-first)=\n\n## Your first script\n\nCopy the following example into your favorite text editor and save it to a file named `tutorial.nf`:\n\n```groovy\nparams.str = 'Hello world!'\n\nprocess splitLetters {\n  output:\n    path 'chunk_*'\n\n  \"\"\"\n  printf '${params.str}' | split -b 6 - chunk_\n  \"\"\"\n}\n\nprocess convertToUpper {\n  input:\n    path x\n  output:\n    stdout\n\n  \"\"\"\n  cat $x | tr '[a-z]' '[A-Z]'\n  \"\"\"\n}\n\nworkflow {\n  splitLetters | flatten | convertToUpper | view { it.trim() }\n}\n```\n\n:::{note}\nFor versions of Nextflow prior to `22.10.0`, you must explicitly enable DSL2 by adding `nextflow.enable.dsl=2` to the top of the script or by using the `-dsl2` command-line option.\n:::\n\nThis script defines two processes. The first splits a string into 6-character chunks, writing each one to a file with the prefix `chunk_`, and the second receives these files and transforms their contents to uppercase letters. The resulting strings are emitted on the `result` channel and the final output is printed by the `view` operator.\n\nExecute the script by entering the following command in your terminal:\n\n```console\n$ nextflow run tutorial.nf\n\nN E X T F L O W  ~  version 22.10.0\nexecutor >  local (3)\n[69/c8ea4a] process > splitLetters   [100%] 1 of 1 \u2714\n[84/c8b7f1] process > convertToUpper [100%] 2 of 2 \u2714\nHELLO\nWORLD!\n```\n\nYou can see that the first process is executed once, and the second twice. Finally the result string is printed.\n\nIt's worth noting that the process `convertToUpper` is executed in parallel, so there's no guarantee that the instance processing the first split (the chunk `Hello`) will be executed before the one processing the second split (the chunk `world!`).\n\nThus, it is perfectly possible that you will get the final result printed out in a different order:\n\n```\nWORLD!\nHELLO\n```\n\n:::{tip}\nThe hexadecimal string, e.g. `22/7548fa`, is the unique hash of a task, and the prefix of the directory where the task is executed. You can inspect a task's files by changing to the directory `$PWD/work` and using this string to find the specific task directory.\n:::\n\n(getstarted-resume)=\n\n### Modify and resume\n\nNextflow keeps track of all the processes executed in your pipeline. If you modify some parts of your script, only the processes that are actually changed will be re-executed. The execution of the processes that are not changed will be skipped and the cached result used instead.\n\nThis helps a lot when testing or modifying part of your pipeline without having to re-execute it from scratch.\n\nFor the sake of this tutorial, modify the `convertToUpper` process in the previous example, replacing the process script with the string `rev $x`, so that the process looks like this:\n\n```groovy\nprocess convertToUpper {\n  input:\n    path x\n  output:\n    stdout\n\n  \"\"\"\n  rev $x\n  \"\"\"\n}\n```\n\nThen save the file with the same name, and execute it by adding the `-resume` option to the command", "start_char_idx": 3298, "end_char_idx": 6507, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "392de5e3-6ee2-443c-a63d-0fd4b3e16985": {"__data__": {"id_": "392de5e3-6ee2-443c-a63d-0fd4b3e16985", "embedding": null, "metadata": {"file_path": "docs/getstarted.md", "file_name": "getstarted.md"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "d2ef3a5aebad2315dfabfe610a95a2148843b2f3", "node_type": null, "metadata": {"file_path": "docs/getstarted.md", "file_name": "getstarted.md"}, "hash": "52049d4dedb5af267c1d8058ab6409f1a1eee0799bbd6857baab529a2731e9df"}, "2": {"node_id": "50ec52c6-eb13-47d9-9789-198df925f274", "node_type": null, "metadata": {"file_path": "docs/getstarted.md", "file_name": "getstarted.md"}, "hash": "47d1fdf85f0b0530f94f9adc756c660a85379649ed8c7c6ebe768b9b23aa7ca9"}}, "hash": "c0147b8560f22c4586db1107ddfa48705a5a773230bf604372a2722c47372902", "text": "with the same name, and execute it by adding the `-resume` option to the command line:\n\n```bash\nnextflow run tutorial.nf -resume\n```\n\nIt will print output similar to this:\n\n```\nN E X T F L O W  ~  version 22.10.0\nexecutor >  local (2)\n[69/c8ea4a] process > splitLetters   [100%] 1 of 1, cached: 1 \u2714\n[d0/e94f07] process > convertToUpper [100%] 2 of 2 \u2714\nolleH\n!dlrow\n```\n\nYou will see that the execution of the process `splitLetters` is actually skipped (the process ID is the same), and its results are retrieved from the cache. The second process is executed as expected, printing the reversed strings.\n\n:::{tip}\nThe pipeline results are cached by default in the directory `$PWD/work`. Depending on your script, this folder can take up a lot of disk space. It's a good idea to clean this folder periodically, as long as you know you won't need to resume any pipeline runs.\n:::\n\n(getstarted-params)=\n\n### Pipeline parameters\n\nPipeline parameters are simply declared by prepending to a variable name the prefix `params`, separated by dot character. Their value can be specified on the command line by prefixing the parameter name with a double dash character, i.e. `--paramName`\n\nFor the sake of this tutorial, you can try to execute the previous example specifying a different input string parameter, as shown below:\n\n```bash\nnextflow run tutorial.nf --str 'Bonjour le monde'\n```\n\nThe string specified on the command line will override the default value of the parameter. The output will look like this:\n\n```\nN E X T F L O W  ~  version 22.10.0\nexecutor >  local (4)\n[8b/16e7d7] process > splitLetters   [100%] 1 of 1 \u2714\n[eb/729772] process > convertToUpper [100%] 3 of 3 \u2714\nm el r\nedno\nuojnoB\n```\n\n:::{versionchanged} 20.11.0-edge\nAny `.` (dot) character in a parameter name is interpreted as the delimiter of a nested scope. For example, `--foo.bar Hello` will be interpreted as `params.foo.bar`. If you want to have a parameter name that contains a `.` (dot) character, escape it using the back-slash character, e.g. `--foo\\.bar Hello`.\n:::", "start_char_idx": 6493, "end_char_idx": 8533, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "5a0f079e-c8f8-453c-b868-f7678a5c89ea": {"__data__": {"id_": "5a0f079e-c8f8-453c-b868-f7678a5c89ea", "embedding": null, "metadata": {"file_path": "docs/google.md", "file_name": "google.md"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "36a88465987820e159f5c5c85e7ef67bf6fcee5f", "node_type": null, "metadata": {"file_path": "docs/google.md", "file_name": "google.md"}, "hash": "39e3a1ae1f1177a2e03e0e37405ab4c8a1fa701a9edd130ee288a1d6a7952a90"}, "3": {"node_id": "90f94734-6c75-4bc2-b66f-8bedcaf69a26", "node_type": null, "metadata": {"file_path": "docs/google.md", "file_name": "google.md"}, "hash": "a1e8361ed11ae6114726c04d9fbfd5c11b98464e20c0964c9c0e92a5b7cd6a94"}}, "hash": "81329f64cc822544dc88cdda502849faa5e670f201942800ebeaaa56fa941742", "text": "(google-page)=\n\n# Google Cloud\n\n## Credentials\n\nCredentials for submitting requests to the Google Cloud Batch and Cloud LifeSciences API are picked up from your environment using [Application Default Credentials](https://github.com/googleapis/google-auth-library-java#google-auth-library-oauth2-http). Application Default Credentials are designed to use the credentials most natural to the environment in which a tool runs.\n\nThe most common case will be to pick up your end-user Google credentials from your workstation. You can create these by running the command:\n\n```bash\ngcloud auth application-default login\n```\n\nand running through the authentication flow. This will write a credential file to your gcloud configuration directory that will be used for any tool you run on your workstation that picks up default credentials.\n\nThe next most common case would be when running on a Compute Engine VM. In this case, Application Default Credentials will pick up the Compute Engine Service Account credentials for that VM.\n\nSee the [Application Default Credentials](https://github.com/googleapis/google-auth-library-java#google-auth-library-oauth2-http) documentation for how to enable other use cases.\n\nFinally, the `GOOGLE_APPLICATION_CREDENTIALS` environment variable can be used to specify location of the Google credentials file.\n\nIf you don't have it, the credentials file can be downloaded from the Google Cloud Console following these steps:\n\n- Open the [Google Cloud Console](https://console.cloud.google.com)\n- Go to APIs & Services \u2192 Credentials\n- Click on the *Create credentials* (blue) drop-down and choose *Service account key*, in the following page\n- Select an existing *Service account* or create a new one if needed\n- Select JSON as *Key type*\n- Click the *Create* button and download the JSON file giving a name of your choice e.g. `creds.json`.\n\nThen, define the following variable replacing the path in the example with the one of your credentials file just downloaded:\n\n```bash\nexport GOOGLE_APPLICATION_CREDENTIALS=\"/path/your/file/creds.json\"\n```\n\n(google-batch)=\n\n## Cloud Batch\n\n:::{versionadded} 22.07.1-edge\n:::\n\n[Google Cloud Batch](https://cloud.google.com/batch) is a managed computing service that allows the execution of containerized workloads in the Google Cloud Platform infrastructure.\n\nNextflow provides built-in support for Google Cloud Batch, allowing the seamless deployment of Nextflow pipelines in the cloud, in which tasks are offloaded to the Cloud Batch service.\n\nRead the {ref}`Google Cloud Batch executor <google-batch-executor>` section to learn more about the `google-batch` executor in Nextflow.\n\n(google-batch-config)=\n\n### Configuration\n\nMake sure to have defined in your environment the `GOOGLE_APPLICATION_CREDENTIALS` variable. See the [Credentials](#credentials) section for details.\n\n:::{note}\nMake sure your Google account is allowed to access the Google Cloud Batch service by checking the [APIs & Services](https://console.cloud.google.com/apis/dashboard) dashboard.\n:::\n\nCreate or edit the file `nextflow.config` in your project root directory. The config must specify the following parameters:\n\n- Google Cloud Batch as Nextflow executor\n- The Docker container image(s) for pipeline tasks\n- The Google Cloud project ID and location\n\nExample:\n\n```groovy\nprocess {\n    executor = 'google-batch'\n    container = 'your/container:latest'\n}\n\ngoogle {\n    project = 'your-project-id'\n    location = 'us-central1'\n}\n```\n\nNotes:\n\n- A container image must be specified to execute processes. You can use a different Docker image for each process using one or more {ref}`config-process-selectors`.\n-", "start_char_idx": 0, "end_char_idx": 3649, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "90f94734-6c75-4bc2-b66f-8bedcaf69a26": {"__data__": {"id_": "90f94734-6c75-4bc2-b66f-8bedcaf69a26", "embedding": null, "metadata": {"file_path": "docs/google.md", "file_name": "google.md"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "36a88465987820e159f5c5c85e7ef67bf6fcee5f", "node_type": null, "metadata": {"file_path": "docs/google.md", "file_name": "google.md"}, "hash": "39e3a1ae1f1177a2e03e0e37405ab4c8a1fa701a9edd130ee288a1d6a7952a90"}, "2": {"node_id": "5a0f079e-c8f8-453c-b868-f7678a5c89ea", "node_type": null, "metadata": {"file_path": "docs/google.md", "file_name": "google.md"}, "hash": "81329f64cc822544dc88cdda502849faa5e670f201942800ebeaaa56fa941742"}, "3": {"node_id": "8fdc6edc-7354-446d-b226-25a7cfe06b39", "node_type": null, "metadata": {"file_path": "docs/google.md", "file_name": "google.md"}, "hash": "1ba1b35a7032883bf80592b4c09cd59cbebdc902125a924f23221677812e5b67"}}, "hash": "a1e8361ed11ae6114726c04d9fbfd5c11b98464e20c0964c9c0e92a5b7cd6a94", "text": "for each process using one or more {ref}`config-process-selectors`.\n- Make sure to specify the project ID, not the project name.\n- Make sure to specify a location where Google Batch is available. Refer to the [Google Batch documentation](https://cloud.google.com/batch/docs/get-started#locations) for region availability.\n\nRead the {ref}`Google configuration<config-google>` section to learn more about advanced configuration options.\n\n(google-batch-process)=\n\n### Process definition\n\nProcesses can be defined as usual and by default the `cpus` and `memory` directives are used to find the cheapest machine type available at current location that fits the requested resources. If `memory` is not specified, 1GB of memory is allocated per cpu.\n\n:::{versionadded} 23.02.0-edge\nThe `machineType` directive can be a list of patterns separated by comma. The pattern can contain a `*` to match any number of characters and `?` to match any single character. Examples of valid patterns: `c2-*`, `m?-standard*`, `n*`.\n\nAlternatively it can also be used to define a specific predefined Google Compute Platform [machine type](https://cloud.google.com/compute/docs/machine-types) or a custom machine type.\n:::\n\nExamples:\n\n```groovy\nprocess automatic_resources_task {\n    cpus 8\n    memory '40 GB'\n\n    \"\"\"\n    <Your script here>\n    \"\"\"\n}\n\nprocess allowing_some_series {\n    cpus 8\n    memory '20 GB'\n    machineType 'n2-*,c2-*,m3-*'\n\n    \"\"\"\n    <Your script here>\n    \"\"\"\n}\n\nprocess predefined_resources_task {\n    machineType 'n1-highmem-8'\n\n    \"\"\"\n    <Your script here>\n    \"\"\"\n}\n```\n\n:::{versionadded} 23.06.0-edge\n:::\n\nThe `disk` directive can be used to set the boot disk size or provision a disk for scratch storage. If the disk type is specified with the `type` option, a new disk will be mounted to the task VM at `/tmp` with the requested size and type. Otherwise, it will set the boot disk size, overriding the `google.batch.bootDiskSize` config option. See the [Google Batch documentation](https://cloud.google.com/compute/docs/disks) for more information about the available disk types.\n\nExamples:\n\n```groovy\n// set the boot disk size\ndisk 100.GB\n\n// mount a persistent disk at '/tmp'\ndisk 100.GB, type: 'pd-standard'\n\n// mount a local SSD disk at '/tmp' (should be a multiple of 375 GB)\ndisk 375.GB, type: 'local-ssd'\n```\n\n### Pipeline execution\n\nThe pipeline can be launched either in a local computer or a cloud instance. Pipeline input data can be stored either locally or in a Google Storage bucket.\n\nThe pipeline execution must specify a Google Storage bucket where the workflow's intermediate results are stored using the `-work-dir` command line options. For example:\n\n```bash\nnextflow run <script or project name> -work-dir gs://my-bucket/some/path\n```\n\n:::{tip}\nAny input data **not** stored in a Google Storage bucket will automatically be transferred to the pipeline work bucket. Use this feature with caution being careful to avoid unnecessary data transfers.\n:::\n\n:::{warning}\nThe Google Storage path needs to contain at least sub-directory. Don't use only the bucket name e.g. `gs://my-bucket`.\n:::\n\n### Spot instances\n\nSpot instances are supported adding the following setting in the Nextflow config file:\n\n```groovy\ngoogle {\n    batch.spot = true\n}\n```\n\nSince this type of virtual machines can be retired by the", "start_char_idx": 3588, "end_char_idx": 6921, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "8fdc6edc-7354-446d-b226-25a7cfe06b39": {"__data__": {"id_": "8fdc6edc-7354-446d-b226-25a7cfe06b39", "embedding": null, "metadata": {"file_path": "docs/google.md", "file_name": "google.md"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "36a88465987820e159f5c5c85e7ef67bf6fcee5f", "node_type": null, "metadata": {"file_path": "docs/google.md", "file_name": "google.md"}, "hash": "39e3a1ae1f1177a2e03e0e37405ab4c8a1fa701a9edd130ee288a1d6a7952a90"}, "2": {"node_id": "90f94734-6c75-4bc2-b66f-8bedcaf69a26", "node_type": null, "metadata": {"file_path": "docs/google.md", "file_name": "google.md"}, "hash": "a1e8361ed11ae6114726c04d9fbfd5c11b98464e20c0964c9c0e92a5b7cd6a94"}, "3": {"node_id": "6aeffd42-ab0c-41c4-acbb-694beac9a0a4", "node_type": null, "metadata": {"file_path": "docs/google.md", "file_name": "google.md"}, "hash": "a498198b739d265d487b10ba62f86719b21e49fc74eac729570d985f4e64868e"}}, "hash": "1ba1b35a7032883bf80592b4c09cd59cbebdc902125a924f23221677812e5b67", "text": "this type of virtual machines can be retired by the provider before the job completion, it is advisable to add the following retry strategy to your config file to instruct Nextflow to automatically re-execute a job if the virtual machine was terminated preemptively:\n\n```groovy\nprocess {\n    errorStrategy = { task.exitStatus==14 ? 'retry' : 'terminate' }\n    maxRetries = 5\n}\n```\n\n### Fusion file system\n\n:::{versionadded} 23.02.0-edge\n:::\n\nThe Google Batch executor supports the use of {ref}`fusion-page`. Fusion allows the use of Google Cloud Storage as a virtual distributed file system, optimising the data transfer and speeding up most job I/O operations.\n\nTo enable the use of Fusion file system in your pipeline, add the following snippet to your Nextflow configuration file:\n\n```groovy\nfusion.enabled = true\nwave.enabled = true\nprocess.scratch = false\ntower.accessToken = '<YOUR ACCESS TOKEN>'\n```\n\nThe [Tower](https://cloud.tower.nf) access token is optional, but it enables higher API rate limits for the {ref}`wave-page` service required by Fusion.\n\nBy default, Fusion mounts a local SSD disk to the VM at `/tmp`, using a machine type that can attach local SSD disks. If you specify your own machine type or machine series, they should be able to attach local SSD disks, otherwise the job scheduling will fail.\n\n:::{versionadded} 23.06.0-edge\n:::\n\nThe `disk` directive can be used to override the disk requested by Fusion. See the {ref}`Process definition <google-batch-process>` section above for examples. Note that local SSD disks must be a multiple of 375 GB in size, otherwise the size will be increased to the next multiple of 375 GB.\n\n### Supported directives\n\nThe integration with Google Batch is a developer preview feature. Currently, the following Nextflow directives are supported:\n\n- {ref}`process-accelerator`\n- {ref}`process-container`\n- {ref}`process-containeroptions`\n- {ref}`process-cpus`\n- {ref}`process-disk`\n- {ref}`process-executor`\n- {ref}`process-machinetype`\n- {ref}`process-memory`\n- {ref}`process-time`\n\n(google-lifesciences)=\n\n## Cloud Life Sciences\n\n:::{versionadded} 20.01.0-edge\n:::\n\n:::{note}\nIn versions of Nextflow prior to `21.04.0`, the following variables must be defined in your system environment:\n\n```bash\nexport NXF_VER=20.01.0\nexport NXF_MODE=google\n```\n:::\n\n[Cloud Life Sciences](https://cloud.google.com/life-sciences/) is a managed computing service that allows the execution of containerized workloads in the Google Cloud Platform infrastructure.\n\nNextflow provides built-in support for Cloud Life Sciences, allowing the seamless deployment of Nextflow pipelines in the cloud, in which tasks are offloaded to the Cloud Life Sciences service.\n\nRead the {ref}`Google Life Sciences executor <google-lifesciences-executor>` page to learn about the `google-lifesciences` executor in Nextflow.\n\n:::{warning}\nThis API works well for coarse-grained workloads (i.e. long-running jobs). It's not suggested the use this feature for pipelines spawning many short lived tasks.\n:::\n\n(google-lifesciences-config)=\n\n### Configuration\n\nMake sure to have defined in your environment the `GOOGLE_APPLICATION_CREDENTIALS` variable. See the section [Credentials](#credentials) for details.\n\n:::{tip}\nMake sure to enable the Cloud Life Sciences API beforehand. To learn how to enable it follow [this", "start_char_idx": 6942, "end_char_idx": 10277, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "6aeffd42-ab0c-41c4-acbb-694beac9a0a4": {"__data__": {"id_": "6aeffd42-ab0c-41c4-acbb-694beac9a0a4", "embedding": null, "metadata": {"file_path": "docs/google.md", "file_name": "google.md"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "36a88465987820e159f5c5c85e7ef67bf6fcee5f", "node_type": null, "metadata": {"file_path": "docs/google.md", "file_name": "google.md"}, "hash": "39e3a1ae1f1177a2e03e0e37405ab4c8a1fa701a9edd130ee288a1d6a7952a90"}, "2": {"node_id": "8fdc6edc-7354-446d-b226-25a7cfe06b39", "node_type": null, "metadata": {"file_path": "docs/google.md", "file_name": "google.md"}, "hash": "1ba1b35a7032883bf80592b4c09cd59cbebdc902125a924f23221677812e5b67"}, "3": {"node_id": "b0a9a076-eaed-4258-86a0-51da739d8798", "node_type": null, "metadata": {"file_path": "docs/google.md", "file_name": "google.md"}, "hash": "e1858e31b06ddcf9d1c72f36ea66eec110c355cf89776d494efde6aef9cf98a8"}}, "hash": "a498198b739d265d487b10ba62f86719b21e49fc74eac729570d985f4e64868e", "text": "enable the Cloud Life Sciences API beforehand. To learn how to enable it follow [this link](https://cloud.google.com/life-sciences/docs/quickstart).\n:::\n\nCreate a `nextflow.config` file in the project root directory. The config must specify the following parameters:\n\n- Google Life Sciences as Nextflow executor\n- The Docker container image(s) for pipeline tasks\n- The Google Cloud project ID\n- The Google Cloud region or zone where the Compute Engine VMs will be executed.\n  You need to specify one or the other, *not* both. Multiple regions or zones can be specified as a comma-separated list, e.g. `google.zone = 'us-central1-f,us-central-1-b'`.\n\nExample:\n\n```groovy\nprocess {\n    executor = 'google-lifesciences'\n    container = 'your/container:latest'\n}\n\ngoogle {\n    project = 'your-project-id'\n    zone = 'europe-west1-b'\n}\n```\n\nNotes:\n- A container image must be specified to execute processes. You can use a different Docker image for each process using one or more {ref}`config-process-selectors`.\n- Make sure to specify the project ID, not the project name.\n- Make sure to specify a location where Google Life Sciences is available. Refer to the [Google Cloud documentation](https://cloud.google.com/life-sciences/docs/concepts/locations) for details.\n\nRead the {ref}`Google configuration<config-google>` section to learn more about advanced configuration options.\n\n### Process definition\n\nProcesses can be defined as usual and by default the `cpus` and `memory` directives are used to instantiate a custom machine type with the specified compute resources. If `memory` is not specified, 1GB of memory is allocated per cpu. A persistent disk will be created with size corresponding to the `disk` directive. If `disk` is not specified, the instance default is chosen to ensure reasonable I/O performance.\n\nThe process `machineType` directive may optionally be used to specify a predefined Google Compute Platform [machine type](https://cloud.google.com/compute/docs/machine-types) If specified, this value overrides the `cpus` and `memory` directives. If the `cpus` and `memory` directives are used, the values must comply with the allowed custom machine type [specifications](https://cloud.google.com/compute/docs/instances/creating-instance-with-custom-machine-type#specifications) . Extended memory is not directly supported, however high memory or cpu predefined instances may be utilized using the `machineType` directive\n\nExamples:\n\n```groovy\nprocess custom_resources_task {\n    cpus 8\n    memory '40 GB'\n    disk '200 GB'\n\n    \"\"\"\n    <Your script here>\n    \"\"\"\n}\n\nprocess predefined_resources_task {\n    machineType 'n1-highmem-8'\n\n    \"\"\"\n    <Your script here>\n    \"\"\"\n}\n```\n\n### Pipeline execution\n\nThe pipeline can be launched either in a local computer or a cloud instance. Pipeline input data can be stored either locally or in a Google Storage bucket.\n\nThe pipeline execution must specify a Google Storage bucket where the workflow's intermediate results are stored using the `-work-dir` command line options. For example:\n\n```bash\nnextflow run <script or project name> -work-dir gs://my-bucket/some/path\n```\n\n:::{tip}\nAny input data *not* stored in a Google Storage bucket will be automatically transferred to the pipeline work bucket. Use this feature with caution, being careful to avoid unnecessary data transfers.\n:::\n\n### Preemptible instances\n\nPreemptible instances are supported adding the following setting in the Nextflow config file:\n\n```groovy\ngoogle {\n", "start_char_idx": 10249, "end_char_idx": 13739, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "b0a9a076-eaed-4258-86a0-51da739d8798": {"__data__": {"id_": "b0a9a076-eaed-4258-86a0-51da739d8798", "embedding": null, "metadata": {"file_path": "docs/google.md", "file_name": "google.md"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "36a88465987820e159f5c5c85e7ef67bf6fcee5f", "node_type": null, "metadata": {"file_path": "docs/google.md", "file_name": "google.md"}, "hash": "39e3a1ae1f1177a2e03e0e37405ab4c8a1fa701a9edd130ee288a1d6a7952a90"}, "2": {"node_id": "6aeffd42-ab0c-41c4-acbb-694beac9a0a4", "node_type": null, "metadata": {"file_path": "docs/google.md", "file_name": "google.md"}, "hash": "a498198b739d265d487b10ba62f86719b21e49fc74eac729570d985f4e64868e"}, "3": {"node_id": "5be61f5e-83c2-4b85-a269-3abf9837eb1e", "node_type": null, "metadata": {"file_path": "docs/google.md", "file_name": "google.md"}, "hash": "bb821b0690e11be1e00f11ba6685e648b445f5a8e5dd0b3cae7433ff5fedbd0b"}}, "hash": "e1858e31b06ddcf9d1c72f36ea66eec110c355cf89776d494efde6aef9cf98a8", "text": "following setting in the Nextflow config file:\n\n```groovy\ngoogle {\n    lifeSciences.preemptible = true\n}\n```\n\nSince this type of virtual machines can be retired by the provider before the job completion, it is advisable to add the following retry strategy to your config file to instruct Nextflow to automatically re-execute a job if the virtual machine was terminated preemptively:\n\n```groovy\nprocess {\n    errorStrategy = { task.exitStatus==14 ? 'retry' : 'terminate' }\n    maxRetries = 5\n}\n```\n\n:::{note}\nPreemptible instances have a [runtime limit](https://cloud.google.com/compute/docs/instances/preemptible) of 24 hours.\n:::\n\n:::{tip}\nFor an exhaustive list of error codes, refer to the official Google Life Sciences [documentation](https://cloud.google.com/life-sciences/docs/troubleshooting#error_codes).\n:::\n\n### Hybrid execution\n\nNextflow allows the use of multiple executors in the same workflow. This feature enables the deployment of hybrid workloads, in which some jobs are executed in the local computer or local computing cluster, and some jobs are offloaded to Google Life Sciences.\n\nTo enable this feature, use one or more {ref}`config-process-selectors` in your Nextflow configuration file to apply the Google Life Sciences executor to the subset of processes that you want to offload. For example:\n\n```groovy\nprocess {\n    withLabel: bigTask {\n        executor = 'google-lifesciences'\n        container = 'my/image:tag'\n    }\n}\n\ngoogle {\n    project = 'your-project-id'\n    zone = 'europe-west1-b'\n}\n```\n\nThen launch the pipeline with the `-bucket-dir` option to specify a Google Storage path for the jobs computed with Google Life Sciences and, optionally, the `-work-dir` to specify the local storage for the jobs computed locally:\n\n```bash\nnextflow run <script or project name> -bucket-dir gs://my-bucket/some/path\n```\n\n:::{warning}\nThe Google Storage path needs to contain at least one sub-directory (e.g. `gs://my-bucket/work` rather than `gs://my-bucket`).\n:::\n\n### Limitations\n\n- Compute resources in Google Cloud are subject to [resource quotas](https://cloud.google.com/compute/quotas), which may affect your ability to run pipelines at scale. You can request quota increases, and your quotas may automatically increase over time as you use the platform. In particular, GPU quotas are initially set to 0, so you must explicitly request a quota increase in order to use GPUs. You can initially request an increase to 1 GPU at a time, and after one billing cycle you may be able to increase it further.\n\n- Currently, it's not possible to specify a disk type different from the default one assigned by the service depending on the chosen instance type.\n\n### Troubleshooting\n\n- Make sure to enable the Compute Engine API, Life Sciences API and Cloud Storage API in the [APIs & Services Dashboard](https://console.cloud.google.com/apis/dashboard) page.\n\n- Make sure to have enough compute resources to run your pipeline in your project [Quotas](https://console.cloud.google.com/iam-admin/quotas) (i.e. Compute Engine CPUs, Compute Engine Persistent Disk, Compute Engine In-use IP addresses, etc).\n\n- Make sure your security credentials allow you to access any Google Storage bucket where input data and temporary files are stored.\n\n- When a job fails, you can check the `google/` directory in the task work directory (in the bucket storage), which contains useful information about the", "start_char_idx": 13751, "end_char_idx": 17160, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "5be61f5e-83c2-4b85-a269-3abf9837eb1e": {"__data__": {"id_": "5be61f5e-83c2-4b85-a269-3abf9837eb1e", "embedding": null, "metadata": {"file_path": "docs/google.md", "file_name": "google.md"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "36a88465987820e159f5c5c85e7ef67bf6fcee5f", "node_type": null, "metadata": {"file_path": "docs/google.md", "file_name": "google.md"}, "hash": "39e3a1ae1f1177a2e03e0e37405ab4c8a1fa701a9edd130ee288a1d6a7952a90"}, "2": {"node_id": "b0a9a076-eaed-4258-86a0-51da739d8798", "node_type": null, "metadata": {"file_path": "docs/google.md", "file_name": "google.md"}, "hash": "e1858e31b06ddcf9d1c72f36ea66eec110c355cf89776d494efde6aef9cf98a8"}}, "hash": "bb821b0690e11be1e00f11ba6685e648b445f5a8e5dd0b3cae7433ff5fedbd0b", "text": "in the task work directory (in the bucket storage), which contains useful information about the job execution. To enable the creation of this directory, set `google.lifeSciences.debug = true` in the Nextflow config.\n\n- You can enable the optional SSH daemon in the job VM by setting `google.lifeSciences.sshDaemon = true` in the Nextflow config.\n\n- Make sure you are choosing a `location` where the [Cloud Life Sciences API is available](https://cloud.google.com/life-sciences/docs/concepts/locations), and a `region` or `zone` where the [Compute Engine API is available](https://cloud.google.com/compute/docs/regions-zones/).", "start_char_idx": 17125, "end_char_idx": 17751, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "3ac5d80d-d57e-495b-92b2-963f3a83d847": {"__data__": {"id_": "3ac5d80d-d57e-495b-92b2-963f3a83d847", "embedding": null, "metadata": {"file_path": "docs/ignite.md", "file_name": "ignite.md"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "4c3aa6bae911907618067c75cd83a8585070dfb1", "node_type": null, "metadata": {"file_path": "docs/ignite.md", "file_name": "ignite.md"}, "hash": "dbc03fec25907ada9df8b46032560d2846e80d310d7ff635d8d853853f99c484"}, "3": {"node_id": "021840f5-53ac-49dc-8804-813553c72162", "node_type": null, "metadata": {"file_path": "docs/ignite.md", "file_name": "ignite.md"}, "hash": "0460e41ccfc337186d10937b4c9dcf7d9393b6f2a8a9d4b74a299399232a4abc"}}, "hash": "6b4ed7990495163df7fb1a7bde3c0fb61e702a7bfb796a06123f3696554f602d", "text": "(ignite-page)=\n\n# Apache Ignite\n\n:::{warning}\nThis feature is no longer maintained.\n:::\n\n:::{versionchanged} 22.01.0-edge\nThe `ignite` executor must be enabled via the `nf-ignite` plugin.\n:::\n\nNextflow can be deployed in a *cluster* mode by using [Apache Ignite](https://ignite.apache.org/), an in-memory data-grid and clustering platform.\n\nApache Ignite is packaged with Nextflow itself, so you won't need to install it separately or configure other third party software.\n\n(ignite-daemon)=\n\n## Cluster daemon\n\nIn order to setup a cluster you will need to run a cluster daemon on each node within your cluster. If you want to support the {ref}`Docker integration <container-docker>` provided by Nextflow, the Docker engine has to be installed and must run in each node.\n\nIn its simplest form just launch the Nextflow daemon in each cluster node as shown below:\n\n```bash\nnextflow node -bg\n```\n\nThe command line option `-bg` launches the node daemon in the background. The output is stored in the log file `.node-nextflow.log`. The daemon process `PID` is saved in the file `.nextflow.pid` in the same folder.\n\n### Multicast discovery\n\nBy default, the Ignite daemon tries to automatically discover all members in the cluster by using the network *multicast* discovery. Note that NOT all networks support this feature (for example Amazon AWS does not).\n\n:::{tip}\nTo check if multicast is available in your network, use the [iperf](http://sourceforge.net/projects/iperf/) tool. To test multicast, open a terminal on two machines within the network and run the following command in the first one `iperf -s -u -B 228.1.2.4 -i 1` and `iperf -c 228.1.2.4 -u -T 32 -t 3 -i 1` on the second. If data is being transferred then multicast is working.\n:::\n\nIgnite uses the multicast group `228.1.2.4` and port `47400` by default. You can change these values, by using the `cluster.join` command line option, as shown below:\n\n```bash\nnextflow node -cluster.join multicast:224.2.2.3:55701\n```\n\nIn case multicast discovery is not available in your network, you can try one of the following alternative methods:\n\n### Shared file system\n\nSimply provide a path shared across the cluster by a network file system, as shown below:\n\n```bash\nnextflow node -bg -cluster.join path:/net/shared/cluster\n```\n\nThe cluster members will use that path to discover each other.\n\n### IP addresses\n\nProvide a list of pre-configured IP addresses on the daemon launch command line, for example:\n\n```bash\nnextflow node -cluster.join ip:10.0.2.1,10.0.2.2,10.0.2.4\n```\n\n### Advanced options\n\nThe following cluster node configuration options can be used.\n\n| Name                          | Description                                                                                                                                                                       ", "start_char_idx": 0, "end_char_idx": 2825, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "021840f5-53ac-49dc-8804-813553c72162": {"__data__": {"id_": "021840f5-53ac-49dc-8804-813553c72162", "embedding": null, "metadata": {"file_path": "docs/ignite.md", "file_name": "ignite.md"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "4c3aa6bae911907618067c75cd83a8585070dfb1", "node_type": null, "metadata": {"file_path": "docs/ignite.md", "file_name": "ignite.md"}, "hash": "dbc03fec25907ada9df8b46032560d2846e80d310d7ff635d8d853853f99c484"}, "2": {"node_id": "3ac5d80d-d57e-495b-92b2-963f3a83d847", "node_type": null, "metadata": {"file_path": "docs/ignite.md", "file_name": "ignite.md"}, "hash": "6b4ed7990495163df7fb1a7bde3c0fb61e702a7bfb796a06123f3696554f602d"}, "3": {"node_id": "0fd1bdd5-9695-4177-ae1a-0c5adef838f8", "node_type": null, "metadata": {"file_path": "docs/ignite.md", "file_name": "ignite.md"}, "hash": "45eb6a20585ff98f95abfe68631e8481fce304b9a5a8cfd9a1fef05a288f5b3c"}}, "hash": "0460e41ccfc337186d10937b4c9dcf7d9393b6f2a8a9d4b74a299399232a4abc", "text": "                                                                                           |\n| ----------------------------- | --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |\n| join                          | IP address(es) of one or more cluster nodes to which the daemon will join.                                                                                                                                                                                |\n| group                         | The name of the cluster to which this node join. It allows the creation of separate cluster instances. Default: `nextflow`                                                                                                                                |\n| maxCpus                       | Max number of CPUs that can be used by the daemon to run user tasks. By default it is equal to the number of CPU cores.                                                                                                                                   |\n| maxDisk                       | Max amount of disk storage that can be used by user tasks e.g. `500 GB`.                                                                                                                                                                                   |\n| maxMemory                     | Max amount of memory that can be used by user tasks e.g. `64 GB`. Default total available memory.                    ", "start_char_idx": 2826, "end_char_idx": 4501, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "0fd1bdd5-9695-4177-ae1a-0c5adef838f8": {"__data__": {"id_": "0fd1bdd5-9695-4177-ae1a-0c5adef838f8", "embedding": null, "metadata": {"file_path": "docs/ignite.md", "file_name": "ignite.md"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "4c3aa6bae911907618067c75cd83a8585070dfb1", "node_type": null, "metadata": {"file_path": "docs/ignite.md", "file_name": "ignite.md"}, "hash": "dbc03fec25907ada9df8b46032560d2846e80d310d7ff635d8d853853f99c484"}, "2": {"node_id": "021840f5-53ac-49dc-8804-813553c72162", "node_type": null, "metadata": {"file_path": "docs/ignite.md", "file_name": "ignite.md"}, "hash": "0460e41ccfc337186d10937b4c9dcf7d9393b6f2a8a9d4b74a299399232a4abc"}, "3": {"node_id": "3ddc18b6-7593-4e54-b150-f9249eb2f9e0", "node_type": null, "metadata": {"file_path": "docs/ignite.md", "file_name": "ignite.md"}, "hash": "2b845e08d60f7e8b58e073fca5325a0c9ef3c4239dbab8eefc4127677ba7192e"}}, "hash": "45eb6a20585ff98f95abfe68631e8481fce304b9a5a8cfd9a1fef05a288f5b3c", "text": "                                                                                                                                                         |\n| interface                     | Network interfaces that Ignite will use. It can be the interface IP address or name.                                                                                                                                                                      |\n| failureDetectionTimeout       | Failure detection timeout is used to determine how long the communication or discovery SPIs should wait before considering a remote connection failed.                                                                                                    |\n| clientFailureDetectionTimeout | Failure detection timeout is used to determine how long the communication or discovery SPIs should wait before considering a remote connection failed.                                                                                                    |\n| tcp.localAddress              | Defines the local host IP address.                                                                                                                                                                                                                        |\n| tcp.localPort                 | Defines the local port to listen to.                                                                                  ", "start_char_idx": 4502, "end_char_idx": 5953, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "3ddc18b6-7593-4e54-b150-f9249eb2f9e0": {"__data__": {"id_": "3ddc18b6-7593-4e54-b150-f9249eb2f9e0", "embedding": null, "metadata": {"file_path": "docs/ignite.md", "file_name": "ignite.md"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "4c3aa6bae911907618067c75cd83a8585070dfb1", "node_type": null, "metadata": {"file_path": "docs/ignite.md", "file_name": "ignite.md"}, "hash": "dbc03fec25907ada9df8b46032560d2846e80d310d7ff635d8d853853f99c484"}, "2": {"node_id": "0fd1bdd5-9695-4177-ae1a-0c5adef838f8", "node_type": null, "metadata": {"file_path": "docs/ignite.md", "file_name": "ignite.md"}, "hash": "45eb6a20585ff98f95abfe68631e8481fce304b9a5a8cfd9a1fef05a288f5b3c"}, "3": {"node_id": "5d4dc916-6a40-44c4-94d4-799b499573b0", "node_type": null, "metadata": {"file_path": "docs/ignite.md", "file_name": "ignite.md"}, "hash": "1926f9a59a863f59169a3306c15d62857c56998b9b01c071764e8e41f111f90f"}}, "hash": "2b845e08d60f7e8b58e073fca5325a0c9ef3c4239dbab8eefc4127677ba7192e", "text": "                                                                                                                                                       |\n| tcp.localPortRange            | Range for local ports.                                                                                                                                                                                                                                    |\n| tcp.reconnectCount            | Number of times the node tries to (re)establish connection to another node.                                                                                                                                                                               |\n| tcp.networkTimeout            | Defines the network timeout.                                                                                                                                                                                                                              |\n| tcp.socketTimeout             | Defines the socket operations timeout. This timeout is used to limit connection time and write-to-socket time. Note that when running Ignite on Amazon EC2, socket timeout must be set to a value significantly greater than the default (e.g. to 30000). |\n| tcp.ackTimeout                | Defines the timeout for receiving acknowledgement for sent message.                     ", "start_char_idx": 5954, "end_char_idx": 7373, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "5d4dc916-6a40-44c4-94d4-799b499573b0": {"__data__": {"id_": "5d4dc916-6a40-44c4-94d4-799b499573b0", "embedding": null, "metadata": {"file_path": "docs/ignite.md", "file_name": "ignite.md"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "4c3aa6bae911907618067c75cd83a8585070dfb1", "node_type": null, "metadata": {"file_path": "docs/ignite.md", "file_name": "ignite.md"}, "hash": "dbc03fec25907ada9df8b46032560d2846e80d310d7ff635d8d853853f99c484"}, "2": {"node_id": "3ddc18b6-7593-4e54-b150-f9249eb2f9e0", "node_type": null, "metadata": {"file_path": "docs/ignite.md", "file_name": "ignite.md"}, "hash": "2b845e08d60f7e8b58e073fca5325a0c9ef3c4239dbab8eefc4127677ba7192e"}, "3": {"node_id": "21d436f7-2402-45d8-b5af-8b5822c28dac", "node_type": null, "metadata": {"file_path": "docs/ignite.md", "file_name": "ignite.md"}, "hash": "98b2df5eaf0f3988fb5fe06d09f982fc249577951d4954d15619e4de78564a0e"}}, "hash": "1926f9a59a863f59169a3306c15d62857c56998b9b01c071764e8e41f111f90f", "text": "                                                                                                                                                                                     |\n| tcp.maxAckTimeout             | Defines the maximum timeout for receiving acknowledgement for sent message.                                                                                                                                                                               |\n| tcp.joinTimeout               | Defines the join timeout.                                                                                                                                                                                                                                 |\n\nThese options can be specified as command line parameters by adding the prefix `-cluster.` to them, as shown below:\n\n```bash\nnextflow node -bg -cluster.maxCpus 4 -cluster.interface eth0\n```\n\nThe same options can be entered into the Nextflow {ref}`configuration file<config-page>`, as shown below:\n\n```groovy\ncluster {\n    join = 'ip:192.168.1.104'\n    interface = 'eth0'\n}\n```\n\nFinally daemon options can be provided also as environment variables having the name in upper-case and by adding the prefix `NXF_CLUSTER_` to them, for example:\n\n```bash\nexport NXF_CLUSTER_JOIN='ip:192.168.1.104'\nexport NXF_CLUSTER_INTERFACE='eth0'\n```\n\n## Pipeline execution\n\nThe pipeline execution needs to be launched in a `head` node i.e. a cluster node where the Nextflow node daemon is **not** running. In order to execute your pipeline in the Ignite cluster you will need to use the Ignite executor, as shown below:\n\n```bash\nnextflow run <your pipeline> -process.executor ignite\n```\n\nIf your network does no support multicast discovery, you will need to specify the `joining` strategy as you did for the cluster", "start_char_idx": 7374, "end_char_idx": 9226, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "21d436f7-2402-45d8-b5af-8b5822c28dac": {"__data__": {"id_": "21d436f7-2402-45d8-b5af-8b5822c28dac", "embedding": null, "metadata": {"file_path": "docs/ignite.md", "file_name": "ignite.md"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "4c3aa6bae911907618067c75cd83a8585070dfb1", "node_type": null, "metadata": {"file_path": "docs/ignite.md", "file_name": "ignite.md"}, "hash": "dbc03fec25907ada9df8b46032560d2846e80d310d7ff635d8d853853f99c484"}, "2": {"node_id": "5d4dc916-6a40-44c4-94d4-799b499573b0", "node_type": null, "metadata": {"file_path": "docs/ignite.md", "file_name": "ignite.md"}, "hash": "1926f9a59a863f59169a3306c15d62857c56998b9b01c071764e8e41f111f90f"}, "3": {"node_id": "f0809c93-14fc-44f7-b228-d78dc4a9028c", "node_type": null, "metadata": {"file_path": "docs/ignite.md", "file_name": "ignite.md"}, "hash": "b29b80358bc5e53ba09476331a14c3bf7d8cfd7c52d5957ae8d25ba55298b817"}}, "hash": "98b2df5eaf0f3988fb5fe06d09f982fc249577951d4954d15619e4de78564a0e", "text": "you will need to specify the `joining` strategy as you did for the cluster daemons. For example, using a shared path:\n\n```bash\nnextflow run <your pipeline> -process.executor ignite -cluster.join path:/net/shared/path\n```\n\n## Execution with MPI\n\nNextflow is able to deploy and self-configure an Ignite cluster on demand, taking advantage of the Open [MPI](https://en.wikipedia.org/wiki/Message_Passing_Interface) standard that is commonly available in grid and supercomputer facilities.\n\nIn this scenario a Nextflow workflow needs to be executed as an MPI job. Under the hood Nextflow will launch a *driver* process in the first of the nodes, allocated by your job request, and an Ignite daemon in the remaining nodes.\n\nIn practice you will need a launcher script to submit an MPI job request to your batch scheduler/resource manager. The batch scheduler must reserve the compute nodes in an exclusive manner to avoid having multiple Ignite daemons running on the same node. Nextflow must be launched using the `mpirun` utility, as if it were an MPI application, specifying the `--pernode` option.\n\n### Platform LSF launcher\n\nThe following example shows a launcher script for the [Platform LSF](https://en.wikipedia.org/wiki/Platform_LSF/) resource manager:\n\n```bash\n#!/bin/bash\n#BSUB -oo output_%J.out\n#BSUB -eo output_%J.err\n#BSUB -J <job name>\n#BSUB -q <queue name>\n#BSUB -W 02:00\n#BSUB -x\n#BSUB -n 80\n#BSUB -M 10240\n#BSUB -R \"span[ptile=16]\"\nexport NXF_CLUSTER_SEED=$(shuf -i 0-16777216 -n 1)\nmpirun --pernode nextflow run <your-project-name> -with-mpi [pipeline parameters]\n```\n\nIt requests 5 nodes (80 processes, with 16 cpus per node). The `-x` directive allocates the node in an exclusive manner. Nextflow needs to be executed using the `-with-mpi` command line option. It will automatically use `ignite` as the executor.\n\nThe variable `NXF_CLUSTER_SEED` must contain an integer value (in the range 0-16777216) that will unequivocally identify your cluster instance. In the above example it is randomly generated by using the [shuf](http://linux.die.net/man/1/shuf) Linux tool.\n\n### Univa Grid Engine launcher\n\nThe following example shows a launcher script for the [Univa Grid Engine](https://en.wikipedia.org/wiki/Univa_Grid_Engine) (aka SGE):\n\n```bash\n#!/bin/bash\n#$ -cwd\n#$ -j y\n#$ -o <output file name>\n#$ -l virtual_free=10G\n#$ -q <queue name>\n#$ -N <job name>\n#$ -pe ompi 5\nexport NXF_CLUSTER_SEED=$(shuf -i 0-16777216 -n 1)\nmpirun --pernode nextflow run <your-project-name> -with-mpi [pipeline parameters]\n```\n\nAs in the previous script it allocates 5 processing nodes. UGE/SGE does not have an option to reserve a node in an exclusive manner. A common workaround is to request the maximum amount of memory or cpus available in the nodes of your cluster.\n\n### Linux SLURM launcher\n\nWhen using Linux SLURM you will need to use `srun` instead `mpirun` in your launcher script. For example:\n\n```bash\n#!/bin/bash\n#SBATCH --job-name=<job name>\n#SBATCH --output=<log file %j>\n#SBATCH --ntasks=5\n#SBATCH --cpus-per-task=16\n#SBATCH --tasks-per-node=1\nexport", "start_char_idx": 9166, "end_char_idx": 12229, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "f0809c93-14fc-44f7-b228-d78dc4a9028c": {"__data__": {"id_": "f0809c93-14fc-44f7-b228-d78dc4a9028c", "embedding": null, "metadata": {"file_path": "docs/ignite.md", "file_name": "ignite.md"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "4c3aa6bae911907618067c75cd83a8585070dfb1", "node_type": null, "metadata": {"file_path": "docs/ignite.md", "file_name": "ignite.md"}, "hash": "dbc03fec25907ada9df8b46032560d2846e80d310d7ff635d8d853853f99c484"}, "2": {"node_id": "21d436f7-2402-45d8-b5af-8b5822c28dac", "node_type": null, "metadata": {"file_path": "docs/ignite.md", "file_name": "ignite.md"}, "hash": "98b2df5eaf0f3988fb5fe06d09f982fc249577951d4954d15619e4de78564a0e"}}, "hash": "b29b80358bc5e53ba09476331a14c3bf7d8cfd7c52d5957ae8d25ba55298b817", "text": "--tasks-per-node=1\nexport NXF_CLUSTER_SEED=$(shuf -i 0-16777216 -n 1)\nsrun nextflow run hello.nf -with-mpi\n```\n\nAs before, this allocates 5 processing nodes (`--ntasks=5`) and each node will be able to use up to 16 cpus (`--cpus-per-task=16`). When using SLURM it's not necessary to allocate compute nodes in an exclusive manner. It's even possible to launch more than one Nextflow daemon instance per node, though not suggested.\n\nTo submit the pipeline execution create a file like the above, then use the following command:\n\n```bash\nsbatch <launcher script name>\n```", "start_char_idx": 12265, "end_char_idx": 12833, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "bf38b9e5-0243-436e-9683-94696aca428e": {"__data__": {"id_": "bf38b9e5-0243-436e-9683-94696aca428e", "embedding": null, "metadata": {"file_path": "docs/index.md", "file_name": "index.md"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "95ce5773add72664c480be8dc558573dfd0daa1a", "node_type": null, "metadata": {"file_path": "docs/index.md", "file_name": "index.md"}, "hash": "1eed00862dcbb971f5cdd1c4cb57d687b5461020bb0d78c99908792cddd543ee"}}, "hash": "7d65b16b9b2e85fd1e39ff8ee65dfadade23e4841fdd240c15e9c0cfc5ec1c05", "text": "# Nextflow's documentation!\n\nContents:\n\n```{toctree}\n:maxdepth: 2\n\ngetstarted\nbasic\nscript\nprocess\nchannel\noperator\nexecutor\nconfig\ndsl2\ncli\ncontainer\nconda\nspack\nwave\nfusion\naws\namazons3\nazure\nflux\ngoogle\nignite\nkubernetes\ntracing\nmetrics\nsharing\nmetadata\nmail\nplugins\nsecrets\n```", "start_char_idx": 0, "end_char_idx": 281, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "5679e811-ced1-405d-b111-8832c2bd191b": {"__data__": {"id_": "5679e811-ced1-405d-b111-8832c2bd191b", "embedding": null, "metadata": {"file_path": "docs/kubernetes.md", "file_name": "kubernetes.md"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "06c954d55f081965a46e87f087da2d13a4d78a54", "node_type": null, "metadata": {"file_path": "docs/kubernetes.md", "file_name": "kubernetes.md"}, "hash": "a042409209e76a618397be1df009eda2162825171ed85218d1a7ffc98289a1f6"}, "3": {"node_id": "e325d2b6-1b9c-4e4f-899a-931e94194617", "node_type": null, "metadata": {"file_path": "docs/kubernetes.md", "file_name": "kubernetes.md"}, "hash": "a304f169cd0dfa21dd27863b93941ed075c29630eb66d1d70055167568d39dd5"}}, "hash": "a328cc2ed6557048ac0fa8810e55d0583f3e39a43dd84d08b1388c9ce0dd9873", "text": "(k8s-page)=\n\n# Kubernetes\n\n[Kubernetes](https://kubernetes.io/) is a cloud-native open-source system for deployment, scaling, and management of containerized applications.\n\nIt provides clustering and file system abstractions that allows the execution of containerised workloads across different cloud platforms and on-premises installations.\n\nThe built-in support for Kubernetes provided by Nextflow streamlines the execution of containerised workflows in Kubernetes clusters.\n\n## Concepts\n\nKubernetes main abstraction is the `pod`. A `pod` defines the (desired) state of one or more containers i.e. required computing resources, storage, network configuration.\n\nKubernetes abstracts also the storage provisioning through the definition of one more persistent volumes that allow containers to access to the underlying storage systems in a transparent and portable manner.\n\nWhen using the `k8s` executor Nextflow deploys the workflow execution as a Kubernetes pod. This pod orchestrates the workflow execution and submits a separate pod execution for each job that need to be carried out by the workflow application.\n\n```{image} /images/nextflow-k8s-min.png\n```\n\n## Requirements\n\nAt least a [Persistent Volume](https://kubernetes.io/docs/concepts/storage/persistent-volumes/#persistent-volumes) with `ReadWriteMany` access mode has to be defined in the Kubernetes cluster (check the supported storage systems at [this link](https://kubernetes.io/docs/concepts/storage/persistent-volumes/#access-modes)).\n\nSuch volume needs to be accessible through a [Persistent Volume Claim](https://kubernetes.io/docs/concepts/storage/persistent-volumes/#persistentvolumeclaims), which will be used by Nextflow to run the application and store the scratch data and the pipeline final result.\n\nThe workflow application has to be containerised using the usual Nextflow {ref}`container<process-container>` directive.\n\n:::{tip}\nWhen using {ref}`wave-page` and {ref}`fusion-page` there is no need to use a shared file system and configure a persistent volume claim for the deployment of Nextflow pipeline with Kubernetes. You can ignore this requirement when using the Fusion file system. See the {ref}`fusion-page` documentation for further details.\n:::\n\n## Execution\n\nThe workflow execution needs to be submitted from a computer able to connect to the Kubernetes cluster.\n\nNextflow uses the Kubernetes configuration file available at the path `$HOME/.kube/config` or the file specified by the environment variable `KUBECONFIG`.\n\nYou can verify such configuration with the command below:\n\n```console\n$ kubectl cluster-info\nKubernetes master is running at https://your-host:6443\nKubeDNS is running at https://your-host:6443/api/v1/namespaces/kube-system/services/kube-dns:dns/proxy\n```\n\n### Launch with `kuberun`\n\n:::{warning}\nThe `kuberun` is considered an obsolete approach for the deployment of Nextflow pipeline with Kubernetes and is no longer maintained. For a better alternative, consider using [Launch with Fusion](#launch-with-fusion).\n:::\n\nTo deploy and launch the workflow execution use the Nextflow command `kuberun` as shown below:\n\n```bash\nnextflow kuberun <pipeline-name> -v vol-claim:/mount/path\n```\n\nThis command will create and execute a pod running the nextflow orchestrator for the specified workflow. In the above example replace `<pipeline-name>` with an existing nextflow project or the absolute path of a workflow already deployed in the Kubernetes cluster.\n\nThe `-v` command line option is required to", "start_char_idx": 0, "end_char_idx": 3505, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "e325d2b6-1b9c-4e4f-899a-931e94194617": {"__data__": {"id_": "e325d2b6-1b9c-4e4f-899a-931e94194617", "embedding": null, "metadata": {"file_path": "docs/kubernetes.md", "file_name": "kubernetes.md"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "06c954d55f081965a46e87f087da2d13a4d78a54", "node_type": null, "metadata": {"file_path": "docs/kubernetes.md", "file_name": "kubernetes.md"}, "hash": "a042409209e76a618397be1df009eda2162825171ed85218d1a7ffc98289a1f6"}, "2": {"node_id": "5679e811-ced1-405d-b111-8832c2bd191b", "node_type": null, "metadata": {"file_path": "docs/kubernetes.md", "file_name": "kubernetes.md"}, "hash": "a328cc2ed6557048ac0fa8810e55d0583f3e39a43dd84d08b1388c9ce0dd9873"}, "3": {"node_id": "f9108746-9470-4159-abf0-6f1dc870148f", "node_type": null, "metadata": {"file_path": "docs/kubernetes.md", "file_name": "kubernetes.md"}, "hash": "c39ab15734e4a9d1ee47db4780db2cf61fc8b4925567388658438e79e79968cc"}}, "hash": "a304f169cd0dfa21dd27863b93941ed075c29630eb66d1d70055167568d39dd5", "text": "Kubernetes cluster.\n\nThe `-v` command line option is required to specify the volume claim name and mount path to use for the workflow execution. In the above example replace `vol-claim` with the name of an existing persistent volume claim and `/mount/path` with the path where the volume is required to be mount in the container. Volume claims can also be specified in the Nextflow configuration file, see the {ref}`Kubernetes configuration section<config-k8s>` for details.\n\nOnce the pod execution starts, the application in the foreground prints the console output produced by the running workflow pod.\n\n### Interactive login\n\nFor debugging purpose it's possible to execute a Nextflow pod and launch an interactive shell using the following command:\n\n```bash\nnextflow kuberun login -v vol-claim:/mount/path\n```\n\nThis command creates a pod, sets up the volume claim(s), configures the Nextflow environment and finally launches a Bash login session.\n\n:::{warning}\nThe pod is automatically destroyed once the shell session terminates. Do not use it to launch long-running workflows in the background.\n:::\n\n### Launch with Fusion\n\n:::{versionadded} 22.10.0\n:::\n\nThe use of {ref}`fusion-page` allows deploying a Nextflow pipeline to a remote (or local) cluster without the need to use a shared file system and configure a persistent volume claim for the deployment of Nextflow pipeline with Kubernetes.\n\nThis also makes unnecessary the use of the special `kuberun` command for the pipeline execution.\n\nFor this deployment scenario the following configuration can be used:\n\n```groovy\nwave {\n    enabled = true\n}\n\nfusion {\n    enabled = true\n}\n\nprocess {\n    executor = 'k8s'\n}\n\nk8s {\n    context = '<YOUR K8S CONFIGURATION CONTEXT>'\n    namespace = '<YOUR K8S NAMESPACE>'\n    serviceAccount = '<YOUR K8S SERVICE ACCOUNT>'\n}\n```\n\nThe `k8s.context` represents the Kubernetes configuration context to be used for the pipeline execution. This setting can be omitted if Nextflow itself is run as a pod in the Kubernetes cluster.\n\nThe `k8s.namespace` represents the Kubernetes namespace where the jobs submitted by the pipeline execution should be executed.\n\nThe `k8s.serviceAccount` represents the Kubernetes service account that should be used to grant the execution permission to jobs launched by Nextflow. You can find more details how to configure it as the [following link](https://github.com/seqeralabs/wave-showcase/tree/master/example8).\n\nThen the pipeline execution can be launched using the usual run command and specifying a AWS S3 bucket work directory, for example:\n\n```bash\nnextflow run <YOUR PIPELINE> -work-dir s3://<YOUR-BUCKET>/scratch\n```\n\n### Running in a pod\n\nNextflow can be executed directly from a pod running in a Kubernetes cluster. In these cases you will need to use the plain Nextflow `run` command and specify the `k8s` executor and the required persistent volume claim in the `nextflow.config` file as shown below:\n\n```groovy\nprocess {\n    executor = 'k8s'\n}\n\nk8s {\n    storageClaimName = 'vol-claim'\n    storageMountPath = '/mount/path'\n    storageSubPath = '/my-data'\n}\n```\n\nIn the above snippet replace `vol-claim` with the name of an existing persistent volume claim and replace `/mount/path` with the actual desired mount path (default: `/workspace`) and `storageSubPath` with the directory in the volume to be mounted (default:", "start_char_idx": 3450, "end_char_idx": 6805, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "f9108746-9470-4159-abf0-6f1dc870148f": {"__data__": {"id_": "f9108746-9470-4159-abf0-6f1dc870148f", "embedding": null, "metadata": {"file_path": "docs/kubernetes.md", "file_name": "kubernetes.md"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "06c954d55f081965a46e87f087da2d13a4d78a54", "node_type": null, "metadata": {"file_path": "docs/kubernetes.md", "file_name": "kubernetes.md"}, "hash": "a042409209e76a618397be1df009eda2162825171ed85218d1a7ffc98289a1f6"}, "2": {"node_id": "e325d2b6-1b9c-4e4f-899a-931e94194617", "node_type": null, "metadata": {"file_path": "docs/kubernetes.md", "file_name": "kubernetes.md"}, "hash": "a304f169cd0dfa21dd27863b93941ed075c29630eb66d1d70055167568d39dd5"}}, "hash": "c39ab15734e4a9d1ee47db4780db2cf61fc8b4925567388658438e79e79968cc", "text": "and `storageSubPath` with the directory in the volume to be mounted (default: `/`).\n\n:::{warning}\nThe running pod must have been created with the same persistent volume claim name and mount as the one specified in your Nextflow configuration file. Note also that the `run` command does not support the `-v` option.\n:::\n\n:::{tip}\nIt is also possible to mount multiple volumes using the `pod` directive, for example:\n\n```groovy\nk8s.pod = [ [volumeClaim: \"other-pvc\", mountPath: \"/other\" ]]\n```\n:::\n\n## Pod settings\n\nThe process {ref}`process-pod` directive allows the definition of pods specific settings, such as environment variables, secrets and config maps when using the {ref}`k8s-executor` executor. See the {ref}`process-pod` directive for more details.\n\n## Limitations\n\nThe `kuberun` command does not allow the execution of local Nextflow scripts. It is only intended as a convenient way to test the deployment of pipelines to a Kubernetes cluster.\n\n## Advanced configuration\n\nRead the {ref}`Kubernetes configuration<config-k8s>` and {ref}`executor <k8s-executor>` sections to learn more about advanced configuration options.", "start_char_idx": 6784, "end_char_idx": 7915, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "650167e0-b75b-423e-9c86-95d1e6628dc8": {"__data__": {"id_": "650167e0-b75b-423e-9c86-95d1e6628dc8", "embedding": null, "metadata": {"file_path": "docs/mail.md", "file_name": "mail.md"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "5396077a51c81e98f78ee259c51b9d568b48e656", "node_type": null, "metadata": {"file_path": "docs/mail.md", "file_name": "mail.md"}, "hash": "6c418e0765c05453369024cf377969a4a19bea7d62f3323f5cf53ffc15fe1a47"}, "3": {"node_id": "55490239-e8fc-406c-bff6-89cbb86ce742", "node_type": null, "metadata": {"file_path": "docs/mail.md", "file_name": "mail.md"}, "hash": "3952239b80473a33667a05307b45640eb19bbec315060f0028595ea05b0c7434"}}, "hash": "06d6d5c3eca6e0d38f5889d8d2a0cb9e61567bb1f74214cfa508b9b2bed7299e", "text": "(mail-page)=\n\n# Mail & Notifications\n\n## Mail message\n\nThe built-in function `sendMail` allows you to send a mail message from a workflow script.\n\n(mail-basic)=\n\n### Basic mail\n\nThe mail attributes are specified as named parameters or providing an equivalent associative array as argument. For example:\n\n```groovy\nsendMail(\n    to: 'you@gmail.com',\n    subject: 'Catch up',\n    body: 'Hi, how are you!',\n    attach: '/some/path/attachment/file.txt'\n)\n```\n\nwhich is equivalent to:\n\n```groovy\nmail = [\n    to: 'you@gmail.com',\n    subject: 'Catch up',\n    body: 'Hi, how are you!',\n    attach: '/some/path/attachment/file.txt'\n]\n\nsendMail(mail)\n```\n\nThe following parameters can be specified:\n\n| Name          | Description                                                            |\n| ------------- | ---------------------------------------------------------------------- |\n| to {sup}`*`   | The mail target recipients.                                            |\n| cc {sup}`*`   | The mail CC recipients.                                                |\n| bcc {sup}`*`  | The mail BCC recipients.                                               |\n| from {sup}`*` | The mail sender address.                                               |\n| subject       | The mail subject.                                                      |\n| charset       | The mail content charset (default: `UTF-8`).                           |\n| text          | The mail plain text content.                                           |\n| body          | The mail body content. It can be either plain text or HTML content.    |\n| type          | The mail body mime type. If not specified it's automatically detected. |\n| attach        | Single file or a list of files to be included as mail attachments.     |\n\n`*` Multiple email addresses can be specified separating them with a comma.\n\n(mail-advanced)=\n\n### Advanced mail\n\nAnother version of `sendMail` allows a more idiomatic syntax:\n\n```groovy\nsendMail {\n    to 'you@gmail.com'\n    from 'me@gmail.com'\n    attach '/some/path/attachment/file.txt'\n    attach '/other/path/image.png'\n    subject 'Catch up'\n\n    '''\n  ", "start_char_idx": 0, "end_char_idx": 2143, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "55490239-e8fc-406c-bff6-89cbb86ce742": {"__data__": {"id_": "55490239-e8fc-406c-bff6-89cbb86ce742", "embedding": null, "metadata": {"file_path": "docs/mail.md", "file_name": "mail.md"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "5396077a51c81e98f78ee259c51b9d568b48e656", "node_type": null, "metadata": {"file_path": "docs/mail.md", "file_name": "mail.md"}, "hash": "6c418e0765c05453369024cf377969a4a19bea7d62f3323f5cf53ffc15fe1a47"}, "2": {"node_id": "650167e0-b75b-423e-9c86-95d1e6628dc8", "node_type": null, "metadata": {"file_path": "docs/mail.md", "file_name": "mail.md"}, "hash": "06d6d5c3eca6e0d38f5889d8d2a0cb9e61567bb1f74214cfa508b9b2bed7299e"}, "3": {"node_id": "146d6f8a-443f-473b-a5a2-637b3ce26e86", "node_type": null, "metadata": {"file_path": "docs/mail.md", "file_name": "mail.md"}, "hash": "961f2a95848bc8fd730ac95238ae1b250c386d68c1c74e0df5b26e8cd5f2ce14"}}, "hash": "3952239b80473a33667a05307b45640eb19bbec315060f0028595ea05b0c7434", "text": "   subject 'Catch up'\n\n    '''\n    Hi there,\n    Look! Multi-lines\n    mail content!\n    '''\n}\n```\n\nThe same attributes listed in the table in the previous section are allowed.\n\n:::{tip}\nA string expression at the end is implicitly interpreted as the mail body content, therefore the `body` parameter can be omitted as shown above.\n:::\n\n:::{tip}\nTo send an email that includes text and HTML content, use both the `text` and `body` attributes. The first is used for the plain text content, while the second is used for the rich HTML content.\n:::\n\n(mail-attachments)=\n\n### Mail attachments\n\nWhen using the curly brackets syntax, the `attach` parameter can be repeated two or more times to include multiple attachments in the mail message.\n\nMoreover for each attachment it's possible to specify one or more of the following optional attributes:\n\n| Name        | Description                                                                 |\n| ----------- | --------------------------------------------------------------------------- |\n| contentId   | Defines the `Content-ID` header field for the attachment.                   |\n| disposition | Defines the `Content-Disposition` header field for the attachment.          |\n| fileName    | Defines the `filename` parameter of the \"Content-Disposition\" header field. |\n| description | Defines the `Content-Description` header field for the attachment.          |\n\nFor example:\n\n```groovy\nsendMail {\n    to 'you@dot.com'\n    attach '/some/file.txt', fileName: 'manuscript.txt'\n    attach '/other/image.png', disposition: 'inline'\n    subject 'Sending documents'\n    '''\n    the mail body\n    '''\n}\n```\n\n(mail-config)=\n\n### Mail configuration\n\nIf no mail server configuration is provided, Nextflow tries to send the email by using the external mail command eventually provided by the underlying system (e.g. `sendmail` or `mail`).\n\nIf your system does not provide access to none of the above you can configure a SMTP server in the `nextflow.config` file. For example:\n\n```groovy\nmail {\n    smtp.host = 'your.smtp-server.com'\n    smtp.port = 475\n    smtp.user = 'my-user'\n}\n```\n\nSee the {ref}`mail scope <config-mail>` section to learn more the mail server configuration options.\n\n## Mail notification\n\nYou can use the `sendMail` function with a {ref}`workflow completion handler <metadata-completion-handler>` to notify the completion of a workflow completion. For example:\n\n```groovy\nworkflow.onComplete {\n\n    def msg = \"\"\"\\\n        Pipeline execution summary\n        ---------------------------\n        Completed at: ${workflow.complete}\n        Duration    : ${workflow.duration}\n        Success     : ${workflow.success}\n        workDir     : ${workflow.workDir}\n        exit status : ${workflow.exitStatus}\n        \"\"\"\n        .stripIndent()\n\n    sendMail(to: 'you@gmail.com', subject: 'My pipeline execution', body: msg)\n}\n```\n\nThis is useful to send a custom notification message. Note however", "start_char_idx": 2122, "end_char_idx": 5065, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "146d6f8a-443f-473b-a5a2-637b3ce26e86": {"__data__": {"id_": "146d6f8a-443f-473b-a5a2-637b3ce26e86", "embedding": null, "metadata": {"file_path": "docs/mail.md", "file_name": "mail.md"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "5396077a51c81e98f78ee259c51b9d568b48e656", "node_type": null, "metadata": {"file_path": "docs/mail.md", "file_name": "mail.md"}, "hash": "6c418e0765c05453369024cf377969a4a19bea7d62f3323f5cf53ffc15fe1a47"}, "2": {"node_id": "55490239-e8fc-406c-bff6-89cbb86ce742", "node_type": null, "metadata": {"file_path": "docs/mail.md", "file_name": "mail.md"}, "hash": "3952239b80473a33667a05307b45640eb19bbec315060f0028595ea05b0c7434"}}, "hash": "961f2a95848bc8fd730ac95238ae1b250c386d68c1c74e0df5b26e8cd5f2ce14", "text": "is useful to send a custom notification message. Note however that Nextflow includes a built-in notification mechanism which is the most convenient way to notify the completion of a workflow execution in most cases. Read the following section to learn about it.\n\n## Workflow notification\n\nNextflow includes a built-in workflow notification features that automatically sends a notification message when a workflow execution terminates.\n\nTo enable simply specify the `-N` option when launching the pipeline execution. For example:\n\n```bash\nnextflow run <pipeline name> -N <recipient address>\n```\n\nIt will send a notification mail when the execution completes similar to the one shown below:\n\n```{image} images/workflow-notification-min.png\n```\n\n:::{warning}\nBy default the notification message is sent with the `sendmail` system tool, which is assumed to be available in the environment where Nextflow is running. Make sure it's properly installed and configured. Alternatively, you can provide the SMTP server configuration settings to use the Nextflow built-in mail support, which doesn't require any external system tool.\n:::\n\nSee the [Mail configuration](#mail-configuration) section to learn about the available mail delivery options and configuration settings.\n\nRead {ref}`Notification scope <config-notification>` section to learn more about the workflow notification configuration details.", "start_char_idx": 5026, "end_char_idx": 6421, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "7b7d0930-3f63-44f4-acdc-cb59b02848d8": {"__data__": {"id_": "7b7d0930-3f63-44f4-acdc-cb59b02848d8", "embedding": null, "metadata": {"file_path": "docs/metadata.md", "file_name": "metadata.md"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "7d353751852345c3464fab7b1c2160fd7e745d75", "node_type": null, "metadata": {"file_path": "docs/metadata.md", "file_name": "metadata.md"}, "hash": "f981f32bcffd1bb02aa8dcc72f74402d7ad1cdcf2b83327f6e3c3f414105837e"}, "3": {"node_id": "ba656205-9446-431c-84c9-e5e336774d5a", "node_type": null, "metadata": {"file_path": "docs/metadata.md", "file_name": "metadata.md"}, "hash": "bde7b828c04164df6c6f213339159208e70545dbee9223b3ade197cb867ea3ce"}}, "hash": "1b727d51ad4d1236b144477361af4f2662817e8f69dfff9d89605c442cd35e9e", "text": "(metadata-page)=\n\n# Workflow introspection\n\n(metadata-workflow)=\n\n## Runtime metadata\n\nThe implicit `workflow` object allows you to access some workflow and runtime metadata in your Nextflow scripts. For example:\n\n```groovy\nprintln \"Project : $workflow.projectDir\"\nprintln \"Git info: $workflow.repository - $workflow.revision [$workflow.commitId]\"\nprintln \"Cmd line: $workflow.commandLine\"\nprintln \"Manifest's pipeline version: $workflow.manifest.version\"\n```\n\n:::{tip}\nTo shortcut access to multiple `workflow` properties, you can use the Groovy [with](<http://docs.groovy-lang.org/latest/html/groovy-jdk/java/lang/Object.html#with(groovy.lang.Closure)>) method.\n:::\n\nThe following table lists the properties that can be accessed on the `workflow` object:\n\n\n`workflow.commandLine`\n: Command line as entered by the user to launch the workflow execution.\n\n`workflow.commitId`\n: Git commit ID of the executed workflow repository.\n: When providing a Git tag, branch name, or commit hash using the `-r` CLI option, the associated `workflow.commitId` is also populated.\n\n`workflow.complete`\n: *Available only in the `workflow.onComplete` handler*\n: Timestamp of workflow when execution is completed.\n\n`workflow.configFiles`\n: Configuration files used for the workflow execution.\n\n`workflow.container`\n: Docker image used to run workflow tasks. When more than one image is used it returns a map object containing `[process name, image name]` pair entries.\n\n`workflow.containerEngine`\n: Returns the name of the container engine (e.g. docker or singularity) or null if no container engine is enabled.\n\n`workflow.duration`\n: *Available only in the `workflow.onComplete` handler*\n: Time elapsed to complete workflow execution.\n\n`workflow.errorMessage`\n: *Available only in the `workflow.onComplete` and `workflow.onError` handlers*\n: Error message of the task that caused the workflow execution to fail.\n\n`workflow.errorReport`\n: *Available only in the `workflow.onComplete` and `workflow.onError` handlers*\n: Detailed error of the task that caused the workflow execution to fail.\n\n`workflow.exitStatus`\n: *Available only in the `workflow.onComplete` and `workflow.onError` handlers*\n: Exit status of the task that caused the workflow execution to fail.\n\n`workflow.homeDir`\n: User system home directory.\n\n`workflow.launchDir`\n: Directory where the workflow execution has been launched.\n\n`workflow.manifest`\n: Entries of the workflow manifest.\n\n`workflow.profile`\n: Used configuration profile.\n\n`workflow.projectDir`\n: Directory where the workflow project is stored in the computer.\n\n`workflow.repository`\n: Project repository Git remote URL.\n\n`workflow.resume`\n: Returns `true` whenever the current instance is resumed from a previous execution.\n\n`workflow.revision`\n: Git branch/tag of the executed workflow repository.\n: When providing a Git tag or branch name using the `-r` CLI option, the `workflow.revision` is also populated.\n\n`workflow.runName`\n: Mnemonic name assigned to this execution instance.\n\n`workflow.scriptFile`\n: Project main script file path.\n\n`workflow.scriptId`\n: Project main script unique hash ID.\n\n`workflow.scriptName`\n: Project main script file name.\n\n`workflow.sessionId`\n: Unique identifier (UUID) associated to current execution.\n\n`workflow.start`\n: Timestamp of workflow at execution start.\n\n`workflow.stubRun`\n:", "start_char_idx": 0, "end_char_idx": 3332, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "ba656205-9446-431c-84c9-e5e336774d5a": {"__data__": {"id_": "ba656205-9446-431c-84c9-e5e336774d5a", "embedding": null, "metadata": {"file_path": "docs/metadata.md", "file_name": "metadata.md"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "7d353751852345c3464fab7b1c2160fd7e745d75", "node_type": null, "metadata": {"file_path": "docs/metadata.md", "file_name": "metadata.md"}, "hash": "f981f32bcffd1bb02aa8dcc72f74402d7ad1cdcf2b83327f6e3c3f414105837e"}, "2": {"node_id": "7b7d0930-3f63-44f4-acdc-cb59b02848d8", "node_type": null, "metadata": {"file_path": "docs/metadata.md", "file_name": "metadata.md"}, "hash": "1b727d51ad4d1236b144477361af4f2662817e8f69dfff9d89605c442cd35e9e"}}, "hash": "bde7b828c04164df6c6f213339159208e70545dbee9223b3ade197cb867ea3ce", "text": "of workflow at execution start.\n\n`workflow.stubRun`\n: Returns `true` whenever the current instance is a stub-run execution .\n\n`workflow.success`\n: *Available only in the `workflow.onComplete` and `workflow.onError` handlers*\n: Reports if the execution completed successfully.\n\n`workflow.userName`\n: User system account name.\n\n`workflow.workDir`\n: Workflow working directory.\n\n(metadata-nextflow)=\n\n## Nextflow metadata\n\nThe implicit `nextflow` object allows you to access the metadata information of the Nextflow runtime.\n\n`nextflow.build`\n: Nextflow runtime build number.\n\n`nextflow.timestamp`\n: Nextflow runtime compile timestamp.\n\n`nextflow.version`\n: Nextflow runtime version number.\n\n`nextflow.version.matches()`\n: This method allows you to check if the Nextflow runtime satisfies a version requirement for your workflow script. The version requirement string can be prefixed with the usual comparison operators eg `>`, `>=`, `=`, etc. or postfixed with the `+` operator to specify a minimum version requirement. For example:\n\n  ```groovy\n  if( !nextflow.version.matches('21.04+') ) {\n      println \"This workflow requires Nextflow version 21.04 or greater -- You are running version $nextflow.version\"\n      exit 1\n  }\n  ```\n\n(metadata-completion-handler)=\n\n## Completion handler\n\nDue to the asynchronous nature of Nextflow the termination of a script does not correspond to the termination of the running workflow. Thus some information, only available on execution completion, needs to be accessed by using an asynchronous handler.\n\nThe `onComplete` event handler is invoked by the framework when the workflow execution is completed. It allows one to access the workflow termination status and other useful information. For example:\n\n```groovy\nworkflow.onComplete {\n    println \"Pipeline completed at: $workflow.complete\"\n    println \"Execution status: ${ workflow.success ? 'OK' : 'failed' }\"\n}\n```\n\nIf you want an e-mail notification on completion, check {ref}`mail-page`.\n\n(metadata-error-handler)=\n\n## Error handler\n\nThe `onError` event handler is invoked by Nextflow when a runtime or process error caused the pipeline execution to stop. For example:\n\n```groovy\nworkflow.onError {\n    println \"Oops... Pipeline execution stopped with the following message: ${workflow.errorMessage}\"\n}\n```\n\n:::{note}\nBoth the `onError` and `onComplete` handlers are invoked when an error condition is encountered. The first is called as soon as the error is raised, while the second is called just before the pipeline execution is about to terminate. When using the `finish` {ref}`process-error-strategy`, there may be a significant gap between the two, depending on the time required to complete any pending job.\n:::\n\n## Decoupling metadata\n\nThe workflow event handlers can be defined also in the `nextflow.config` file. This is useful to decouple the handling of pipeline events from the main script logic.\n\nWhen the event handlers are included in a configuration file the only difference is that the `onComplete` and the `onError` closures have to be defined by using the assignment operator as shown below:\n\n```groovy\nworkflow.onComplete = {\n    // any workflow property can be used here\n    println \"Pipeline complete\"\n    println \"Command line: $workflow.commandLine\"\n}\n\nworkflow.onError = {\n    println \"Oops .. something when wrong\"\n}\n```\n\n:::{note}\nIt is possible to define workflow event handlers both in the pipeline script **and** in the configuration file.\n:::", "start_char_idx": 3279, "end_char_idx": 6748, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "76ac2da5-c3f4-448b-a853-1215816a7da8": {"__data__": {"id_": "76ac2da5-c3f4-448b-a853-1215816a7da8", "embedding": null, "metadata": {"file_path": "docs/metrics.md", "file_name": "metrics.md"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "694d68cd05211c977fd232cf1e8166579eef2d90", "node_type": null, "metadata": {"file_path": "docs/metrics.md", "file_name": "metrics.md"}, "hash": "a6eba2869da6ab3285e7b018cf8d243c2acff34c9f63f9fdbf3a1457786fd8e9"}, "3": {"node_id": "36a2215c-9a6d-45ac-bfa1-dd0349e75cf5", "node_type": null, "metadata": {"file_path": "docs/metrics.md", "file_name": "metrics.md"}, "hash": "798a425bc7769eeae2deebb62d70c7ec4cc120112d96e7295853bddeb60c4eb3"}}, "hash": "68b560968d3e8ecec1f289307b33d4744a6eb992347ed49cc5275a6804ae2c49", "text": "(metrics-page)=\n\n# Metrics\n\nThis section details how the resource usage metrics from the {ref}`Execution report <execution-report>` are computed.\n\n## CPU Usage\n\nThe plot reports how much CPU resources were used by each process.\n\n```{image} images/report-resource-cpu.png\n```\n\nLet's illustrate how this plot behaves with several examples.\n\nIn the first example, let's consider the simple use case in which a process performs one task of pure computation using one CPU. Then, you expect the `Raw Usage` tab to report 100%. If the task is distributed over, 2, 3, 4, `etc.` CPUs, then the `Raw Usage` will be 200%, 300%, 400%, `etc.` respectively. The `% Allocated` tab just rescales the raw value usage with respect to the number of CPUs set with the `cpus` directive (if not set with the directive, the number of CPUs is set to 1, thus showing the same values as in the `Raw Usage` tab). Using the program [stress](https://people.seas.harvard.edu/~apw/stress/) as follows would report 100% in the `Raw Usage` tab and 50% in the `% Allocated` tab since the process asked twice the number of CPUs needed by the process:\n\n```groovy\n#!/usr/bin/env nextflow\n\nprocess CpuUsageEx1 {\n  cpus 2\n\n  \"\"\"\n  stress -c 1 -t 10 # compute square-root of random numbers during 10s using 1 CPU\n  \"\"\"\n}\n```\n\nIn the second example, some time will be spent performing pure computation and some time just waiting. Using the program [stress](https://people.seas.harvard.edu/~apw/stress/) and `sleep` as follows would report 75% in the `Raw Usage` tab:\n\n```groovy\n#!/usr/bin/env nextflow\n\nprocess CpuUsageEx2 {\n  cpus 1\n\n  \"\"\"\n  stress -c 1 -t 10 # compute square-root of random numbers during 10s using 1 CPU\n  stress -c 1 -t 5 # compute square-root of random numbers during 5s using 1 CPU\n  sleep 5 # use no CPU during 5s\n  \"\"\"\n}\n```\n\nIndeed, the percentage of the CPU that this process got is a weighted average taking into account the percentage of the CPU and duration of each individual program over the job duration (a.k.a. elapsed real time, real time or wall time ) as follows:\n\n$$\n\\frac{ 100\\% \\times 10s + 100\\% \\times 5s + 0\\% \\times 5s }{10s+5s+5s} = 75\\%\n$$\n\nThe third example is similar to the second one except that the pure computation stage is performed in a single step forked on 2 CPUs:\n\n```groovy\n#!/usr/bin/env nextflow\n\nprocess CpuUsageEx3 {\n  cpus 2\n\n  \"\"\"\n  stress -c 2 -t 10 # compute square-root of random numbers during 10s using 2 CPUs\n  sleep 10 # use no CPU during 10s\n  \"\"\"\n}\n```\n\nThe `Raw Usage` tab would report 100% in the `Raw Usage` tab:\n\n$$\n\\frac{ 200\\% \\times 10s }{10s+10s} = 100\\%\n$$\n\nThe `% Allocated` tab would report 50%, however, it would not be relevant to change the `cpus` directive from 2 to 1 as the process really uses 2 CPUs at it peak load.\n\n:::{tip}\nThe [stress](https://people.seas.harvard.edu/~apw/stress/) program can be installed with `sudo apt-get install stress` or `sudo yum install stress` depending on your Linux distribution.\n:::\n\n## Memory Usage\n\nThe plot has three tabs showing the usage of the physical memory (RAM), the virtual memory (vmem) and the percentage of RAM used by the process with respect to what was set in the `memory` directive. The peak usage during the execution of the process is reported for both physical and virtual", "start_char_idx": 0, "end_char_idx": 3277, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "36a2215c-9a6d-45ac-bfa1-dd0349e75cf5": {"__data__": {"id_": "36a2215c-9a6d-45ac-bfa1-dd0349e75cf5", "embedding": null, "metadata": {"file_path": "docs/metrics.md", "file_name": "metrics.md"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "694d68cd05211c977fd232cf1e8166579eef2d90", "node_type": null, "metadata": {"file_path": "docs/metrics.md", "file_name": "metrics.md"}, "hash": "a6eba2869da6ab3285e7b018cf8d243c2acff34c9f63f9fdbf3a1457786fd8e9"}, "2": {"node_id": "76ac2da5-c3f4-448b-a853-1215816a7da8", "node_type": null, "metadata": {"file_path": "docs/metrics.md", "file_name": "metrics.md"}, "hash": "68b560968d3e8ecec1f289307b33d4744a6eb992347ed49cc5275a6804ae2c49"}, "3": {"node_id": "78eeaf75-8b78-460e-b074-7f868bbbe16f", "node_type": null, "metadata": {"file_path": "docs/metrics.md", "file_name": "metrics.md"}, "hash": "9050bd9ae728df2f876713c5466bf9c69b29ba354318115645b0e12e1ae5e7da"}}, "hash": "798a425bc7769eeae2deebb62d70c7ec4cc120112d96e7295853bddeb60c4eb3", "text": "directive. The peak usage during the execution of the process is reported for both physical and virtual memories.\n\n:::{note}\nTo better understand the memory usage plot, it is important to know that:\n\n- the total amount of memory used by a process is the `virtual memory (vmem)`. The `vmem` contains all memory areas whether they are in the physical memory (RAM), in the Swap space, on the disk or shared with other processes,\n- the `resident set size (RSS)` is the amount of space of `physical memory (RAM)` held by a process,\n- the relationship is: vmem $\\geq$ RSS + Swap,\n- the `memory` directive sets the RAM requested by the process.\n:::\n\nLet's illustrate how this plot behaves with one example which relies on two C programs.\n\nThe first program just allocates a variable of 1 GiB:\n\n```{code-block} c\n:emphasize-lines: 31,43\n:linenos: true\n\n#include <stdio.h>\n#include <stdlib.h>\n#include <sys/resource.h>\n#include <sys/time.h>\n#include <sys/types.h>\n#include <unistd.h>\n#include <time.h>\n\n/* Get vmem and rss usage from /proc/<pid>/statm */\nstatic int mem_used(pid_t pid, unsigned long* vmem, unsigned long* rss) {\n    FILE* file;\n    char path[40];\n    unsigned int page_size;\n\n    snprintf(path, 40, \"/proc/%ld/statm\", (long) pid);\n    file = fopen(path, \"r\");\n    // vmem and rss are the first values in the file\n    fscanf(file, \"%lu %lu\", vmem, rss);\n    // values in statm are in pages so to get bytes we need to know page size\n    page_size = (unsigned) getpagesize();\n    *vmem = *vmem * page_size;\n    *rss = *rss * page_size;\n\n    fclose(file);\n    return 0;\n}\n\nint main(int argc, char **argv) {\n    unsigned char *address;\n    char input;\n    size_t size = 1024*1024*1024;  // 1 GiB\n    unsigned long i;\n    unsigned long vmem = 0;\n    unsigned long rss = 0;\n    pid_t pid;\n\n    pid = getpid();\n    printf(\"Pid: %ld\\n\", (long) pid);\n\n    mem_used(pid, &vmem, &rss);\n    printf(\"VMEM: %lu RSS: %lu\\n\", vmem, rss);\n\n    address = malloc(size);\n    printf(\"Allocated %d Bytes of memory\\n\", (int) size);\n\n    mem_used(pid, &vmem, &rss);\n    printf(\"VMEM: %lu RSS: %lu\\n\", vmem, rss);\n\n    // Leave time for nextflow to get information\n    sleep(15);\n\n    free(address);\n    return 0;\n}\n```\n\nThe second program allocates a variable of 1 GiB and fills it with data:\n\n```{code-block} c\n:emphasize-lines: 31,43,49-53\n:linenos: true\n\n#include <stdio.h>\n#include <stdlib.h>\n#include <sys/resource.h>\n#include <sys/time.h>\n#include <sys/types.h>\n#include <unistd.h>\n#include <time.h>\n\n/* Get vmem and rss usage from /proc/<pid>/statm */\nstatic int mem_used(pid_t pid, unsigned long* vmem, unsigned long* rss) {\n    FILE* file;\n    char path[40];\n    unsigned int page_size;\n\n ", "start_char_idx": 3191, "end_char_idx": 5872, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "78eeaf75-8b78-460e-b074-7f868bbbe16f": {"__data__": {"id_": "78eeaf75-8b78-460e-b074-7f868bbbe16f", "embedding": null, "metadata": {"file_path": "docs/metrics.md", "file_name": "metrics.md"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "694d68cd05211c977fd232cf1e8166579eef2d90", "node_type": null, "metadata": {"file_path": "docs/metrics.md", "file_name": "metrics.md"}, "hash": "a6eba2869da6ab3285e7b018cf8d243c2acff34c9f63f9fdbf3a1457786fd8e9"}, "2": {"node_id": "36a2215c-9a6d-45ac-bfa1-dd0349e75cf5", "node_type": null, "metadata": {"file_path": "docs/metrics.md", "file_name": "metrics.md"}, "hash": "798a425bc7769eeae2deebb62d70c7ec4cc120112d96e7295853bddeb60c4eb3"}, "3": {"node_id": "24085bab-197a-40a6-92bd-31637ca55de8", "node_type": null, "metadata": {"file_path": "docs/metrics.md", "file_name": "metrics.md"}, "hash": "8b08734473a4537d77f67307441d5a6df7f05b124145bb5c3a588897206171cd"}}, "hash": "9050bd9ae728df2f876713c5466bf9c69b29ba354318115645b0e12e1ae5e7da", "text": "   char path[40];\n    unsigned int page_size;\n\n    snprintf(path, 40, \"/proc/%ld/statm\", (long) pid);\n    file = fopen(path, \"r\");\n    // vmem and rss are the first values in the file\n    fscanf(file, \"%lu %lu\", vmem, rss);\n    // values in statm are in pages so to get bytes we need to know page size\n    page_size = (unsigned) getpagesize();\n    *vmem = *vmem * page_size;\n    *rss = *rss * page_size;\n\n    fclose(file);\n    return 0;\n}\n\nint main(int argc, char **argv) {\n    unsigned char *address;\n    char input;\n    size_t size = 1024*1024*1024;  // 1 GiB\n    unsigned long i;\n    unsigned long vmem = 0;\n    unsigned long rss = 0;\n    pid_t pid;\n\n    pid = getpid();\n    printf(\"Pid: %ld\\n\", (long) pid);\n\n    mem_used(pid, &vmem, &rss);\n    printf(\"VMEM: %lu RSS: %lu\\n\", vmem, rss);\n\n    address = malloc(size);\n    printf(\"Allocated %d Bytes of memory\\n\", (int) size);\n\n    mem_used(pid, &vmem, &rss);\n    printf(\"VMEM: %lu RSS: %lu\\n\", vmem, rss);\n\n    printf(\"Filling memory with data...\");\n    fflush(stdout);\n    for (i = 0; i < size; i++) {\n        *(address + i) = 123;\n    }\n\n    mem_used(pid, &vmem, &rss);\n    printf(\"\\nVMEM: %lu RSS: %lu\\n\", vmem, rss);\n\n    // Leave time for nextflow to get information\n    sleep(15);\n\n    free(address);\n    return 0;\n}\n```\n\nThe first and second programs are executed in `foo` and `bar` processes respectively as follows:\n\n```groovy\n#!/usr/bin/env nextflow\n\nprocess foo {\n    memory '1.5 GB'\n\n    \"\"\"\n    memory_vmem_1GiB_ram_0Gib\n    \"\"\"\n}\n\nprocess bar {\n    memory '1.5 GB'\n\n    \"\"\"\n    memory_vmem_1GiB_ram_1Gib\n    \"\"\"\n}\n```\n\nThe `Virtual (RAM + Disk swap)` tab shows that both `foo` and `bar` processes use the same amount of virtual memory (~1 GiB):\n\n```{image} images/report-resource-memory-vmem.png\n```\n\nHowever, the `Physical (RAM)` tab shows that only the `bar` process uses ~1 GiB of RAM while `foo` process uses ~0 GiB:\n\n```{image} images/report-resource-memory-ram.png\n```\n\nAs expected, the `% RAM Allocated` tab shows that 0% of the resource set in the `memory` directive was used for `foo` process while 67% (= 1 / 1.5) of the resource were used for `bar` process:\n\n```{image} images/report-resource-memory-pctram.png\n```\n\n:::{warning}\nMemory and storage metrics are reported in bytes. This means that 1KB = $1024$ bytes, 1 MB = $1024^2$ bytes, 1 GB = $1024^3$ bytes, etc.\n:::\n\n## Job Duration\n\nThe plot has two tabs the job duration (a.k.a. elapsed real time, real time or wall time ) in the `Raw Usage` tag and the percentage of requested time used in the `%", "start_char_idx": 5923, "end_char_idx": 8454, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "24085bab-197a-40a6-92bd-31637ca55de8": {"__data__": {"id_": "24085bab-197a-40a6-92bd-31637ca55de8", "embedding": null, "metadata": {"file_path": "docs/metrics.md", "file_name": "metrics.md"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "694d68cd05211c977fd232cf1e8166579eef2d90", "node_type": null, "metadata": {"file_path": "docs/metrics.md", "file_name": "metrics.md"}, "hash": "a6eba2869da6ab3285e7b018cf8d243c2acff34c9f63f9fdbf3a1457786fd8e9"}, "2": {"node_id": "78eeaf75-8b78-460e-b074-7f868bbbe16f", "node_type": null, "metadata": {"file_path": "docs/metrics.md", "file_name": "metrics.md"}, "hash": "9050bd9ae728df2f876713c5466bf9c69b29ba354318115645b0e12e1ae5e7da"}}, "hash": "8b08734473a4537d77f67307441d5a6df7f05b124145bb5c3a588897206171cd", "text": "in the `Raw Usage` tag and the percentage of requested time used in the `% Allocated` tab with respect to the duration set in the `time` directive of the process.\n\n```{image} images/report-resource-job-duration.png\n```\n\n## I/O Usage\n\nThe plot has two tabs showing how many data were read and/or written each process. For example, the following processes read and write 1GB and 256MB of data respectively:\n\n```groovy\n#!/usr/bin/env nextflow\n\nprocess io_read_write_1G {\n  \"\"\"\n  dd if=/dev/zero of=/dev/null bs=1G count=1\n  \"\"\"\n}\n\nprocess io_read_write_256M {\n  \"\"\"\n  dd if=/dev/zero of=/dev/null bs=256M count=1\n  \"\"\"\n}\n```\n\n`Read` tab:\n\n```{image} images/report-resource-io-read.png\n```\n\n`Write` tab:\n\n```{image} images/report-resource-io-write.png\n```", "start_char_idx": 8417, "end_char_idx": 9168, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "e279fcd0-f738-464e-bfc1-64bf088b3bc1": {"__data__": {"id_": "e279fcd0-f738-464e-bfc1-64bf088b3bc1", "embedding": null, "metadata": {"file_path": "docs/operator.md", "file_name": "operator.md"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "636d82cbe736f0e4b87074e2c4bac77becae2a28", "node_type": null, "metadata": {"file_path": "docs/operator.md", "file_name": "operator.md"}, "hash": "088b8b3573d8ad47eb5406f5c3ce360a661b9661e95b0a4c8ff24504e0be2726"}, "3": {"node_id": "51b774b9-2723-42b6-88e8-a121bc275146", "node_type": null, "metadata": {"file_path": "docs/operator.md", "file_name": "operator.md"}, "hash": "25c727b44473dfc4300518a1d5a5e945ebc3069037c6c15dcb11e11d8f036d07"}}, "hash": "7b94cd8fa3f6e358c74bf8abe97daccb49fe3268ecff86a3f28e4b67478d71b4", "text": "(operator-page)=\n\n# Operators\n\nNextflow **operators** are methods that allow you to manipulate channels. Every operator, with the exception of [set](#set) and [subscribe](#subscribe), produces one or more new channels, allowing you to chain operators to fit your needs.\n\nThis page is a comprehensive reference for all Nextflow operators. However, if you are new to Nextflow, here are some suggested operators to learn for common use cases:\n\n- Filtering: [filter](#filter), [randomSample](#randomsample), [take](#take), [unique](#unique)\n- Reduction: [collect](#collect), [groupTuple](#grouptuple), [reduce](#reduce)\n- Parsing text data: [splitCsv](#splitcsv), [splitJson](#splitjson), [splitText](#splittext)\n- Combining channels: [combine](#combine), [concat](#concat), [join](#join), [mix](#mix)\n- Forking channels: [branch](#branch), [multiMap](#multimap)\n- Maths: [count](#count), [max](#max), [min](#min), [sum](#sum)\n- Other: [ifEmpty](#ifempty), [map](#map), [set](#set), [view](#view)\n\n(operator-branch)=\n\n## branch\n\n:::{versionadded} 19.08.0-edge\n:::\n\n*Returns: map of queue channels*\n\nThe `branch` operator allows you to forward the items emitted by a source channel to one or more output channels, choosing one out of them at a time.\n\nThe selection criteria is defined by specifying a {ref}`closure <script-closure>` that provides one or more boolean expression, each of which is identified by a unique label. On the first expression that evaluates to a *true* value, the current item is bound to a named channel as the label identifier. For example:\n\n```groovy\nChannel\n    .of(1, 2, 3, 40, 50)\n    .branch {\n        small: it < 10\n        large: it > 10\n    }\n    .set { result }\n\n result.small.view { \"$it is small\" }\n result.large.view { \"$it is large\" }\n```\n\n```\n1 is small\n2 is small\n3 is small\n40 is large\n50 is large\n```\n\n:::{note}\nThe above *small* and *large* strings may be printed in any order due to the asynchronous execution of the `view` operator.\n:::\n\nA default fallback condition can be specified using `true` as the last branch condition:\n\n```groovy\nChannel\n    .from(1, 2, 3, 40, 50)\n    .branch {\n        small: it < 10\n        large: it < 50\n        other: true\n    }\n```\n\nThe value returned by each branch condition can be customised by specifying an optional expression statement(s) just after the condition expression. For example:\n\n```groovy\nChannel\n    .from(1, 2, 3, 40, 50)\n    .branch {\n        foo: it < 10\n            return it+2\n\n        bar: it < 50\n            return it-2\n\n        other: true\n            return 0\n    }\n```\n\n:::{tip}\nWhen the `return` keyword is omitted, the value of the last expression statement is implicitly returned.\n:::\n\nTo create a branch criteria as variable that can be passed as an argument to more than one `branch` operator use the `branchCriteria` built-in method as shown below:\n\n```groovy\ndef criteria", "start_char_idx": 0, "end_char_idx": 2879, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "51b774b9-2723-42b6-88e8-a121bc275146": {"__data__": {"id_": "51b774b9-2723-42b6-88e8-a121bc275146", "embedding": null, "metadata": {"file_path": "docs/operator.md", "file_name": "operator.md"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "636d82cbe736f0e4b87074e2c4bac77becae2a28", "node_type": null, "metadata": {"file_path": "docs/operator.md", "file_name": "operator.md"}, "hash": "088b8b3573d8ad47eb5406f5c3ce360a661b9661e95b0a4c8ff24504e0be2726"}, "2": {"node_id": "e279fcd0-f738-464e-bfc1-64bf088b3bc1", "node_type": null, "metadata": {"file_path": "docs/operator.md", "file_name": "operator.md"}, "hash": "7b94cd8fa3f6e358c74bf8abe97daccb49fe3268ecff86a3f28e4b67478d71b4"}, "3": {"node_id": "1fd51f4e-62cd-4a67-a4cb-96f822030abe", "node_type": null, "metadata": {"file_path": "docs/operator.md", "file_name": "operator.md"}, "hash": "cdbf31755dc5524adccd58009d11248c3afcfd242426d1e34e3296eced102001"}}, "hash": "25c727b44473dfc4300518a1d5a5e945ebc3069037c6c15dcb11e11d8f036d07", "text": "built-in method as shown below:\n\n```groovy\ndef criteria = branchCriteria {\n    small: it < 10\n    large: it > 10\n}\n\nChannel.of(1, 2, 30).branch(criteria).set { ch1 }\nChannel.of(10, 20, 1).branch(criteria).set { ch2 }\n```\n\n## buffer\n\n*Returns: queue channel*\n\nThe `buffer` operator gathers the items emitted by the source channel into subsets and emits these subsets separately.\n\nThere are a number of ways you can regulate how `buffer` gathers the items from the source channel into subsets:\n\n- `buffer( closingCondition )`: starts to collect the items emitted by the channel into a subset until the `closingCondition` is verified. After that the subset is emitted to the resulting channel and new items are gathered into a new subset. The process is repeated until the last value in the source channel is sent. The `closingCondition` can be specified either as a {ref}`regular expression <script-regexp>`, a Java class, a literal value, or a boolean predicate that has to be satisfied. For example:\n\n  ```groovy\n  Channel\n      .of( 1, 2, 3, 1, 2, 3 )\n      .buffer { it == 2 }\n      .view()\n\n  // emitted values\n  [1,2]\n  [3,1,2]\n  ```\n\n- `buffer( openingCondition, closingCondition )`: starts to gather the items emitted by the channel as soon as one of the them verify the `openingCondition` and it continues until there is one item which verify the `closingCondition`. After that the subset is emitted and it continues applying the described logic until the last channel item is emitted. Both conditions can be defined either as a {ref}`regular expression <script-regexp>`, a literal value, a Java class, or a boolean predicate that need to be satisfied. For example:\n\n  ```groovy\n  Channel\n      .of( 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2 )\n      .buffer( 2, 4 )\n      .view()\n\n  // emits bundles starting with '2' and ending with'4'\n  [2,3,4]\n  [2,3,4]\n  ```\n\n- `buffer( size: n )`: transform the source channel in such a way that it emits tuples made up of `n` elements. An incomplete tuple is discarded. For example:\n\n  ```groovy\n  Channel\n      .of( 1, 2, 3, 1, 2, 3, 1 )\n      .buffer( size: 2 )\n      .view()\n\n  // emitted values\n  [1, 2]\n  [3, 1]\n  [2, 3]\n  ```\n\n  If you want to emit the last items in a tuple containing less than `n` elements, simply add the parameter `remainder` specifying `true`, for example:\n\n  ```groovy\n  Channel\n      .of( 1, 2, 3, 1, 2, 3, 1 )\n      .buffer( size: 2, remainder: true )\n      .view()\n\n  // emitted values\n  [1, 2]\n  [3, 1]\n  [2, 3]\n  [1]\n  ```\n\n- `buffer( size: n, skip: m )`: as in the previous example, it emits tuples containing `n` elements, but skips `m` values before starting to collect the values for the next tuple (including the first emission). For example:\n\n  ```groovy\n  Channel\n      .of( 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2 )\n      .buffer( size:3, skip:2 )\n", "start_char_idx": 2830, "end_char_idx": 5655, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "1fd51f4e-62cd-4a67-a4cb-96f822030abe": {"__data__": {"id_": "1fd51f4e-62cd-4a67-a4cb-96f822030abe", "embedding": null, "metadata": {"file_path": "docs/operator.md", "file_name": "operator.md"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "636d82cbe736f0e4b87074e2c4bac77becae2a28", "node_type": null, "metadata": {"file_path": "docs/operator.md", "file_name": "operator.md"}, "hash": "088b8b3573d8ad47eb5406f5c3ce360a661b9661e95b0a4c8ff24504e0be2726"}, "2": {"node_id": "51b774b9-2723-42b6-88e8-a121bc275146", "node_type": null, "metadata": {"file_path": "docs/operator.md", "file_name": "operator.md"}, "hash": "25c727b44473dfc4300518a1d5a5e945ebc3069037c6c15dcb11e11d8f036d07"}, "3": {"node_id": "aa7b7f87-d852-426b-bd8e-7fbbe178a655", "node_type": null, "metadata": {"file_path": "docs/operator.md", "file_name": "operator.md"}, "hash": "c3d97554a4bebef2f3cb594215ce6f3adfa83a68b78d3d071f4bf03db7771f1a"}}, "hash": "cdbf31755dc5524adccd58009d11248c3afcfd242426d1e34e3296eced102001", "text": "2 )\n      .buffer( size:3, skip:2 )\n      .view()\n\n  // emitted values\n  [3, 4, 5]\n  [3, 4, 5]\n  ```\n\n  If you want to emit the remaining items in a tuple containing less than `n` elements, simply add the parameter `remainder` specifying `true`, as shown in the previous example.\n\nSee also: [collate](#collate) operator.\n\n## collate\n\n*Returns: queue channel*\n\nThe `collate` operator transforms a channel in such a way that the emitted values are grouped in tuples containing `n` items. For example:\n\n```groovy\nChannel\n    .of(1,2,3,1,2,3,1)\n    .collate( 3 )\n    .view()\n```\n\n```\n[1, 2, 3]\n[1, 2, 3]\n[1]\n```\n\nAs shown in the above example the last tuple may be incomplete e.g. contain fewer elements than the specified size. If you want to avoid this, specify `false` as the second parameter. For example:\n\n```groovy\nChannel\n    .of(1,2,3,1,2,3,1)\n    .collate( 3, false )\n    .view()\n```\n\n```\n[1, 2, 3]\n[1, 2, 3]\n```\n\nA second version of the `collate` operator allows you to specify, after the `size`, the `step` by which elements are collected in tuples. For example:\n\n```groovy\nChannel\n    .of(1,2,3,4)\n    .collate( 3, 1 )\n    .view()\n```\n\n```\n[1, 2, 3]\n[2, 3, 4]\n[3, 4]\n[4]\n```\n\nAs before, if you don't want to emit the last items which do not complete a tuple, specify `false` as the third parameter.\n\nSee also: [buffer](#buffer) operator.\n\n(operator-collect)=\n\n## collect\n\n*Returns: value channel*\n\nThe `collect` operator collects all the items emitted by a channel to a `List` and return the resulting object as a sole emission. For example:\n\n```groovy\nChannel\n    .of( 1, 2, 3, 4 )\n    .collect()\n    .view()\n```\n\n```\n[1,2,3,4]\n```\n\nAn optional {ref}`closure <script-closure>` can be specified to transform each item before adding it to the resulting list. For example:\n\n```groovy\nChannel\n    .of( 'hello', 'ciao', 'bonjour' )\n    .collect { it.length() }\n    .view()\n```\n\n```\n[5,4,7]\n```\n\nAvailable options:\n\n`flat`\n: When `true` nested list structures are normalised and their items are added to the resulting list object (default: `true`).\n\n`sort`\n: When `true` the items in the resulting list are sorted by their natural ordering. It is possible to provide a custom ordering criteria by using either a {ref}`closure <script-closure>` or a [Comparator](https://docs.oracle.com/javase/8/docs/api/java/util/Comparator.html) object (default: `false`).\n\nSee also: [toList](#tolist) and [toSortedList](#tosortedlist) operator.\n\n## collectFile\n\n*Returns: queue channel*\n\nThe `collectFile` operator allows you to gather the items emitted by a channel and save them to one or more files. The operator returns a new channel that emits the collected file(s).\n\nIn the simplest case, just specify the name of a file where the entries have to be stored. For", "start_char_idx": 5680, "end_char_idx": 8436, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "aa7b7f87-d852-426b-bd8e-7fbbe178a655": {"__data__": {"id_": "aa7b7f87-d852-426b-bd8e-7fbbe178a655", "embedding": null, "metadata": {"file_path": "docs/operator.md", "file_name": "operator.md"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "636d82cbe736f0e4b87074e2c4bac77becae2a28", "node_type": null, "metadata": {"file_path": "docs/operator.md", "file_name": "operator.md"}, "hash": "088b8b3573d8ad47eb5406f5c3ce360a661b9661e95b0a4c8ff24504e0be2726"}, "2": {"node_id": "1fd51f4e-62cd-4a67-a4cb-96f822030abe", "node_type": null, "metadata": {"file_path": "docs/operator.md", "file_name": "operator.md"}, "hash": "cdbf31755dc5524adccd58009d11248c3afcfd242426d1e34e3296eced102001"}, "3": {"node_id": "e260f3c4-051e-4076-b03c-37a78b8189b2", "node_type": null, "metadata": {"file_path": "docs/operator.md", "file_name": "operator.md"}, "hash": "d2f7f1e39d099453336a09bd24b695e938640f9b54d07196edae92dfcf5378e6"}}, "hash": "c3d97554a4bebef2f3cb594215ce6f3adfa83a68b78d3d071f4bf03db7771f1a", "text": "just specify the name of a file where the entries have to be stored. For example:\n\n```groovy\nChannel\n    .of('alpha', 'beta', 'gamma')\n    .collectFile(name: 'sample.txt', newLine: true)\n    .subscribe {\n        println \"Entries are saved to file: $it\"\n        println \"File content is: ${it.text}\"\n    }\n```\n\nA second version of the `collectFile` operator allows you to gather the items emitted by a channel and group them together into files whose name can be defined by a dynamic criteria. The grouping criteria is specified by a {ref}`closure <script-closure>` that must return a pair in which the first element defines the file name for the group and the second element the actual value to be appended to that file. For example:\n\n```groovy\nChannel\n    .of('Hola', 'Ciao', 'Hello', 'Bonjour', 'Halo')\n    .collectFile() { item ->\n        [ \"${item[0]}.txt\", item + '\\n' ]\n    }\n    .subscribe {\n        println \"File ${it.name} contains:\"\n        println it.text\n    }\n```\n\n```\nFile 'B.txt' contains:\nBonjour\n\nFile 'C.txt' contains:\nCiao\n\nFile 'H.txt' contains:\nHalo\nHola\nHello\n```\n\n:::{tip}\nWhen the items emitted by the source channel are files, the grouping criteria can be omitted. In this case the items content will be grouped into file(s) having the same name as the source items.\n:::\n\nAvailable options:\n\n`cache`\n: Controls the caching ability of the `collectFile` operator when using the *resume* feature. It follows the same semantic of the {ref}`process-cache` directive (default: `true`).\n\n`keepHeader`\n: Prepend the resulting file with the header fetched in the first collected file. The header size (ie. lines) can be specified by using the `skip` parameter (default: `false`), to determine how many lines to remove from all collected files except for the first (where no lines will be removed).\n\n`name`\n: Name of the file where all received values are stored.\n\n`newLine`\n: Appends a `newline` character automatically after each entry (default: `false`).\n\n`seed`\n: A value or a map of values used to initialise the files content.\n\n`skip`\n: Skip the first `n` lines e.g. `skip: 1`.\n\n`sort`\n: Defines sorting criteria of content in resulting file(s). Can be one of the following values:\n\n  - `false`: Disable content sorting. Entries are appended as they are produced.\n  - `true`: Order the content by the entry's natural ordering i.e. numerical for number, lexicographic for string, etc. See the [Java documentation](http://docs.oracle.com/javase/tutorial/collections/interfaces/order.html) for more information.\n  - `'index'`: Order the content by the incremental index number assigned to each entry while they are collected.\n  - `'hash'`: (default) Order the content by the hash number associated to each entry\n  - `'deep'`: Similar to the previous, but the hash number is created on actual entries content e.g. when the entry is a file the hash is created on the actual file content.\n  - A custom sorting criteria can be specified by using either a {ref}`Closure <script-closure>` or a [Comparator](http://docs.oracle.com/javase/7/docs/api/java/util/Comparator.html) object.\n\n  The file content is sorted in such a way that it does not depend on the order in which entries were added to it, which guarantees that it is consistent", "start_char_idx": 8405, "end_char_idx": 11654, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "e260f3c4-051e-4076-b03c-37a78b8189b2": {"__data__": {"id_": "e260f3c4-051e-4076-b03c-37a78b8189b2", "embedding": null, "metadata": {"file_path": "docs/operator.md", "file_name": "operator.md"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "636d82cbe736f0e4b87074e2c4bac77becae2a28", "node_type": null, "metadata": {"file_path": "docs/operator.md", "file_name": "operator.md"}, "hash": "088b8b3573d8ad47eb5406f5c3ce360a661b9661e95b0a4c8ff24504e0be2726"}, "2": {"node_id": "aa7b7f87-d852-426b-bd8e-7fbbe178a655", "node_type": null, "metadata": {"file_path": "docs/operator.md", "file_name": "operator.md"}, "hash": "c3d97554a4bebef2f3cb594215ce6f3adfa83a68b78d3d071f4bf03db7771f1a"}, "3": {"node_id": "91eaade7-7478-4723-8fb6-3e34868a4859", "node_type": null, "metadata": {"file_path": "docs/operator.md", "file_name": "operator.md"}, "hash": "b68b01bf951143c5512dee3ab377ec0cce7b5b22655b0af27fa54baa1d31a605"}}, "hash": "d2f7f1e39d099453336a09bd24b695e938640f9b54d07196edae92dfcf5378e6", "text": "the order in which entries were added to it, which guarantees that it is consistent (i.e. does not change) across different executions with the same data.\n\n`storeDir`\n: Folder where the resulting file(s) are be stored.\n\n`tempDir`\n: Folder where temporary files, used by the collecting process, are stored.\n\nThe following snippet shows how sort the content of the result file alphabetically:\n\n```groovy\nChannel\n    .of('Z'..'A')\n    .collectFile(name:'result', sort: true, newLine: true)\n    .view { it.text }\n```\n\n```\nA\nB\nC\n:\nZ\n```\n\nThe following example shows how use a `closure` to collect and sort all sequences in a FASTA file from shortest to longest:\n\n```groovy\nChannel\n    .fromPath('/data/sequences.fa')\n    .splitFasta( record: [id: true, sequence: true] )\n    .collectFile( name:'result.fa', sort: { it.size() } ) {\n        it.sequence\n    }\n    .view { it.text }\n```\n\n:::{warning}\nThe `collectFile` operator needs to store files in a temporary folder that is automatically deleted on workflow completion. For performance reasons this folder is located in the machine's local storage, and it will require as much free space as the data that is being collected. Optionally, a different temporary data folder can be specified by using the `tempDir` parameter.\n:::\n\n(operator-combine)=\n\n## combine\n\n*Returns: queue channel*\n\nThe `combine` operator combines (cartesian product) the items emitted by two channels or by a channel and a `Collection` object (as right operand). For example:\n\n```groovy\nnumbers = Channel.of(1, 2, 3)\nwords = Channel.of('hello', 'ciao')\nnumbers\n    .combine(words)\n    .view()\n```\n\n```\n[1, hello]\n[2, hello]\n[3, hello]\n[1, ciao]\n[2, ciao]\n[3, ciao]\n```\n\nA second version of the `combine` operator allows you to combine items that share a common matching key. The index of the key element is specified by using the `by` parameter (zero-based index, multiple indices can be specified as a list of integers). For example:\n\n```groovy\nleft = Channel.of(['A', 1], ['B', 2], ['A', 3])\nright = Channel.of(['B', 'x'], ['B', 'y'], ['A', 'z'], ['A', 'w'])\n\nleft\n    .combine(right, by: 0)\n    .view()\n```\n\n```\n[A, 1, z]\n[A, 3, z]\n[A, 1, w]\n[A, 3, w]\n[B, 2, x]\n[B, 2, y]\n```\n\nSee also [join](#join) and [cross](#cross).\n\n(operator-concat)=\n\n## concat\n\n*Returns: queue channel*\n\nThe `concat` operator allows you to *concatenate* the items emitted by two or more channels to a new channel. The items emitted by the resulting channel are in the same order as specified in the operator arguments.\n\nIn other words, given *N* channels, the items from the *i+1 th* channel are emitted only after all of the items from the *i th* channel have been emitted.\n\nFor example:\n\n```groovy\na = Channel.of('a', 'b', 'c')\nb = Channel.of(1, 2, 3)\nc = Channel.of('p', 'q')\n\nc.concat( b, a", "start_char_idx": 11644, "end_char_idx": 14433, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "91eaade7-7478-4723-8fb6-3e34868a4859": {"__data__": {"id_": "91eaade7-7478-4723-8fb6-3e34868a4859", "embedding": null, "metadata": {"file_path": "docs/operator.md", "file_name": "operator.md"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "636d82cbe736f0e4b87074e2c4bac77becae2a28", "node_type": null, "metadata": {"file_path": "docs/operator.md", "file_name": "operator.md"}, "hash": "088b8b3573d8ad47eb5406f5c3ce360a661b9661e95b0a4c8ff24504e0be2726"}, "2": {"node_id": "e260f3c4-051e-4076-b03c-37a78b8189b2", "node_type": null, "metadata": {"file_path": "docs/operator.md", "file_name": "operator.md"}, "hash": "d2f7f1e39d099453336a09bd24b695e938640f9b54d07196edae92dfcf5378e6"}, "3": {"node_id": "c71dbe1d-3141-4b8c-a3a9-ebdf501354c5", "node_type": null, "metadata": {"file_path": "docs/operator.md", "file_name": "operator.md"}, "hash": "10ca2f84f79b3d50d22a875969a3a3565de25805d1abbcfc0edf2d6af8f632fb"}}, "hash": "b68b01bf951143c5512dee3ab377ec0cce7b5b22655b0af27fa54baa1d31a605", "text": "= Channel.of('p', 'q')\n\nc.concat( b, a ).view()\n```\n\n```\np\nq\n1\n2\n3\na\nb\nc\n```\n\n(operator-count)=\n\n## count\n\n*Returns: value channel*\n\nThe `count` operator creates a channel that emits a single item: a number that represents the total number of items emitted by the source channel. For example:\n\n```groovy\nChannel\n    .of(9,1,7,5)\n    .count()\n    .view()\n// -> 4\n```\n\nAn optional parameter can be provided to select which items are to be counted. The selection criteria can be specified either as a {ref}`regular expression <script-regexp>`, a literal value, a Java class, or a boolean predicate that needs to be satisfied. For example:\n\n```groovy\nChannel\n    .of(4,1,7,1,1)\n    .count(1)\n    .view()\n// -> 3\n\nChannel\n    .of('a','c','c','q','b')\n    .count ( ~/c/ )\n    .view()\n// -> 2\n\nChannel\n    .of('a','c','c','q','b')\n    .count { it <= 'c' }\n    .view()\n// -> 4\n```\n\n(operator-countfasta)=\n\n## countFasta\n\n*Returns: value channel*\n\nCounts the total number of records in a channel of FASTA files, equivalent to `splitFasta | count`. See [splitFasta](#splitfasta) for the list of available options.\n\n(operator-countfastq)=\n\n## countFastq\n\n*Returns: value channel*\n\nCounts the total number of records in a channel of FASTQ files, equivalent to `splitFastq | count`. See [splitFastq](#splitfastq) for the list of available options.\n\n(operator-countjson)=\n\n## countJson\n\n*Returns: value channel*\n\nCounts the total number of records in a channel of JSON files, equivalent to `splitJson | count`. See [splitJson](#splitjson) for the list of available options.\n\n(operator-countlines)=\n\n## countLines\n\n*Returns: value channel*\n\nCounts the total number of lines in a channel of text files, equivalent to `splitText | count`. See [splitLines](#splittext) for the list of available options.\n\n(operator-cross)=\n\n## cross\n\n*Returns: queue channel*\n\nThe `cross` operator allows you to combine the items of two channels in such a way that the items of the source channel are emitted along with the items emitted by the target channel for which they have a matching key.\n\nThe key is defined, by default, as the first entry in an array, a list or map object, or the value itself for any other data type. For example:\n\n```groovy\nsource = Channel.of( [1, 'alpha'], [2, 'beta'] )\ntarget = Channel.of( [1, 'x'], [1, 'y'], [1, 'z'], [2, 'p'], [2, 'q'], [2, 't'] )\n\nsource.cross(target).view()\n```\n\n```\n[ [1, alpha], [1, x] ]\n[ [1, alpha], [1, y] ]\n[ [1, alpha], [1, z] ]\n[ [2, beta],  [2, p] ]\n[ [2, beta],  [2, q] ]\n[ [2, beta],  [2, t] ]\n```\n\nThe above example shows how the items emitted by the source channels are associated to the ones emitted by the target channel (on the right) having the same key.\n\nThere are two important caveats when using the `cross` operator:\n\n1. The operator is not `commutative`, i.e. the result of `a.cross(b)` is different from", "start_char_idx": 14469, "end_char_idx": 17314, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "c71dbe1d-3141-4b8c-a3a9-ebdf501354c5": {"__data__": {"id_": "c71dbe1d-3141-4b8c-a3a9-ebdf501354c5", "embedding": null, "metadata": {"file_path": "docs/operator.md", "file_name": "operator.md"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "636d82cbe736f0e4b87074e2c4bac77becae2a28", "node_type": null, "metadata": {"file_path": "docs/operator.md", "file_name": "operator.md"}, "hash": "088b8b3573d8ad47eb5406f5c3ce360a661b9661e95b0a4c8ff24504e0be2726"}, "2": {"node_id": "91eaade7-7478-4723-8fb6-3e34868a4859", "node_type": null, "metadata": {"file_path": "docs/operator.md", "file_name": "operator.md"}, "hash": "b68b01bf951143c5512dee3ab377ec0cce7b5b22655b0af27fa54baa1d31a605"}, "3": {"node_id": "d88f337e-736e-4b55-bc11-3bdbd99a2641", "node_type": null, "metadata": {"file_path": "docs/operator.md", "file_name": "operator.md"}, "hash": "eb691d340e9f1b44cf402985e123289c044ffde622446bcd7b69bb79d661f0b0"}}, "hash": "10ca2f84f79b3d50d22a875969a3a3565de25805d1abbcfc0edf2d6af8f632fb", "text": "i.e. the result of `a.cross(b)` is different from `b.cross(a)`\n2. The source channel should emits items for which there's no key repetition i.e. the emitted items have an unique key identifier.\n\nOptionally, a mapping function can be specified in order to provide a custom rule to associate an item to a key.\n\n## distinct\n\n*Returns: queue channel*\n\nThe `distinct` operator allows you to remove *consecutive* duplicated items from a channel, so that each emitted item is different from the preceding one. For example:\n\n```groovy\nChannel\n    .of( 1,1,2,2,2,3,1,1,2,2,3 )\n    .distinct()\n    .subscribe onNext: { println it }, onComplete: { println 'Done' }\n```\n\n```\n1\n2\n3\n1\n2\n3\nDone\n```\n\nYou can also specify an optional {ref}`closure <script-closure>` that customizes the way it distinguishes between distinct items. For example:\n\n```groovy\nChannel\n    .of( 1,1,2,2,2,3,1,1,2,4,6 )\n    .distinct { it % 2 }\n    .subscribe onNext: { println it }, onComplete: { println 'Done' }\n```\n\n```\n1\n2\n3\n2\nDone\n```\n\n(operator-dump)=\n\n## dump\n\n*Returns: queue channel or value channel, depending on the input*\n\nThe `dump` operator prints the items emitted by the channel to which is applied only when the option `-dump-channels` is specified on the `run` command line, otherwise it is ignored.\n\nThis is useful to enable the debugging of one or more channel content on-demand by using a command line option instead of modifying your script code.\n\nAn optional `tag` parameter allows you to select which channel to dump. For example:\n\n```groovy\nChannel\n    .of(1,2,3)\n    .map { it+1 }\n    .dump(tag: 'foo')\n\nChannel\n    .of(1,2,3)\n    .map { it^2 }\n    .dump(tag: 'bar')\n```\n\nThen you will be able to specify the tag `foo` or `bar` as an argument of the `-dump-channels` option to print either the content of the first or the second channel. Multiple tag names can be specified separating them with a `,` character.\n\n:::{versionadded} 22.10.0\nThe output can be formatted by enabling the `pretty` option:\n\n```groovy\nChannel\n    .fromSRA('SRP043510')\n    .dump(tag: 'foo', pretty: true)\n```\n:::\n\n## filter\n\n*Returns: queue channel*\n\nThe `filter` operator allows you to get only the items emitted by a channel that satisfy a condition and discarding all the others. The filtering condition can be specified by using either a {ref}`regular expression <script-regexp>`, a literal value, a type qualifier (i.e. a Java class) or any boolean predicate.\n\nThe following example shows how to filter a channel by using a regular expression that returns only strings that begin with `a`:\n\n```groovy\nChannel\n    .of( 'a', 'b', 'aa', 'bc', 3, 4.5 )\n    .filter( ~/^a.*/ )\n    .view()\n```\n\n```\na\naa\n```\n\nThe following example shows how to filter a channel by specifying the type qualifier `Number` so that only numbers are returned:\n\n```groovy\nChannel\n    .of( 'a', 'b', 'aa', 'bc', 3, 4.5 )\n    .filter( Number )\n   ", "start_char_idx": 17307, "end_char_idx": 20191, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "d88f337e-736e-4b55-bc11-3bdbd99a2641": {"__data__": {"id_": "d88f337e-736e-4b55-bc11-3bdbd99a2641", "embedding": null, "metadata": {"file_path": "docs/operator.md", "file_name": "operator.md"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "636d82cbe736f0e4b87074e2c4bac77becae2a28", "node_type": null, "metadata": {"file_path": "docs/operator.md", "file_name": "operator.md"}, "hash": "088b8b3573d8ad47eb5406f5c3ce360a661b9661e95b0a4c8ff24504e0be2726"}, "2": {"node_id": "c71dbe1d-3141-4b8c-a3a9-ebdf501354c5", "node_type": null, "metadata": {"file_path": "docs/operator.md", "file_name": "operator.md"}, "hash": "10ca2f84f79b3d50d22a875969a3a3565de25805d1abbcfc0edf2d6af8f632fb"}, "3": {"node_id": "5c02ba1a-6514-48fe-bffb-8d9085d8d849", "node_type": null, "metadata": {"file_path": "docs/operator.md", "file_name": "operator.md"}, "hash": "0b79ff2ad11944f9791e7e9923ba3bbca7f439e66fa17fd7ca878ca2790b1f97"}}, "hash": "eb691d340e9f1b44cf402985e123289c044ffde622446bcd7b69bb79d661f0b0", "text": "3, 4.5 )\n    .filter( Number )\n    .view()\n```\n\n```\n3\n4.5\n```\n\nFinally, a filtering condition can be defined by using any a boolean predicate. A predicate is expressed by a {ref}`closure <script-closure>` returning a boolean value. For example the following fragment shows how filter a channel emitting numbers so that the odd values are returned:\n\n```groovy\nChannel\n    .of( 1, 2, 3, 4, 5 )\n    .filter { it % 2 == 1 }\n    .view()\n```\n\n```\n1\n3\n5\n```\n\n:::{tip}\nIn the above example the filter condition is wrapped in curly brackets, instead of parentheses, because it specifies a {ref}`closure <script-closure>` as the operator's argument. In reality it is just syntactic sugar for `filter({ it % 2 == 1 })`\n:::\n\n(operator-first)=\n\n## first\n\n*Returns: value channel*\n\nThe `first` operator creates a channel that returns the first item emitted by the source channel, or eventually the first item that matches an optional condition. The condition can be specified by using a {ref}`regular expression<script-regexp>`, a Java `class` type or any boolean predicate. For example:\n\n```groovy\n// no condition is specified, emits the very first item: 1\nChannel\n    .of( 1, 2, 3 )\n    .first()\n    .view()\n\n// emits the first String value: 'a'\nChannel\n    .of( 1, 2, 'a', 'b', 3 )\n    .first( String )\n    .view()\n\n// emits the first item matching the regular expression: 'aa'\nChannel\n    .of( 'a', 'aa', 'aaa' )\n    .first( ~/aa.*/ )\n    .view()\n\n// emits the first item for which the predicate evaluates to true: 4\nChannel\n    .of( 1,2,3,4,5 )\n    .first { it > 3 }\n    .view()\n```\n\n(operator-flatmap)=\n\n## flatMap\n\n*Returns: queue channel*\n\nThe `flatMap` operator applies a function of your choosing to every item emitted by a channel, and returns the items so obtained as a new channel. Whereas the mapping function returns a list of items, this list is flattened so that each single item is emitted on its own.\n\nFor example:\n\n```groovy\n// create a channel of numbers\nnumbers = Channel.of( 1, 2, 3 )\n\n// map each number to a tuple (array), which items are emitted separately\nresults = numbers.flatMap { n -> [ n*2, n*3 ] }\n\n// print the final results\nresults.subscribe onNext: { println it }, onComplete: { println 'Done' }\n```\n\n```\n2\n3\n4\n6\n6\n9\nDone\n```\n\nAssociative arrays are handled in the same way, so that each array entry is emitted as a single key-value pair. For example:\n\n```groovy\nChannel\n    .of ( 1, 2, 3 )\n    .flatMap { it -> [ number: it, square: it*it ] }\n    .view { it.key + ': ' + it.value }\n```\n\n```\nnumber: 1\nsquare: 1\nnumber: 2\nsquare: 4\nnumber: 3\nsquare: 9\n```\n\n(operator-flatten)=\n\n## flatten\n\n*Returns: queue channel*\n\nThe `flatten` operator transforms a channel in such a way that every item of type `Collection` or `Array` is flattened so that each single entry is emitted separately by the resulting channel. For example:\n\n```groovy\nChannel\n    .of( [1,[2,3]], 4, [5,[6]] )\n   ", "start_char_idx": 20211, "end_char_idx": 23110, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "5c02ba1a-6514-48fe-bffb-8d9085d8d849": {"__data__": {"id_": "5c02ba1a-6514-48fe-bffb-8d9085d8d849", "embedding": null, "metadata": {"file_path": "docs/operator.md", "file_name": "operator.md"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "636d82cbe736f0e4b87074e2c4bac77becae2a28", "node_type": null, "metadata": {"file_path": "docs/operator.md", "file_name": "operator.md"}, "hash": "088b8b3573d8ad47eb5406f5c3ce360a661b9661e95b0a4c8ff24504e0be2726"}, "2": {"node_id": "d88f337e-736e-4b55-bc11-3bdbd99a2641", "node_type": null, "metadata": {"file_path": "docs/operator.md", "file_name": "operator.md"}, "hash": "eb691d340e9f1b44cf402985e123289c044ffde622446bcd7b69bb79d661f0b0"}, "3": {"node_id": "510939b7-eda6-4953-bfe8-678dffdafac7", "node_type": null, "metadata": {"file_path": "docs/operator.md", "file_name": "operator.md"}, "hash": "5da208af0a1cf2a294f0adf024f1889202b08625dafecd8a97dc9c902853dbbe"}}, "hash": "0b79ff2ad11944f9791e7e9923ba3bbca7f439e66fa17fd7ca878ca2790b1f97", "text": "[1,[2,3]], 4, [5,[6]] )\n    .flatten()\n    .view()\n```\n\n```\n1\n2\n3\n4\n5\n6\n```\n\nSee also: [flatMap](#flatmap) operator.\n\n(operator-grouptuple)=\n\n## groupTuple\n\n*Returns: queue channel*\n\nThe `groupTuple` operator collects tuples (or lists) of values emitted by the source channel grouping together the elements that share the same key. Finally it emits a new tuple object for each distinct key collected.\n\nIn other words, the operator transforms a sequence of tuple like *(K, V, W, ..)* into a new channel emitting a sequence of *(K, list(V), list(W), ..)*\n\nFor example:\n\n```groovy\nChannel\n    .of( [1, 'A'], [1, 'B'], [2, 'C'], [3, 'B'], [1, 'C'], [2, 'A'], [3, 'D'] )\n    .groupTuple()\n    .view()\n```\n\n```\n[1, [A, B, C]]\n[2, [C, A]]\n[3, [B, D]]\n```\n\nBy default the first entry in the tuple is used as grouping key. A different key can be chosen by using the `by` parameter and specifying the index of the entry to be used as key (the index is zero-based). For example, grouping by the second value in each tuple:\n\n```groovy\nChannel\n    .of( [1, 'A'], [1, 'B'], [2, 'C'], [3, 'B'], [1, 'C'], [2, 'A'], [3, 'D'] )\n    .groupTuple(by: 1)\n    .view()\n```\n\n```\n[[1, 2], A]\n[[1, 3], B]\n[[2, 1], C]\n[[3], D]\n```\n\nBy default, if you don't specify a size, the `groupTuple` operator will not emit any groups until *all* inputs have been received. If possible, you should always try to specify the number of expected elements in each group using the `size` option, so that each group can be emitted as soon as it's ready. In cases where the size of each group varies based on the grouping key, you can use the built-in `groupKey` function, which allows you to create a special grouping key with an associated size:\n\n```groovy\nchr_frequency = [ \"chr1\": 2, \"chr2\": 3 ]\n\nChannel.of(\n        [ 'region1', 'chr1', '/path/to/region1_chr1.vcf' ],\n        [ 'region2', 'chr1', '/path/to/region2_chr1.vcf' ],\n        [ 'region1', 'chr2', '/path/to/region1_chr2.vcf' ],\n        [ 'region2', 'chr2', '/path/to/region2_chr2.vcf' ],\n        [ 'region3', 'chr2', '/path/to/region3_chr2.vcf' ]\n    )\n    .map { region, chr, vcf -> tuple( groupKey(chr, chr_frequency[chr]), vcf ) }\n    .groupTuple()\n    .view()\n```\n\n```\n[chr1, [/path/to/region1_chr1.vcf, /path/to/region2_chr1.vcf]]\n[chr2, [/path/to/region1_chr2.vcf, /path/to/region2_chr2.vcf, /path/to/region3_chr2.vcf]]\n```\n\nAvailable", "start_char_idx": 23113, "end_char_idx": 25473, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "510939b7-eda6-4953-bfe8-678dffdafac7": {"__data__": {"id_": "510939b7-eda6-4953-bfe8-678dffdafac7", "embedding": null, "metadata": {"file_path": "docs/operator.md", "file_name": "operator.md"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "636d82cbe736f0e4b87074e2c4bac77becae2a28", "node_type": null, "metadata": {"file_path": "docs/operator.md", "file_name": "operator.md"}, "hash": "088b8b3573d8ad47eb5406f5c3ce360a661b9661e95b0a4c8ff24504e0be2726"}, "2": {"node_id": "5c02ba1a-6514-48fe-bffb-8d9085d8d849", "node_type": null, "metadata": {"file_path": "docs/operator.md", "file_name": "operator.md"}, "hash": "0b79ff2ad11944f9791e7e9923ba3bbca7f439e66fa17fd7ca878ca2790b1f97"}, "3": {"node_id": "3e7c21df-7e8b-473c-90f7-fed1cb0d2511", "node_type": null, "metadata": {"file_path": "docs/operator.md", "file_name": "operator.md"}, "hash": "f32a198d059a2a8053450e0e3829613867a42f07c2ddcc1c5d6cc579d095acc0"}}, "hash": "5da208af0a1cf2a294f0adf024f1889202b08625dafecd8a97dc9c902853dbbe", "text": "options:\n\n`by`\n: The index (zero based) of the element to be used as grouping key. A key composed by multiple elements can be defined specifying a list of indices e.g. `by: [0,2]`\n\n`remainder`\n: When `false` incomplete tuples (i.e. with less than `size` grouped items) are discarded (default). When `true` incomplete tuples are emitted as the ending emission. Only valid when a `size` parameter is specified.\n\n`size`\n: The number of items the grouped list(s) has to contain. When the specified size is reached, the tuple is emitted.\n\n`sort`\n: Defines the sorting criteria for the grouped items. Can be one of the following values:\n\n  - `false`: No sorting is applied (default).\n  - `true`: Order the grouped items by the item's natural ordering i.e. numerical for number, lexicographic for string, etc. See the [Java documentation](http://docs.oracle.com/javase/tutorial/collections/interfaces/order.html) for more information.\n  - `hash`: Order the grouped items by the hash number associated to each entry.\n  - `deep`: Similar to the previous, but the hash number is created on actual entries content e.g. when the item is a file, the hash is created on the actual file content.\n  - A custom sorting criteria used to order the tuples element holding list of values. It can be specified by using either a {ref}`Closure <script-closure>` or a [Comparator](http://docs.oracle.com/javase/7/docs/api/java/util/Comparator.html) object.\n\n(operator-ifempty)=\n\n## ifEmpty\n\n*Returns: value channel*\n\nThe `ifEmpty` operator creates a channel which emits a default value, specified as the operator parameter, when the channel to which is applied is *empty* i.e. doesn't emit any value. Otherwise it will emit the same sequence of entries as the original channel.\n\nThus, the following example prints:\n\n```groovy\nChannel .of(1,2,3) .ifEmpty('Hello') .view()\n```\n\n```\n1\n2\n3\n```\n\nInstead, this one prints:\n\n```groovy\nChannel .empty() .ifEmpty('Hello') .view()\n```\n\n```\nHello\n```\n\nThe `ifEmpty` value parameter can be defined with a {ref}`closure <script-closure>`. In this case the result value of the closure evaluation will be emitted when the empty condition is satisfied.\n\nSee also: {ref}`channel-empty` method.\n\n(operator-join)=\n\n## join\n\n*Returns: queue channel*\n\nThe `join` operator creates a channel that joins together the items emitted by two channels for which exists a matching key. The key is defined, by default, as the first element in each item emitted.\n\nFor example:\n\n```groovy\nleft  = Channel.of(['X', 1], ['Y', 2], ['Z', 3], ['P', 7])\nright = Channel.of(['Z', 6], ['Y', 5], ['X', 4])\nleft.join(right).view()\n```\n\n```\n[Z, 3, 6]\n[Y, 2, 5]\n[X, 1, 4]\n```\n\nThe `index` of a different matching element can be specified by using the `by` parameter.\n\nThe `join` operator can emit all the pairs that are incomplete, i.e. the items for which a matching element is missing, by specifying the optional parameter `remainder` as shown below:\n\n```groovy\nleft  = Channel.of(['X', 1], ['Y', 2], ['Z', 3], ['P', 7])\nright = Channel.of(['Z', 6], ['Y', 5], ['X', 4])\nleft.join(right, remainder: true).view()\n```\n\n```\n[Y, 2, 5]\n[Z, 3,", "start_char_idx": 25495, "end_char_idx": 28613, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "3e7c21df-7e8b-473c-90f7-fed1cb0d2511": {"__data__": {"id_": "3e7c21df-7e8b-473c-90f7-fed1cb0d2511", "embedding": null, "metadata": {"file_path": "docs/operator.md", "file_name": "operator.md"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "636d82cbe736f0e4b87074e2c4bac77becae2a28", "node_type": null, "metadata": {"file_path": "docs/operator.md", "file_name": "operator.md"}, "hash": "088b8b3573d8ad47eb5406f5c3ce360a661b9661e95b0a4c8ff24504e0be2726"}, "2": {"node_id": "510939b7-eda6-4953-bfe8-678dffdafac7", "node_type": null, "metadata": {"file_path": "docs/operator.md", "file_name": "operator.md"}, "hash": "5da208af0a1cf2a294f0adf024f1889202b08625dafecd8a97dc9c902853dbbe"}, "3": {"node_id": "c2785032-514c-4f06-b894-c367e44a06cd", "node_type": null, "metadata": {"file_path": "docs/operator.md", "file_name": "operator.md"}, "hash": "a44caef1dcad3a5f9e07a2fe1696d6bdc69e5bb324933ecd319ec4926000f9a2"}}, "hash": "f32a198d059a2a8053450e0e3829613867a42f07c2ddcc1c5d6cc579d095acc0", "text": "2, 5]\n[Z, 3, 6]\n[X, 1, 4]\n[P, 7, null]\n```\n\nAvailable options:\n\n`by`\n: The index (zero based) of the element to be used as grouping key. A key composed by multiple elements can be defined specifying a list of indices e.g. `by: [0,2]`.\n\n`failOnDuplicate`\n: An error is reported when the same key is found more than once.\n\n`failOnMismatch`\n: An error is reported when a channel emits a value for which there isn't a corresponding element in the joining channel. This option cannot be used with `remainder`.\n\n`remainder`\n: When `false` incomplete tuples (i.e. with less than `size` grouped items) are discarded (default). When `true` incomplete tuples are emitted as the ending emission.\n\n(operator-last)=\n\n## last\n\n*Returns: value channel*\n\nThe `last` operator creates a channel that only returns the last item emitted by the source channel. For example:\n\n```groovy\nChannel\n    .of( 1,2,3,4,5,6 )\n    .last()\n    .view()\n```\n\n```\n6\n```\n\n(operator-map)=\n\n## map\n\n*Returns: queue channel*\n\nThe `map` operator applies a function of your choosing to every item emitted by a channel, and returns the items so obtained as a new channel. The function applied is called the mapping function and is expressed with a {ref}`closure <script-closure>` as shown in the example below:\n\n```groovy\nChannel\n    .of( 1, 2, 3, 4, 5 )\n    .map { it * it }\n    .subscribe onNext: { println it }, onComplete: { println 'Done' }\n```\n\n```\n1\n4\n9\n16\n25\nDone\n```\n\n(operator-max)=\n\n## max\n\n*Returns: value channel*\n\nThe `max` operator waits until the source channel completes, and then emits the item that has the greatest value. For example:\n\n```groovy\nChannel\n    .of( 8, 6, 2, 5 )\n    .max()\n    .view { \"Max value is $it\" }\n```\n\n```\nMax value is 8\n```\n\nAn optional {ref}`closure <script-closure>` parameter can be specified in order to provide a function that returns the value to be compared. The example below shows how to find the string item that has the maximum length:\n\n```groovy\nChannel\n    .of(\"hello\",\"hi\",\"hey\")\n    .max { it.size() }\n    .view()\n```\n\n```\n\"hello\"\n```\n\nAlternatively it is possible to specify a comparator function i.e. a {ref}`closure <script-closure>` taking two parameters that represent two emitted items to be compared. For example:\n\n```groovy\nChannel\n    .of(\"hello\",\"hi\",\"hey\")\n    .max { a,b -> a.size() <=> b.size() }\n    .view()\n```\n\n(operator-merge)=\n\n## merge\n\n*Returns: queue channel*\n\nThe `merge` operator lets you join items emitted by two (or more) channels into a new channel.\n\nFor example, the following code merges two channels together: one which emits a series of odd integers and the other which emits a series of even integers:\n\n```groovy\nodds  = Channel.of(1, 3, 5, 7, 9)\nevens = Channel.of(2, 4, 6)\n\nodds\n    .merge( evens )\n    .view()\n```\n\n```\n[1, 2]\n[3, 4]\n[5, 6]\n```\n\nAn optional closure can be provided to customise the items emitted by", "start_char_idx": 28604, "end_char_idx": 31469, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "c2785032-514c-4f06-b894-c367e44a06cd": {"__data__": {"id_": "c2785032-514c-4f06-b894-c367e44a06cd", "embedding": null, "metadata": {"file_path": "docs/operator.md", "file_name": "operator.md"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "636d82cbe736f0e4b87074e2c4bac77becae2a28", "node_type": null, "metadata": {"file_path": "docs/operator.md", "file_name": "operator.md"}, "hash": "088b8b3573d8ad47eb5406f5c3ce360a661b9661e95b0a4c8ff24504e0be2726"}, "2": {"node_id": "3e7c21df-7e8b-473c-90f7-fed1cb0d2511", "node_type": null, "metadata": {"file_path": "docs/operator.md", "file_name": "operator.md"}, "hash": "f32a198d059a2a8053450e0e3829613867a42f07c2ddcc1c5d6cc579d095acc0"}, "3": {"node_id": "86a518cd-a160-4b6e-a600-3e8e04e7bbf6", "node_type": null, "metadata": {"file_path": "docs/operator.md", "file_name": "operator.md"}, "hash": "7f679fee5141a5c95d43893549e64667ea6dc2a4cbd58457910b27d8cc9c8e8e"}}, "hash": "a44caef1dcad3a5f9e07a2fe1696d6bdc69e5bb324933ecd319ec4926000f9a2", "text": "optional closure can be provided to customise the items emitted by the resulting merged channel. For example:\n\n```groovy\nodds  = Channel.of(1, 3, 5, 7, 9)\nevens = Channel.of(2, 4, 6)\n\nodds\n    .merge( evens ) { a, b -> tuple(b*b, a) }\n    .view()\n```\n\n:::{danger}\nIn general, the use of the `merge` operator is discouraged. Processes and channel operators are not guaranteed to emit items in the order that they were received, due to their parallel and asynchronous nature. Therefore, if you try to merge output channels from different processes, the resulting channel may be different on each run, which will cause resumed runs to not work properly.\n\nYou should always use a matching key (e.g. sample ID) to merge multiple channels, so that they are combined in a deterministic way. For this purpose, you can use the [join](#join) operator.\n:::\n\n(operator-min)=\n\n## min\n\n*Returns: value channel*\n\nThe `min` operator waits until the source channel completes, and then emits the item that has the lowest value. For example:\n\n```groovy\nChannel\n    .of( 8, 6, 2, 5 )\n    .min()\n    .view { \"Min value is $it\" }\n```\n\n```\nMin value is 2\n```\n\nAn optional {ref}`closure <script-closure>` parameter can be specified in order to provide a function that returns the value to be compared. The example below shows how to find the string item that has the minimum length:\n\n```groovy\nChannel\n    .of(\"hello\",\"hi\",\"hey\")\n    .min { it.size() }\n    .view()\n```\n\n```\n\"hi\"\n```\n\nAlternatively it is possible to specify a comparator function i.e. a {ref}`closure <script-closure>` taking two parameters that represent two emitted items to be compared. For example:\n\n```groovy\nChannel\n    .of(\"hello\",\"hi\",\"hey\")\n    .min { a,b -> a.size() <=> b.size() }\n    .view()\n```\n\n(operator-mix)=\n\n## mix\n\n*Returns: queue channel*\n\nThe `mix` operator combines the items emitted by two (or more) channels into a single channel.\n\nFor example:\n\n```groovy\nc1 = Channel.of( 1, 2, 3 )\nc2 = Channel.of( 'a', 'b' )\nc3 = Channel.of( 'z' )\n\nc1.mix(c2,c3)\n    .subscribe onNext: { println it }, onComplete: { println 'Done' }\n```\n\n```\n1\n2\n3\n'a'\n'b'\n'z'\n```\n\n:::{note}\nThe items emitted by the resulting mixed channel may appear in any order, regardless of which source channel they came from. Thus, the following example could also be a possible result of the above example:\n\n```\n'z'\n1\n'a'\n2\n'b'\n3\n```\n:::\n\n(operator-multimap)=\n\n## multiMap\n\n:::{versionadded} 19.11.0-edge\n:::\n\n*Returns: map of queue channels*\n\nThe `multiMap` operator allows you to forward the items emitted by a source channel to two or more output channels, mapping each input value as a separate element.\n\nThe mapping criteria is defined with a {ref}`closure <script-closure>` that specifies the target channels (labelled with a unique identifier) followed by an expression that maps each item from the input channel to the target channel.\n\nFor example:\n\n```groovy\nChannel.of(1, 2, 3, 4)\n    .multiMap { it ->\n        foo: it + 1\n", "start_char_idx": 31424, "end_char_idx": 34384, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "86a518cd-a160-4b6e-a600-3e8e04e7bbf6": {"__data__": {"id_": "86a518cd-a160-4b6e-a600-3e8e04e7bbf6", "embedding": null, "metadata": {"file_path": "docs/operator.md", "file_name": "operator.md"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "636d82cbe736f0e4b87074e2c4bac77becae2a28", "node_type": null, "metadata": {"file_path": "docs/operator.md", "file_name": "operator.md"}, "hash": "088b8b3573d8ad47eb5406f5c3ce360a661b9661e95b0a4c8ff24504e0be2726"}, "2": {"node_id": "c2785032-514c-4f06-b894-c367e44a06cd", "node_type": null, "metadata": {"file_path": "docs/operator.md", "file_name": "operator.md"}, "hash": "a44caef1dcad3a5f9e07a2fe1696d6bdc69e5bb324933ecd319ec4926000f9a2"}, "3": {"node_id": "251985eb-6011-43d3-ad55-53a3d4c7ee30", "node_type": null, "metadata": {"file_path": "docs/operator.md", "file_name": "operator.md"}, "hash": "04aa6c495b9b15985cb26255292e1687419679db4c7d66715c730a106da84ac0"}}, "hash": "7f679fee5141a5c95d43893549e64667ea6dc2a4cbd58457910b27d8cc9c8e8e", "text": ".multiMap { it ->\n        foo: it + 1\n        bar: it * it\n    }\n    .set { result }\n\nresult.foo.view { \"foo $it\" }\nresult.bar.view { \"bar $it\" }\n```\n\n```\nfoo 2\nfoo 3\nfoo 4\nfoo 5\nbar 1\nbar 4\nbar 9\nbar 16\n```\n\nThe mapping expression can be omitted when the value to be emitted is the same as the following one. If you just need to forward the same value to multiple channels, you can use the following shorthand:\n\n```groovy\nChannel\n    .of(1,2,3)\n    .multiMap { it -> foo: bar: it }\n    .set { result }\n```\n\nAs before, this creates two channels, but now both of them receive the same source items.\n\nYou can use the `multiMapCriteria` method to create a multi-map criteria as a variable that can be passed as an argument to one or more `multiMap` operations, as shown below:\n\n```groovy\ndef criteria = multiMapCriteria {\n    small: it < 10\n    large: it > 10\n}\n\nChannel.of(1, 2, 30).multiMap(criteria).set { ch1 }\nChannel.of(10, 20, 1).multiMap(criteria).set { ch2 }\n```\n\n:::{note}\nIf you use `multiMap` to split a tuple or map into multiple channels, it is recommended that you retain a matching key (e.g. sample ID) with *each* new channel, so that you can re-combine these channels later on if needed. In general, you should not expect to be able to merge channels correctly without a matching key, due to the parallel and asynchronous nature of Nextflow pipelines.\n:::\n\n(operator-randomsample)=\n\n## randomSample\n\n*Returns: queue channel*\n\nThe `randomSample` operator allows you to create a channel emitting the specified number of items randomly taken from the channel to which is applied. For example:\n\n```groovy\nChannel\n    .of( 1..100 )\n    .randomSample( 10 )\n    .view()\n```\n\nThe above snippet will print 10 numbers in the range from 1 to 100.\n\nThe operator supports a second parameter that allows you to set the initial `seed` for the random number generator. By setting it, the `randomSample` operator will always return the same pseudo-random sequence. For example:\n\n```groovy\nChannel\n    .of( 1..100 )\n    .randomSample( 10, 234 )\n    .view()\n```\n\nThe above example will print 10 random numbers in the range between 1 and 100. At each run of the script, the same sequence will be returned.\n\n(operator-reduce)=\n\n## reduce\n\n*Returns: value channel*\n\nThe `reduce` operator applies a function of your choosing to every item emitted by a channel. Each time this function is invoked it takes two parameters: the accumulated value and the *i-th* emitted item. The result is passed as the accumulated value to the next function call, along with the *i+1 th* item, until all the items are processed.\n\nFinally, the `reduce` operator emits the result of the last invocation of your function as the sole output.\n\nFor example:\n\n```groovy\nChannel\n    .of( 1, 2, 3, 4, 5 )\n    .reduce { a, b -> println \"a: $a b: $b\"; return a+b }\n    .view { \"result = $it\" }\n```\n\n```\na: 1 b: 2\na: 3 b: 3\na: 6 b: 4\na: 10 b: 5\nresult = 15\n```\n\n:::{tip}\nA common use case for this operator is to use the first parameter as an accumulator and the second parameter as the", "start_char_idx": 34417, "end_char_idx": 37464, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "251985eb-6011-43d3-ad55-53a3d4c7ee30": {"__data__": {"id_": "251985eb-6011-43d3-ad55-53a3d4c7ee30", "embedding": null, "metadata": {"file_path": "docs/operator.md", "file_name": "operator.md"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "636d82cbe736f0e4b87074e2c4bac77becae2a28", "node_type": null, "metadata": {"file_path": "docs/operator.md", "file_name": "operator.md"}, "hash": "088b8b3573d8ad47eb5406f5c3ce360a661b9661e95b0a4c8ff24504e0be2726"}, "2": {"node_id": "86a518cd-a160-4b6e-a600-3e8e04e7bbf6", "node_type": null, "metadata": {"file_path": "docs/operator.md", "file_name": "operator.md"}, "hash": "7f679fee5141a5c95d43893549e64667ea6dc2a4cbd58457910b27d8cc9c8e8e"}, "3": {"node_id": "60b68069-0cc2-495f-a9f5-add32930ef46", "node_type": null, "metadata": {"file_path": "docs/operator.md", "file_name": "operator.md"}, "hash": "38f066063646ff7fc82b687dc9ebc7fa812fbf855e72c1d712c558229c3d802d"}}, "hash": "04aa6c495b9b15985cb26255292e1687419679db4c7d66715c730a106da84ac0", "text": "operator is to use the first parameter as an accumulator and the second parameter as the `i-th` item to be processed.\n:::\n\nOptionally you can specify an initial value for the accumulator as shown below:\n\n```groovy\nmyChannel.reduce( initialValue ) { a, b -> ... }\n```\n\n(operator-set)=\n\n## set\n\n*Returns: nothing*\n\nThe `set` operator assigns the channel to a variable whose name is specified as a closure parameter. For example:\n\n```groovy\nChannel.of(10, 20, 30).set { my_channel }\n```\n\nThis is semantically equivalent to the following assignment:\n\n```groovy\nmy_channel = Channel.of(10, 20, 30)\n```\n\nHowever the `set` operator is more idiomatic in Nextflow scripting, since it can be used at the end of a chain of operator transformations, thus resulting in a more fluent and readable operation.\n\n(operator-splitcsv)=\n\n## splitCsv\n\n*Returns: queue channel*\n\nThe `splitCsv` operator allows you to parse text items emitted by a channel, that are formatted using the [CSV format](http://en.wikipedia.org/wiki/Comma-separated_values), and split them into records or group them into list of records with a specified length.\n\nIn the simplest case just apply the `splitCsv` operator to a channel emitting a CSV formatted text files or text entries. For example:\n\n```groovy\nChannel\n    .of( 'alpha,beta,gamma\\n10,20,30\\n70,80,90' )\n    .splitCsv()\n    .view { row -> \"${row[0]} - ${row[1]} - ${row[2]}\" }\n```\n\nThe above example shows hows CSV text is parsed and is split into single rows. Values can be accessed by its column index in the row object.\n\nWhen the CSV begins with a header line defining the column names, you can specify the parameter `header: true` which allows you to reference each value by its name, as shown in the following example:\n\n```groovy\nChannel\n    .of( 'alpha,beta,gamma\\n10,20,30\\n70,80,90' )\n    .splitCsv(header: true)\n    .view { row -> \"${row.alpha} - ${row.beta} - ${row.gamma}\" }\n```\n\nIt will print\n\n```\n10 - 20 - 30\n70 - 80 - 90\n```\n\nAlternatively you can provide custom header names by specifying a the list of strings in the `header` parameter as shown below:\n\n```groovy\nChannel\n    .of( 'alpha,beta,gamma\\n10,20,30\\n70,80,90' )\n    .splitCsv(header: ['col1', 'col2', 'col3'], skip: 1 )\n    .view { row -> \"${row.col1} - ${row.col2} - ${row.col3}\" }\n```\n\nAvailable options:\n\n`by`\n: The number of rows in each `chunk`\n\n`charset`\n: Parse the content by using the specified charset e.g. `UTF-8`\n\n`decompress`\n: When `true` decompress the content using the GZIP format before processing it (note: files whose name ends with `.gz` extension are decompressed automatically)\n\n`elem`\n: The index of the element to split when the operator is applied to a channel emitting list/tuple objects (default: first file object or first element)\n\n`header`\n: When `true` the first line is used as columns names. Alternatively it can be used to provide the list of columns names.\n\n`limit`\n: Limits the number of retrieved records for each file to the specified value.\n\n`quote`\n: Values may be quoted by single or double quote characters.\n\n`sep`\n: The character used to separate the values (default:", "start_char_idx": 37416, "end_char_idx": 40521, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "60b68069-0cc2-495f-a9f5-add32930ef46": {"__data__": {"id_": "60b68069-0cc2-495f-a9f5-add32930ef46", "embedding": null, "metadata": {"file_path": "docs/operator.md", "file_name": "operator.md"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "636d82cbe736f0e4b87074e2c4bac77becae2a28", "node_type": null, "metadata": {"file_path": "docs/operator.md", "file_name": "operator.md"}, "hash": "088b8b3573d8ad47eb5406f5c3ce360a661b9661e95b0a4c8ff24504e0be2726"}, "2": {"node_id": "251985eb-6011-43d3-ad55-53a3d4c7ee30", "node_type": null, "metadata": {"file_path": "docs/operator.md", "file_name": "operator.md"}, "hash": "04aa6c495b9b15985cb26255292e1687419679db4c7d66715c730a106da84ac0"}, "3": {"node_id": "3df84d5b-5623-4ffb-bc05-0bbdf371eefd", "node_type": null, "metadata": {"file_path": "docs/operator.md", "file_name": "operator.md"}, "hash": "fdfe9f07907c3140c9160a778b409befa4f3a4ce1d841caac9f4ad9111dba251"}}, "hash": "38f066063646ff7fc82b687dc9ebc7fa812fbf855e72c1d712c558229c3d802d", "text": "The character used to separate the values (default: `,`)\n\n`skip`\n: Number of lines since the file beginning to ignore when parsing the CSV content.\n\n`strip`\n: Removes leading and trailing blanks from values (default: `false`)\n\n(operator-splitfasta)=\n\n## splitFasta\n\n*Returns: queue channel*\n\nThe `splitFasta` operator allows you to split the entries emitted by a channel, that are formatted using the [FASTA format](http://en.wikipedia.org/wiki/FASTA_format). It returns a channel which emits text item for each sequence in the received FASTA content.\n\nThe number of sequences in each text chunk produced by the `splitFasta` operator can be set by using the `by` parameter. The following example shows how to read a FASTA file and split it into chunks containing 10 sequences each:\n\n```groovy\nChannel\n     .fromPath('misc/sample.fa')\n     .splitFasta( by: 10 )\n     .view()\n```\n\n:::{warning}\nChunks are stored in memory by default. When splitting large files, specify the parameter `file: true` to save the chunks into files in order to avoid an `OutOfMemoryException`. See the parameter table below for details.\n:::\n\nA second version of the `splitFasta` operator allows you to split a FASTA content into record objects, instead of text chunks. A record object contains a set of fields that let you access and manipulate the FASTA sequence information with ease.\n\nIn order to split a FASTA content into record objects, simply use the `record` parameter specifying the map of required the fields, as shown in the example below:\n\n```groovy\nChannel\n     .fromPath('misc/sample.fa')\n     .splitFasta( record: [id: true, seqString: true ])\n     .filter { record -> record.id =~ /^ENST0.*/ }\n     .view { record -> record.seqString }\n```\n\nIn this example, the file `misc/sample.fa` is split into records containing the `id` and the `seqString` fields (i.e. the sequence id and the sequence data). The following `filter` operator only keeps the sequences whose ID starts with the `ENST0` prefix, finally the sequence content is printed by using the `subscribe` operator.\n\nAvailable options:\n\n`by`\n: Defines the number of sequences in each `chunk` (default: `1`)\n\n`charset`\n: Parse the content by using the specified charset e.g. `UTF-8`.\n\n`compress`\n: When `true` resulting file chunks are GZIP compressed. The `.gz` suffix is automatically added to chunk file names.\n\n`decompress`\n: When `true`, decompress the content using the GZIP format before processing it (note: files whose name ends with `.gz` extension are decompressed automatically).\n\n`elem`\n: The index of the element to split when the operator is applied to a channel emitting list/tuple objects (default: first file object or first element).\n\n`file`\n: When `true` saves each split to a file. Use a string instead of `true` value to create split files with a specific name (split index number is automatically added). Finally, set this attribute to an existing directory, in order to save the split files into the specified folder.\n\n`limit`\n: Limits the number of retrieved sequences for each file to the specified value.\n\n`record`\n: Parse each entry in the FASTA file as record objects. The following fields are available:\n\n  - `id`: The FASTA sequence identifier i.e. the word following the `>` symbol up to the first `blank` or `newline` character\n  - `header`: The first line in a FASTA sequence without the `>` character\n  - `desc`: The text in the FASTA header following the ID value\n  - `text`: The complete FASTA sequence", "start_char_idx": 40551, "end_char_idx": 44037, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "3df84d5b-5623-4ffb-bc05-0bbdf371eefd": {"__data__": {"id_": "3df84d5b-5623-4ffb-bc05-0bbdf371eefd", "embedding": null, "metadata": {"file_path": "docs/operator.md", "file_name": "operator.md"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "636d82cbe736f0e4b87074e2c4bac77becae2a28", "node_type": null, "metadata": {"file_path": "docs/operator.md", "file_name": "operator.md"}, "hash": "088b8b3573d8ad47eb5406f5c3ce360a661b9661e95b0a4c8ff24504e0be2726"}, "2": {"node_id": "60b68069-0cc2-495f-a9f5-add32930ef46", "node_type": null, "metadata": {"file_path": "docs/operator.md", "file_name": "operator.md"}, "hash": "38f066063646ff7fc82b687dc9ebc7fa812fbf855e72c1d712c558229c3d802d"}, "3": {"node_id": "65ce763f-c431-435e-9657-0b6f2d5fc9c4", "node_type": null, "metadata": {"file_path": "docs/operator.md", "file_name": "operator.md"}, "hash": "48d6a25b39e6f92b3b5956c9d459780e69f3c92c57da741d80935b86353dad3d"}}, "hash": "fdfe9f07907c3140c9160a778b409befa4f3a4ce1d841caac9f4ad9111dba251", "text": "header following the ID value\n  - `text`: The complete FASTA sequence including the header\n  - `seqString`: The sequence data as a single line string i.e. containing no `newline` characters\n  - `sequence`: The sequence data as a multi-line string (always ending with a `newline` character)\n  - `width`: Define the length of a single line when the `sequence` field is used, after that the sequence data continues on a new line.\n\n`size`\n: Defines the size in memory units of the expected chunks e.g. `1.MB`.\n\nSee also: [countFasta](#countfasta)\n\n(operator-splitfastq)=\n\n## splitFastq\n\n*Returns: queue channel*\n\nThe `splitFastq` operator allows you to split the entries emitted by a channel, that are formatted using the [FASTQ format](http://en.wikipedia.org/wiki/FASTQ_format). It returns a channel which emits a text chunk for each sequence in the received item.\n\nThe number of sequences in each text chunk produced by the `splitFastq` operator is defined by the parameter `by`. The following example shows you how to read a FASTQ file and split it into chunks containing 10 sequences each:\n\n```groovy\nChannel\n    .fromPath('misc/sample.fastq')\n    .splitFastq( by: 10 )\n    .view()\n```\n\n:::{warning}\nChunks are stored in memory by default. When splitting large files, specify the parameter `file: true` to save the chunks into files in order to avoid an `OutOfMemoryException`. See the parameter table below for details.\n:::\n\nA second version of the `splitFastq` operator allows you to split a FASTQ formatted content into record objects, instead of text chunks. A record object contains a set of fields that let you access and manipulate the FASTQ sequence data with ease.\n\nIn order to split FASTQ sequences into record objects simply use the `record` parameter specifying the map of the required fields, or just specify `record: true` as in the example shown below:\n\n```groovy\nChannel\n    .fromPath('misc/sample.fastq')\n    .splitFastq( record: true )\n    .view { record -> record.readHeader }\n```\n\nFinally the `splitFastq` operator is able to split paired-end read pair FASTQ files. It must be applied to a channel which emits tuples containing at least two elements that are the files to be split. For example:\n\n```groovy\nChannel\n    .fromFilePairs('/my/data/SRR*_{1,2}.fastq', flat: true)\n    .splitFastq(by: 100_000, pe: true, file: true)\n    .view()\n```\n\n:::{note}\nThe `fromFilePairs` requires the `flat: true` option in order to emit the file pairs as separate elements in the produced tuples.\n:::\n\n:::{note}\nThis operator assumes that the order of the paired-end reads correspond with each other and both files contain the same number of reads.\n:::\n\nAvailable options:\n\n`by`\n: Defines the number of *reads* in each `chunk` (default: `1`)\n\n`charset`\n: Parse the content by using the specified charset e.g. `UTF-8`\n\n`compress`\n: When `true` resulting file chunks are GZIP compressed. The `.gz` suffix is automatically added to chunk file names.\n\n`decompress`\n: When `true` decompress the content using the GZIP format before processing it (note: files whose name ends with `.gz` extension are decompressed automatically)\n\n`elem`\n: The index of the element to split when the operator is applied to a channel emitting list/tuple objects (default: first file object or first element)\n\n`file`\n: When `true` saves each split to a file. Use a string instead of", "start_char_idx": 44024, "end_char_idx": 47386, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "65ce763f-c431-435e-9657-0b6f2d5fc9c4": {"__data__": {"id_": "65ce763f-c431-435e-9657-0b6f2d5fc9c4", "embedding": null, "metadata": {"file_path": "docs/operator.md", "file_name": "operator.md"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "636d82cbe736f0e4b87074e2c4bac77becae2a28", "node_type": null, "metadata": {"file_path": "docs/operator.md", "file_name": "operator.md"}, "hash": "088b8b3573d8ad47eb5406f5c3ce360a661b9661e95b0a4c8ff24504e0be2726"}, "2": {"node_id": "3df84d5b-5623-4ffb-bc05-0bbdf371eefd", "node_type": null, "metadata": {"file_path": "docs/operator.md", "file_name": "operator.md"}, "hash": "fdfe9f07907c3140c9160a778b409befa4f3a4ce1d841caac9f4ad9111dba251"}, "3": {"node_id": "df5ea1f3-e42e-4bb5-a8b3-e3a101ead26b", "node_type": null, "metadata": {"file_path": "docs/operator.md", "file_name": "operator.md"}, "hash": "0b8d8186d65e2b1b16fec14c7d5307ce5400a77fc9da470e56fd13a77f4971d4"}}, "hash": "48d6a25b39e6f92b3b5956c9d459780e69f3c92c57da741d80935b86353dad3d", "text": "When `true` saves each split to a file. Use a string instead of `true` value to create split files with a specific name (split index number is automatically added). Finally, set this attribute to an existing directory, in order to save the split files into the specified folder.\n\n`limit`\n: Limits the number of retrieved *reads* for each file to the specified value.\n\n`pe`\n: When `true` splits paired-end read files, therefore items emitted by the source channel must be tuples in which at least two elements are the read-pair files to be split.\n\n`record`\n: Parse each entry in the FASTQ file as record objects. The following fields are available:\n\n  - `readHeader`: Sequence header (without the `@` prefix)\n  - `readString`: The raw sequence data\n  - `qualityHeader`: Base quality header (it may be empty)\n  - `qualityString`: Quality values for the sequence\n\nSee also: [countFastq](#countfastq)\n\n(operator-splitjson)=\n\n## splitJson\n\n*Returns: queue channel*\n\nThe `splitJson` operator allows you to split a JSON document from a source channel into individual records. If the document is a JSON array, each element of the array will be emitted. If the document is a JSON object, each key-value pair will be emitted as a map with the properties `key`  and `value`.\n\nAn example with a JSON array:\n\n```groovy\nChannel.of('[1,null,[\"A\",{}],true]')\n    .splitJson()\n    .view{\"Item: ${it}\"}\n```\n\nProduces the following output:\n\n```\nItem: 1\nItem: null\nItem: [A, [:]]\nItem: true\n```\n\nAn example with a JSON object:\n\n```groovy\nChannel.of('{\"A\":1,\"B\":[1,2,3],\"C\":{\"D\":null}}')\n    .splitJson()\n    .view{\"Item: ${it}\"}\n```\n\nProduces the following output:\n\n```\nItem: [value:1, key:A]\nItem: [value:[1, 2, 3], key:B]\nItem: [value:[D:null], key:C]\n```\n\nYou can optionally query a section of the JSON document to parse and split, using the `path` option:\n\n```groovy\nChannel.of('{\"A\":1,\"B\":[2,3,{\"C\":{\"D\":null,\"E\":4,\"F\":5}}]}')\n    .splitJson(path: \"B[2].C\")\n    .view{\"Item: ${it}\"}\n```\n\nProduces the following output:\n\n```\nItem: [value:null, key:D]\nItem: [value:4, key:E]\nItem: [value:5, key:F]\n```\n\nAvailable options:\n\n`limit`\n: Limits the number of retrieved lines for each file to the specified value.\n\n`path`\n: Define the section of the JSON document that you want to extract. The expression is a set of paths separated by a dot, similar to [JSONPath](https://goessner.net/articles/JsonPath/). The empty string is the document root (default). An integer in brackets is the 0-based index in a JSON array. A string preceded by a dot `.` is the key in a JSON object.\n\nSee also: [countJson](#countjson)\n\n(operator-splittext)=\n\n## splitText\n\n*Returns: queue channel*\n\nThe `splitText` operator allows you to split multi-line strings or text file items, emitted by a source channel into chunks containing `n` lines, which will be emitted by the resulting channel.\n\nFor example:\n\n```groovy\nChannel\n    .fromPath('/some/path/*.txt')\n    .splitText()\n    .view()\n```\n\nIt splits the content of the files with suffix `.txt`, and prints it line by line.\n\nBy default the `splitText` operator splits each item into chunks", "start_char_idx": 47394, "end_char_idx": 50490, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "df5ea1f3-e42e-4bb5-a8b3-e3a101ead26b": {"__data__": {"id_": "df5ea1f3-e42e-4bb5-a8b3-e3a101ead26b", "embedding": null, "metadata": {"file_path": "docs/operator.md", "file_name": "operator.md"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "636d82cbe736f0e4b87074e2c4bac77becae2a28", "node_type": null, "metadata": {"file_path": "docs/operator.md", "file_name": "operator.md"}, "hash": "088b8b3573d8ad47eb5406f5c3ce360a661b9661e95b0a4c8ff24504e0be2726"}, "2": {"node_id": "65ce763f-c431-435e-9657-0b6f2d5fc9c4", "node_type": null, "metadata": {"file_path": "docs/operator.md", "file_name": "operator.md"}, "hash": "48d6a25b39e6f92b3b5956c9d459780e69f3c92c57da741d80935b86353dad3d"}, "3": {"node_id": "8b5ef574-ebaa-4afd-8777-78f7b986a5a1", "node_type": null, "metadata": {"file_path": "docs/operator.md", "file_name": "operator.md"}, "hash": "cc405d6c98f25aa4de7db21a9a4cafa6e7eaccf417d091cca17be27d59498201"}}, "hash": "0b8d8186d65e2b1b16fec14c7d5307ce5400a77fc9da470e56fd13a77f4971d4", "text": "by line.\n\nBy default the `splitText` operator splits each item into chunks of one line. You can define the number of lines in each chunk by using the parameter `by`, as shown in the following example:\n\n```groovy\nChannel\n    .fromPath('/some/path/*.txt')\n    .splitText( by: 10 )\n    .subscribe {\n        print it;\n        print \"--- end of the chunk ---\\n\"\n    }\n```\n\nAn optional {ref}`closure <script-closure>` can be specified in order to transform the text chunks produced by the operator. The following example shows how to split text files into chunks of 10 lines and transform them to capital letters:\n\n```groovy\nChannel\n    .fromPath('/some/path/*.txt')\n    .splitText( by: 10 ) { it.toUpperCase() }\n    .view()\n```\n\n:::{note}\nText chunks returned by the `splitText` operator are always terminated by a `\\n` newline character.\n:::\n\nAvailable options:\n\n`by`\n: Defines the number of lines in each `chunk` (default: `1`).\n\n`charset`\n: Parse the content by using the specified charset e.g. `UTF-8`.\n\n`compress`\n: When `true` resulting file chunks are GZIP compressed. The `.gz` suffix is automatically added to chunk file names.\n\n`decompress`\n: When `true`, decompress the content using the GZIP format before processing it (note: files whose name ends with `.gz` extension are decompressed automatically).\n\n`elem`\n: The index of the element to split when the operator is applied to a channel emitting list/tuple objects (default: first file object or first element).\n\n`file`\n: When `true` saves each split to a file. Use a string instead of `true` value to create split files with a specific name (split index number is automatically added). Finally, set this attribute to an existing directory, in order to save the split files into the specified folder.\n\n`keepHeader`\n: Parses the first line as header and prepends it to each emitted chunk.\n\n`limit`\n: Limits the number of retrieved lines for each file to the specified value.\n\nSee also: [countLines](#countlines)\n\n(operator-subscribe)=\n\n## subscribe\n\n*Returns: nothing*\n\nThe `subscribe` operator allows you to execute a user defined function each time a new value is emitted by the source channel.\n\nThe emitted value is passed implicitly to the specified function. For example:\n\n```groovy\n// define a channel emitting three values\nsource = Channel.of( 'alpha', 'beta', 'delta' )\n\n// subscribe a function to the channel printing the emitted values\nsource.subscribe { println \"Got: $it\" }\n```\n\n```\nGot: alpha\nGot: beta\nGot: delta\n```\n\n:::{note}\nIn Groovy, the language on which Nextflow is based, the user defined function is called a **closure**. Read the {ref}`script-closure` section to learn more about closures.\n:::\n\nIf needed the closure parameter can be defined explicitly, using a name other than `it` and, optionally, specifying the expected value type, as shown in the following example:\n\n```groovy\nChannel\n    .of( 'alpha', 'beta', 'lambda' )\n    .subscribe { String str ->\n        println \"Got: ${str}; len: ${str.size()}\"\n    }\n```\n\n```\nGot: alpha; len: 5\nGot: beta; len: 4\nGot: lambda; len: 6\n```\n\nThe `subscribe` operator may accept one or more of the following event handlers:\n\n- `onNext`: function that is invoked whenever the channel emits a value. Equivalent to using the `subscribe` with a plain closure as", "start_char_idx": 50478, "end_char_idx": 53759, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "8b5ef574-ebaa-4afd-8777-78f7b986a5a1": {"__data__": {"id_": "8b5ef574-ebaa-4afd-8777-78f7b986a5a1", "embedding": null, "metadata": {"file_path": "docs/operator.md", "file_name": "operator.md"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "636d82cbe736f0e4b87074e2c4bac77becae2a28", "node_type": null, "metadata": {"file_path": "docs/operator.md", "file_name": "operator.md"}, "hash": "088b8b3573d8ad47eb5406f5c3ce360a661b9661e95b0a4c8ff24504e0be2726"}, "2": {"node_id": "df5ea1f3-e42e-4bb5-a8b3-e3a101ead26b", "node_type": null, "metadata": {"file_path": "docs/operator.md", "file_name": "operator.md"}, "hash": "0b8d8186d65e2b1b16fec14c7d5307ce5400a77fc9da470e56fd13a77f4971d4"}, "3": {"node_id": "342261b5-48ee-4425-8897-2a96bd90006c", "node_type": null, "metadata": {"file_path": "docs/operator.md", "file_name": "operator.md"}, "hash": "5a56cde521a22d381f69d37154f1de184d4e2d691a19aee5d9d194fee919539c"}}, "hash": "cc405d6c98f25aa4de7db21a9a4cafa6e7eaccf417d091cca17be27d59498201", "text": "channel emits a value. Equivalent to using the `subscribe` with a plain closure as described in the examples above.\n- `onComplete`: function that is invoked after the last value is emitted by the channel.\n- `onError`: function that it is invoked when an exception is raised while handling the `onNext` event. It will not make further calls to `onNext` or `onComplete`. The `onError` method takes as its parameter the `Throwable` that caused the error.\n\nFor example:\n\n```groovy\nChannel\n    .of( 1, 2, 3 )\n    .subscribe onNext: { println it }, onComplete: { println 'Done' }\n```\n\n```\n1\n2\n3\nDone\n```\n\n(operator-sum)=\n\n## sum\n\n*Returns: value channel*\n\nThe `sum` operator creates a channel that emits the sum of all the items emitted by the channel itself. For example:\n\n```groovy\nChannel\n    .of( 8, 6, 2, 5 )\n    .sum()\n    .view { \"The sum is $it\" }\n```\n\n```\nThe sum is 21\n```\n\nAn optional {ref}`closure <script-closure>` parameter can be specified in order to provide a function that, given an item, returns the value to be summed. For example:\n\n```groovy\nChannel\n    .of( 4, 1, 7, 5 )\n    .sum { it * it }\n    .view { \"Square: $it\" }\n```\n\n```\nSquare: 91\n```\n\n## take\n\n*Returns: queue channel*\n\nThe `take` operator allows you to filter only the first `n` items emitted by a channel. For example:\n\n```groovy\nChannel\n    .of( 1, 2, 3, 4, 5, 6 )\n    .take( 3 )\n    .subscribe onNext: { println it }, onComplete: { println 'Done' }\n```\n\n```\n1\n2\n3\nDone\n```\n\n:::{tip}\nSpecifying a size of `-1` causes the operator to take all values.\n:::\n\nSee also [until](#until).\n\n## tap\n\n*Returns: queue channel*\n\nThe `tap` operator is like the [set](#set) operator in that it assigns a source channel to a new target channel.\nbut it also emits the source channel for downstream use. This operator is a useful way to extract intermediate\noutput channels from a chain of operators. For example:\n\n```groovy\nChannel\n    .of ( 'a', 'b', 'c' )\n    .tap { log1 }\n    .map { it * 2 }\n    .tap { log2 }\n    .map { it.toUpperCase() }\n    .view { \"Result: $it\" }\n\nlog1.view { \"Log 1: $it\" }\nlog2.view { \"Log 2: $it\" }\n```\n\n```\nResult: AA\nResult: BB\nResult: CC\n\nLog 1: a\nLog 1: b\nLog 1: c\n\nLog 2: aa\nLog 2: bb\nLog 2: cc\n```\n\n## toInteger\n\n*Returns: queue channel*\n\nThe `toInteger` operator allows you to convert the string values emitted by a channel to `Integer` values. For example:\n\n```groovy\nChannel\n    .of( '1', '7', '12' )\n    .toInteger()\n    .sum()\n    .view()\n```\n\n:::{tip}\nYou can also use `toLong`, `toFloat`, and `toDouble` to convert to other numerical types.\n:::\n\n## toList\n\n*Returns: value channel*\n\nThe `toList` operator collects all the items emitted by a channel to a `List` object and emits the resulting collection as a single item. For example:\n\n```groovy\nChannel\n   ", "start_char_idx": 53755, "end_char_idx": 56514, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "342261b5-48ee-4425-8897-2a96bd90006c": {"__data__": {"id_": "342261b5-48ee-4425-8897-2a96bd90006c", "embedding": null, "metadata": {"file_path": "docs/operator.md", "file_name": "operator.md"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "636d82cbe736f0e4b87074e2c4bac77becae2a28", "node_type": null, "metadata": {"file_path": "docs/operator.md", "file_name": "operator.md"}, "hash": "088b8b3573d8ad47eb5406f5c3ce360a661b9661e95b0a4c8ff24504e0be2726"}, "2": {"node_id": "8b5ef574-ebaa-4afd-8777-78f7b986a5a1", "node_type": null, "metadata": {"file_path": "docs/operator.md", "file_name": "operator.md"}, "hash": "cc405d6c98f25aa4de7db21a9a4cafa6e7eaccf417d091cca17be27d59498201"}, "3": {"node_id": "6bf31471-1b03-4690-a43e-c8c71d8bd884", "node_type": null, "metadata": {"file_path": "docs/operator.md", "file_name": "operator.md"}, "hash": "ab537e6f20d59387fc1b1db89be69618d5f94da2f53151381cfcde665d1cde19"}}, "hash": "5a56cde521a22d381f69d37154f1de184d4e2d691a19aee5d9d194fee919539c", "text": "as a single item. For example:\n\n```groovy\nChannel\n    .of( 1, 2, 3, 4 )\n    .toList()\n    .subscribe onNext: { println it }, onComplete: { println 'Done' }\n```\n\n```\n[1,2,3,4]\nDone\n```\n\n:::{note}\nThere are two differences between `toList` and `collect`:\n\n- When there is no input, `toList` emits an empty list whereas `collect` emits nothing.\n- By default, `collect` flattens list items by one level.\n\nIn other words, `toList` is equivalent to:\n\n```groovy\ncollect(flat: false).ifEmpty([])\n```\n:::\n\nSee also: [collect](#collect) operator.\n\n## toSortedList\n\n*Returns: value channel*\n\nThe `toSortedList` operator collects all the items emitted by a channel to a `List` object where they are sorted and emits the resulting collection as a single item. For example:\n\n```groovy\nChannel\n    .of( 3, 2, 1, 4 )\n    .toSortedList()\n    .subscribe onNext: { println it }, onComplete: { println 'Done' }\n```\n\n```\n[1,2,3,4]\nDone\n```\n\nYou may also pass a comparator closure as an argument to the `toSortedList` operator to customize the sorting criteria. For example, to sort by the second element of a tuple in descending order:\n\n```groovy\nChannel\n    .of( [\"homer\", 5], [\"bart\", 2], [\"lisa\", 10], [\"marge\", 3], [\"maggie\", 7] )\n    .toSortedList( { a, b -> b[1] <=> a[1] } )\n    .view()\n```\n\n```\n[[lisa, 10], [maggie, 7], [homer, 5], [marge, 3], [bart, 2]]\n```\n\nSee also: [collect](#collect) operator.\n\n## transpose\n\n*Returns: queue channel*\n\nThe `transpose` operator transforms a channel in such a way that the emitted items are the result of a transposition of all tuple elements in each item. For example:\n\n```groovy\nChannel.of(\n        [1, ['A', 'B', 'C']],\n        [2, ['C', 'A']],\n        [3, ['B', 'D']]\n    )\n    .transpose()\n    .view()\n```\n\nThe above snippet prints:\n\n```\n[1, A]\n[1, B]\n[1, C]\n[2, C]\n[2, A]\n[3, B]\n[3, D]\n```\n\nAvailable options:\n\n` by`\n: The index (zero based) of the element to be transposed. Multiple elements can be defined specifying as list of indices e.g. `by: [0,2]`\n\n` remainder`\n: When `false` incomplete tuples are discarded (default). When `true` incomplete tuples are emitted containing a `null` in place of a missing element.\n\n## unique\n\n*Returns: queue channel*\n\nThe `unique` operator allows you to remove duplicate items from a channel and only emit single items with no repetition.\n\nFor example:\n\n```groovy\nChannel\n    .of( 1, 1, 1, 5, 7, 7, 7, 3, 3 )\n    .unique()\n    .view()\n```\n\n```\n1\n5\n7\n3\n```\n\nYou can also specify an optional {ref}`closure <script-closure>` that customizes the way it distinguishes between unique items. For example:\n\n```groovy\nChannel\n    .of(1, 3, 4, 5)\n    .unique { it %", "start_char_idx": 56539, "end_char_idx": 59165, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "6bf31471-1b03-4690-a43e-c8c71d8bd884": {"__data__": {"id_": "6bf31471-1b03-4690-a43e-c8c71d8bd884", "embedding": null, "metadata": {"file_path": "docs/operator.md", "file_name": "operator.md"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "636d82cbe736f0e4b87074e2c4bac77becae2a28", "node_type": null, "metadata": {"file_path": "docs/operator.md", "file_name": "operator.md"}, "hash": "088b8b3573d8ad47eb5406f5c3ce360a661b9661e95b0a4c8ff24504e0be2726"}, "2": {"node_id": "342261b5-48ee-4425-8897-2a96bd90006c", "node_type": null, "metadata": {"file_path": "docs/operator.md", "file_name": "operator.md"}, "hash": "5a56cde521a22d381f69d37154f1de184d4e2d691a19aee5d9d194fee919539c"}}, "hash": "ab537e6f20d59387fc1b1db89be69618d5f94da2f53151381cfcde665d1cde19", "text": ".of(1, 3, 4, 5)\n    .unique { it % 2 }\n    .view()\n```\n\n```\n1\n4\n```\n\n## until\n\n*Returns: queue channel*\n\nThe `until` operator creates a channel that returns the items emitted by the source channel and stop when the condition specified is verified. For example:\n\n```groovy\nChannel\n    .of( 3, 2, 1, 5, 1, 5 )\n    .until { it == 5 }\n    .view()\n```\n\n```\n3\n2\n1\n```\n\nSee also [take](#take).\n\n(operator-view)=\n\n## view\n\n*Returns: queue channel*\n\nThe `view` operator prints the items emitted by a channel to the console standard output. For example:\n\n```groovy\nChannel.of(1, 2, 3).view()\n\n1\n2\n3\n```\n\nEach item is printed on a separate line unless otherwise specified by using the `newLine: false` optional parameter.\n\nHow the channel items are printed can be controlled by using an optional closure parameter. The closure must return the actual value of the item to be printed:\n\n```groovy\nChannel.of(1, 2, 3)\n    .map { it -> [it, it*it] }\n    .view { num, sqr -> \"Square of: $num is $sqr\" }\n```\n\n```\nSquare of: 1 is 1\nSquare of: 2 is 4\nSquare of: 3 is 9\n```\n\nThe `view` operator also emits every item that it receives, allowing it to be chained with other operators.", "start_char_idx": 59176, "end_char_idx": 60337, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "bba317d1-c5eb-4944-9ba2-0912a09d1720": {"__data__": {"id_": "bba317d1-c5eb-4944-9ba2-0912a09d1720", "embedding": null, "metadata": {"file_path": "docs/plugins.md", "file_name": "plugins.md"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "5f68cc26ac6f6f085c287888a51130f9e6d399a1", "node_type": null, "metadata": {"file_path": "docs/plugins.md", "file_name": "plugins.md"}, "hash": "5c2e90eec66c45c3ed69e5f3cd5e48ab3579818cde116662386ddf0efd8c4c86"}, "3": {"node_id": "849314f2-9d75-4761-a629-218cd4f2bc8f", "node_type": null, "metadata": {"file_path": "docs/plugins.md", "file_name": "plugins.md"}, "hash": "8b400d9fe1b74cea49d1eed844b5d5919855f61e1099bdfc3751ed3d5eceeeab"}}, "hash": "debbdb4a04f88c59a5a4e1bc19796a4150f8aa96ebc99e5de779e402870d45d3", "text": "(plugins-page)=\n\n# Plugins\n\n## Main concepts\n\nNextflow is based on a plugins system that allows extending core functionalities via pluggable components that are download and installed at runtime.\n\nCurrently the following functionalities are implemented as plugin components and they make part of the Nextflow *default* plugins:\n\n- `nf-amazon`: Support for Amazon cloud.\n- `nf-azure`: Support for Azure cloud.\n- `nf-console`: Implement Nextflow [REPL console](https://www.nextflow.io/blog/2015/introducing-nextflow-console.html).\n- `nf-ga4gh`: Support [GA4GH APIs](https://www.ga4gh.org/).\n- `nf-google`: Support for Google cloud.\n- `nf-tower`: Support for [Nextflow Tower](https://tower.nf) platform.\n\n## Configuration\n\nNextflow *default* plugins do not require any configuration. They are automatically installed when the corresponding feature is requested by a Nextflow pipeline.\n\nTo use **non-default** plugins in your pipeline execution, you must declare them in the Nextflow configuration file, listing each plugin as shown below:\n\n```groovy\nplugins {\n  id 'nf-hello@0.1.0'\n}\n```\n\nThe plugin identifier consists of the plugin name and plugin version separated by a `@`.\n\nAlternatively, plugins can be required using the `-plugins` command line option:\n\n```bash\nnextflow run <PIPELINE NAME> -plugins nf-hello@0.1.0\n```\n\nMultiple plugins can be specified by separating them with a comma. When specifying plugins via the command line, any plugin declarations in the Nextflow config file are ignored.\n\n## Index\n\nNextflow resolves plugins download location through the [Plugins index](https://github.com/nextflow-io/plugins/). The index stores for each plugin the available version, the creation date, checksum and the link from where the plugin file is downloaded.\n\nTo add a new plugin to the Index, create a pull request including the request plugin metadata. The [nf-hello](https://github.com/nextflow-io/nf-hello) repository provides a minimal code example for the implementation of a Nextflow plugin.\n\n## Import operators from plugin\n\n:::{versionadded} 22.04.0\n:::\n\nNextflow supports the inclusion of custom operators from Nextflow plugins.\n\nFor example:\n\n```groovy\ninclude { sqlInsert; fromQuery as selectFromTable } from 'plugin/nf-sqldb'\n\ndef sql = \"select * from FOO\"\nchannel\n    .selectFromTable(sql, db: \"test\", emitColumns:true)\n    .sqlInsert(into:\"BAR\", columns:'id', db:\"test\")\n```\n\nThe above snippet includes the operators `sqlInsert` and `fromQuery` from the [nf-sqldb](https://github.com/nextflow-io/nf-sqldb) plugin. The latter will be accessible using the `selectFromTable` alias in the script.\n\n:::{note}\nThe prefix `plugin/` must precede the plugin name in the include `from` statement.\n:::\n\n## Import custom functions from plugin\n\n:::{versionadded} 22.09.0-edge\n:::\n\nNextflow supports the inclusion of custom functions from Nextflow plugins.\n\nFor example, a plugin can export a util function to reverse a String:\n\n```groovy\n@nextflow.plugin.extension.Function\nString reverseString( String origin ){\n     origin.reverse()\n}\n```\n\nAnd this function can be used by the pipeline:\n\n```groovy\ninclude { reverseString } from 'plugin/my-plugin'\n\nchannel.of( reverseString('hi') )\n```\n\nThe above snippet includes a function from the plugin and allows the channel to call it directly.\n\nIn the same way as operators, functions can be aliased:\n\n```groovy\ninclude { reverseString as", "start_char_idx": 0, "end_char_idx": 3391, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "849314f2-9d75-4761-a629-218cd4f2bc8f": {"__data__": {"id_": "849314f2-9d75-4761-a629-218cd4f2bc8f", "embedding": null, "metadata": {"file_path": "docs/plugins.md", "file_name": "plugins.md"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "5f68cc26ac6f6f085c287888a51130f9e6d399a1", "node_type": null, "metadata": {"file_path": "docs/plugins.md", "file_name": "plugins.md"}, "hash": "5c2e90eec66c45c3ed69e5f3cd5e48ab3579818cde116662386ddf0efd8c4c86"}, "2": {"node_id": "bba317d1-c5eb-4944-9ba2-0912a09d1720", "node_type": null, "metadata": {"file_path": "docs/plugins.md", "file_name": "plugins.md"}, "hash": "debbdb4a04f88c59a5a4e1bc19796a4150f8aa96ebc99e5de779e402870d45d3"}}, "hash": "8b400d9fe1b74cea49d1eed844b5d5919855f61e1099bdfc3751ed3d5eceeeab", "text": "functions can be aliased:\n\n```groovy\ninclude { reverseString as anotherReverseMethod } from 'plugin/my-plugin'\n```\n\n## Testing custom plugins\n\nTo make a plugin available to Nextflow, it needs to be included in the [plugins repository index](https://github.com/nextflow-io/plugins).\n\nHowever, in order to validate a plugin before it's published in the repository index, it is possible to use the environment\nvariable `NXF_PLUGINS_TEST_REPOSITORY` to specify the URI of a custom index JSON file or the plugin JSON meta file.\n\nFor example:\n\n```bash\nexport NXF_PLUGINS_TEST_REPOSITORY=\"https://github.com/nextflow-io/nf-hello/releases/download/0.3.0/nf-hello-0.3.0-meta.json\"\n```\n\nThen run Nextflow with the expected plugin:\n\n```bash\nnextflow rub <your script> -plugins nf-hello\n```", "start_char_idx": 3328, "end_char_idx": 4106, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "b1f0978e-8242-448a-85b3-6bb61f702256": {"__data__": {"id_": "b1f0978e-8242-448a-85b3-6bb61f702256", "embedding": null, "metadata": {"file_path": "docs/process.md", "file_name": "process.md"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "db7a594acaaabbb6b84352a6719c7045b36a31c3", "node_type": null, "metadata": {"file_path": "docs/process.md", "file_name": "process.md"}, "hash": "50cb2af6615db266b968212168f623fa82945be6d88a86b90d19c62bf36f4b94"}, "3": {"node_id": "987e8618-c2ad-403d-8e4a-e478f0ab5260", "node_type": null, "metadata": {"file_path": "docs/process.md", "file_name": "process.md"}, "hash": "1a733e30695a71974ac65a9f1f0d752a9bcc08f95c6da89ff5d8e123e59731a4"}}, "hash": "8f188946c1803f6d10cea4473539b42b3d5d1e5a7f09ac874fd757275d5cc822", "text": "(process-page)=\n\n# Processes\n\nIn Nextflow, a **process** is the basic processing primitive to execute a user script.\n\nThe process definition starts with the keyword `process`, followed by process name and finally the process body delimited by curly brackets. The process body must contain a string which represents the command or, more generally, a script that is executed by it. A basic process looks like the following example:\n\n```groovy\nprocess sayHello {\n    \"\"\"\n    echo 'Hello world!' > file\n    \"\"\"\n}\n```\n\nA process may contain any of the following definition blocks: directives, inputs, outputs, when clause, and the process script. The syntax is defined as follows:\n\n```\nprocess < name > {\n\n  [ directives ]\n\n  input:\n    < process inputs >\n\n  output:\n    < process outputs >\n\n  when:\n    < condition >\n\n  [script|shell|exec]:\n    < user script to be executed >\n\n}\n```\n\n(process-script)=\n\n## Script\n\nThe `script` block defines, as a string expression, the script that is executed by the process.\n\nA process may contain only one script, and if the `script` guard is not explicitly declared, the script must be the final statement in the process block.\n\nThe script string is executed as a [Bash](<http://en.wikipedia.org/wiki/Bash_(Unix_shell)>) script in the host environment. It can be any command or script that you would normally execute on the command line or in a Bash script. Naturally, the script may only use commands that are available in the host environment.\n\nThe script block can be a simple string or a multi-line string. The latter approach makes it easier to write scripts with multiple commands spanning multiple lines. For example:\n\n```groovy\nprocess doMoreThings {\n  \"\"\"\n  blastp -db $db -query query.fa -outfmt 6 > blast_result\n  cat blast_result | head -n 10 | cut -f 2 > top_hits\n  blastdbcmd -db $db -entry_batch top_hits > sequences\n  \"\"\"\n}\n```\n\nAs explained in the script tutorial section, strings can be defined using single-quotes or double-quotes, and multi-line strings are defined by three single-quote or three double-quote characters.\n\nThere is a subtle but important difference between them. Like in Bash, strings delimited by a `\"` character support variable substitutions, while strings delimited by `'` do not.\n\nIn the above code fragment, the `$db` variable is replaced by the actual value defined elsewhere in the pipeline script.\n\n:::{warning}\nSince Nextflow uses the same Bash syntax for variable substitutions in strings, you must manage them carefully depending on whether you want to evaluate a *Nextflow* variable or a *Bash* variable.\n:::\n\nWhen you need to access a system environment variable in your script, you have two options.\n\nIf you don't need to access any Nextflow variables, you can define your script block with single-quotes:\n\n```groovy\nprocess printPath {\n  '''\n  echo The path is: $PATH\n  '''\n}\n```\n\nOtherwise, you can define your script with double-quotes and escape the system environment variables by prefixing them with a back-slash `\\` character, as shown in the following example:\n\n```groovy\nprocess doOtherThings {\n  \"\"\"\n  blastp -db \\$DB -query query.fa -outfmt 6 > blast_result\n  cat blast_result | head -n $MAX | cut -f 2 > top_hits\n  blastdbcmd -db \\$DB -entry_batch top_hits > sequences\n  \"\"\"\n}\n```\n\nIn this example, `$MAX` is a Nextflow variable that must be defined elsewhere in the pipeline script. Nextflow replaces it with the actual value before executing the script. Meanwhile, `$DB` is a Bash variable that must exist in the execution", "start_char_idx": 0, "end_char_idx": 3521, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "987e8618-c2ad-403d-8e4a-e478f0ab5260": {"__data__": {"id_": "987e8618-c2ad-403d-8e4a-e478f0ab5260", "embedding": null, "metadata": {"file_path": "docs/process.md", "file_name": "process.md"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "db7a594acaaabbb6b84352a6719c7045b36a31c3", "node_type": null, "metadata": {"file_path": "docs/process.md", "file_name": "process.md"}, "hash": "50cb2af6615db266b968212168f623fa82945be6d88a86b90d19c62bf36f4b94"}, "2": {"node_id": "b1f0978e-8242-448a-85b3-6bb61f702256", "node_type": null, "metadata": {"file_path": "docs/process.md", "file_name": "process.md"}, "hash": "8f188946c1803f6d10cea4473539b42b3d5d1e5a7f09ac874fd757275d5cc822"}, "3": {"node_id": "d245941a-e0af-4e45-a04b-e23ff5fbfdef", "node_type": null, "metadata": {"file_path": "docs/process.md", "file_name": "process.md"}, "hash": "a92f6282c45cbe66489bfdddd6973e2b8fa38d81b2c2c600e345acfaa0a8a805"}}, "hash": "1a733e30695a71974ac65a9f1f0d752a9bcc08f95c6da89ff5d8e123e59731a4", "text": "script. Meanwhile, `$DB` is a Bash variable that must exist in the execution environment, and Bash will replace it with the actual value during execution.\n\n:::{tip}\nAlternatively, you can use the {ref}`process-shell` block definition, which allows a script to contain both Bash and Nextflow variables without having to escape the first.\n:::\n\n### Scripts *\u00e0 la carte*\n\nThe process script is interpreted by Nextflow as a Bash script by default, but you are not limited to Bash.\n\nYou can use your favourite scripting language (Perl, Python, R, etc), or even mix them in the same pipeline.\n\nA pipeline may be composed of processes that execute very different tasks. With Nextflow, you can choose the scripting language that best fits the task performed by a given process. For example, for some processes R might be more useful than Perl, whereas for others you may need to use Python because it provides better access to a library or an API, etc.\n\nTo use a language other than Bash, simply start your process script with the corresponding [shebang](<http://en.wikipedia.org/wiki/Shebang_(Unix)>). For example:\n\n```groovy\nprocess perlTask {\n    \"\"\"\n    #!/usr/bin/perl\n\n    print 'Hi there!' . '\\n';\n    \"\"\"\n}\n\nprocess pythonTask {\n    \"\"\"\n    #!/usr/bin/python\n\n    x = 'Hello'\n    y = 'world!'\n    print \"%s - %s\" % (x,y)\n    \"\"\"\n}\n\nworkflow {\n    perlTask()\n    pythonTask()\n}\n```\n\n:::{tip}\nSince the actual location of the interpreter binary file can differ across platforms, it is wise to use the `env` command followed by the interpreter name, e.g. `#!/usr/bin/env perl`, instead of the absolute path, in order to make your script more portable.\n:::\n\n### Conditional scripts\n\nSo far, our `script` block has always been a simple string expression, but in reality, the `script` block is just Groovy code that returns a string. This means that you can write arbitrary Groovy code to determine the script to execute, as long as the final statement is a string (remember that the `return` keyword is optional in Groovy).\n\nFor example, you can use flow control statements (`if`, `switch`, etc) to execute a different script based on the process inputs. The only difference here is that you must explicitly declare the `script` guard, whereas before it was not required. Here is an example:\n\n```groovy\nmode = 'tcoffee'\n\nprocess align {\n    input:\n    path sequences\n\n    script:\n    if( mode == 'tcoffee' )\n        \"\"\"\n        t_coffee -in $sequences > out_file\n        \"\"\"\n\n    else if( mode == 'mafft' )\n        \"\"\"\n        mafft --anysymbol --parttree --quiet $sequences > out_file\n        \"\"\"\n\n    else if( mode == 'clustalo' )\n        \"\"\"\n        clustalo -i $sequences -o out_file\n        \"\"\"\n\n    else\n        error \"Invalid alignment mode: ${mode}\"\n}\n```\n\nIn the above example, the process will execute one of the script fragments depending on the value of the `mode` parameter. By default it will execute the `tcoffee` command, but changing the `mode` variable will cause a different branch to be executed.\n\n(process-template)=\n\n### Template\n\nProcess scripts can be externalised to **template** files, which can be reused across different processes and tested independently from the overall pipeline", "start_char_idx": 3458, "end_char_idx": 6661, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "d245941a-e0af-4e45-a04b-e23ff5fbfdef": {"__data__": {"id_": "d245941a-e0af-4e45-a04b-e23ff5fbfdef", "embedding": null, "metadata": {"file_path": "docs/process.md", "file_name": "process.md"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "db7a594acaaabbb6b84352a6719c7045b36a31c3", "node_type": null, "metadata": {"file_path": "docs/process.md", "file_name": "process.md"}, "hash": "50cb2af6615db266b968212168f623fa82945be6d88a86b90d19c62bf36f4b94"}, "2": {"node_id": "987e8618-c2ad-403d-8e4a-e478f0ab5260", "node_type": null, "metadata": {"file_path": "docs/process.md", "file_name": "process.md"}, "hash": "1a733e30695a71974ac65a9f1f0d752a9bcc08f95c6da89ff5d8e123e59731a4"}, "3": {"node_id": "2ea9be27-a1af-461b-ab04-71f4ac6fca73", "node_type": null, "metadata": {"file_path": "docs/process.md", "file_name": "process.md"}, "hash": "a158d97f641fadea8a7b5e9945c3cbc16a4bb63d1a66e20d7b135912ab3e82f1"}}, "hash": "a92f6282c45cbe66489bfdddd6973e2b8fa38d81b2c2c600e345acfaa0a8a805", "text": "be reused across different processes and tested independently from the overall pipeline execution.\n\nA template is simply a shell script file that Nextflow is able to execute by using the `template` function as shown below:\n\n```groovy\nprocess templateExample {\n    input:\n    val STR\n\n    script:\n    template 'my_script.sh'\n}\n\nworkflow {\n    Channel.of('this', 'that') | templateExample\n}\n```\n\nBy default, Nextflow looks for the `my_script.sh` template file in the `templates` directory located alongside the Nextflow script and/or the module script in which the process is defined. Any other location can be specified by using an absolute template path.\n\nThe template script may contain any code that can be executed by the underlying environment. For example:\n\n```bash\n#!/bin/bash\necho \"process started at `date`\"\necho $STR\necho \"process completed\"\n```\n\n:::{tip}\nThe dollar character (`$`) is interpreted as a Nextflow variable when the script is run as a Nextflow template, whereas it is evaluated as a Bash variable when run as a Bash script. This can be very useful for testing your script independently from Nextflow execution. You only need to provide a Bash environment variable for each of the Nextflow variables that are referenced in your script. For example, it would be possible to execute the above script with the following command in the terminal: `STR='foo' bash templates/my_script.sh`\n:::\n\n:::{tip}\nAs a best practice, the template script should not contain any `\\$` escaped variables, because these variables will not be evaluated properly when the script is executed directly.\n:::\n\n(process-shell)=\n\n### Shell\n\nThe `shell` block is a string expression that defines the script that is executed by the process. It is an alternative to the {ref}`process-script` definition with one important difference: it uses the exclamation mark `!` character, instead of the usual dollar `$` character, to denote Nextflow variables.\n\nThis way, it is possible to use both Nextflow and Bash variables in the same script without having to escape the latter, which makes process scripts easier to read and maintain. For example:\n\n```groovy\nprocess myTask {\n    input:\n    val str\n\n    shell:\n    '''\n    echo \"User $USER says !{str}\"\n    '''\n}\n\nworkflow {\n    Channel.of('Hello', 'Hola', 'Bonjour') | myTask\n}\n```\n\nIn the above example, `$USER` is treated as a Bash variable, while `!{str}` is treated as a Nextflow variable.\n\n:::{note}\n- Shell script definitions require the use of single-quote `'` delimited strings. When using double-quote `\"` delimited strings, dollar variables are interpreted as Nextflow variables as usual. See {ref}`string-interpolation`.\n- Variables prefixed with `!` must always be enclosed in curly brackets, i.e. `!{str}` is a valid variable whereas `!str` is ignored.\n- Shell scripts support the use of the {ref}`process-template` mechanism. The same rules are applied to the variables defined in the script template.\n:::\n\n(process-native)=\n\n### Native execution\n\nNextflow processes can also execute native Groovy code as the task itself, using the `exec` block. Whereas the `script` block defines a script to be executed, the `exec` block defines Groovy code to be executed directly.\n\nFor example:\n\n```groovy\nprocess simpleSum {\n    input:\n    val x\n\n    exec:\n    println \"Hello Mr. $x\"\n}\n\nworkflow {\n    Channel.of('a', 'b', 'c') | simpleSum\n}\n```\n\nwill display:\n\n```\nHello Mr. b\nHello", "start_char_idx": 6650, "end_char_idx": 10070, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "2ea9be27-a1af-461b-ab04-71f4ac6fca73": {"__data__": {"id_": "2ea9be27-a1af-461b-ab04-71f4ac6fca73", "embedding": null, "metadata": {"file_path": "docs/process.md", "file_name": "process.md"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "db7a594acaaabbb6b84352a6719c7045b36a31c3", "node_type": null, "metadata": {"file_path": "docs/process.md", "file_name": "process.md"}, "hash": "50cb2af6615db266b968212168f623fa82945be6d88a86b90d19c62bf36f4b94"}, "2": {"node_id": "d245941a-e0af-4e45-a04b-e23ff5fbfdef", "node_type": null, "metadata": {"file_path": "docs/process.md", "file_name": "process.md"}, "hash": "a92f6282c45cbe66489bfdddd6973e2b8fa38d81b2c2c600e345acfaa0a8a805"}, "3": {"node_id": "21251270-23dc-4a6f-b29f-b9c13a48c226", "node_type": null, "metadata": {"file_path": "docs/process.md", "file_name": "process.md"}, "hash": "724a826940b825ac87b695cb9b59747a0f8d63b579d403134446134d5f8b18bc"}}, "hash": "a158d97f641fadea8a7b5e9945c3cbc16a4bb63d1a66e20d7b135912ab3e82f1", "text": "display:\n\n```\nHello Mr. b\nHello Mr. a\nHello Mr. c\n```\n\n(process-stub)=\n\n## Stub\n\n:::{versionadded} 20.11.0-edge\n:::\n\nYou can define a command *stub*, which replaces the actual process command when the `-stub-run` or `-stub` command-line option is enabled:\n\n```groovy\nprocess INDEX {\n  input:\n    path transcriptome\n\n  output:\n    path 'index'\n\n  script:\n    \"\"\"\n    salmon index --threads $task.cpus -t $transcriptome -i index\n    \"\"\"\n\n  stub:\n    \"\"\"\n    mkdir index\n    touch index/seq.bin\n    touch index/info.json\n    touch index/refseq.bin\n    \"\"\"\n}\n```\n\nThe `stub` block can be defined before or after the `script` block. When the pipeline is executed with the `-stub-run` option and a process's `stub` is not defined, the `script` block is executed.\n\nThis feature makes it easier to quickly prototype the workflow logic without using the real commands. The developer can use it to provide a dummy script that mimics the execution of the real one in a quicker manner. In other words, it is a way to perform a dry-run.\n\n(process-input)=\n\n## Inputs\n\nThe `input` block allows you to define the input channels of a process, similar to function arguments. A process may have at most one input block, and it must contain at least one input.\n\nThe input block follows the syntax shown below:\n\n```\ninput:\n  <input qualifier> <input name>\n```\n\nAn input definition consists of a *qualifier* and a *name*. The input qualifier defines the type of data to be received. This information is used by Nextflow to apply the semantic rules associated with each qualifier, and handle it properly depending on the target execution platform (grid, cloud, etc).\n\nWhen a process is invoked in a workflow block, it must be provided a channel for each channel in the process input block, similar to calling a function with specific arguments. The examples provided in the following sections demonstrate how a process is invoked with input channels.\n\nThe following input qualifiers are available:\n\n- `val`: Access the input value by name in the process script.\n- `file`: (DEPRECATED) Handle the input value as a file, staging it properly in the execution context.\n- `path`: Handle the input value as a path, staging the file properly in the execution context.\n- `env`: Use the input value to set an environment variable in the process script.\n- `stdin`: Forward the input value to the process `stdin` special file.\n- `tuple`: Handle a group of input values having any of the above qualifiers.\n- `each`: Execute the process for each element in the input collection.\n\n### Input type `val`\n\nThe `val` qualifier accepts any data type. It can be accessed in the process script by using the specified input name, as shown in the following example:\n\n```groovy\nprocess basicExample {\n  input:\n  val x\n\n  \"echo process job $x\"\n}\n\nworkflow {\n  def num = Channel.of(1,2,3)\n  basicExample(num)\n}\n```\n\nIn the above example, the process is executed three times: once for each value emitted by the `num` channel. The resulting output is similar to the one shown below:\n\n```\nprocess job 3\nprocess job 1\nprocess job 2\n```\n\n:::{note}\nWhile channels do emit items in the order that they are received, *processes* do not necessarily *process* items in the order that they are received. In the above example, the value `3` was processed before the others.\n:::\n\n:::{note}\nWhen the process declares exactly one input, the pipe `|` operator can be used to provide", "start_char_idx": 10118, "end_char_idx": 13535, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "21251270-23dc-4a6f-b29f-b9c13a48c226": {"__data__": {"id_": "21251270-23dc-4a6f-b29f-b9c13a48c226", "embedding": null, "metadata": {"file_path": "docs/process.md", "file_name": "process.md"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "db7a594acaaabbb6b84352a6719c7045b36a31c3", "node_type": null, "metadata": {"file_path": "docs/process.md", "file_name": "process.md"}, "hash": "50cb2af6615db266b968212168f623fa82945be6d88a86b90d19c62bf36f4b94"}, "2": {"node_id": "2ea9be27-a1af-461b-ab04-71f4ac6fca73", "node_type": null, "metadata": {"file_path": "docs/process.md", "file_name": "process.md"}, "hash": "a158d97f641fadea8a7b5e9945c3cbc16a4bb63d1a66e20d7b135912ab3e82f1"}, "3": {"node_id": "964776da-1750-475e-9ff5-efe227f3cb04", "node_type": null, "metadata": {"file_path": "docs/process.md", "file_name": "process.md"}, "hash": "9cd49e8d94a095c38ddfd897716757bfa58d7aaee32362b26b11e274f6cbb78b"}}, "hash": "724a826940b825ac87b695cb9b59747a0f8d63b579d403134446134d5f8b18bc", "text": "process declares exactly one input, the pipe `|` operator can be used to provide inputs to the process, instead of passing it as a parameter. Both methods have identical semantics:\n\n```groovy\nprocess basicExample {\n  input:\n  val x\n\n  \"echo process job $x\"\n}\n\nworkflow {\n  Channel.of(1,2,3) | basicExample\n}\n```\n:::\n\n### Input type `file`\n\n:::{note}\nThe `file` qualifier was the standard way to handle input files prior to Nextflow 19.10.0. In later versions of Nextflow, the `path` qualifier should be preferred over `file`.\n:::\n\nThe `file` qualifier is identical to `path`, with one important difference. When a `file` input receives a value that is not a file, it automatically converts the value to a string and saves it to a temporary file. This behavior is useful in some cases, but tends to be confusing in general. The `path` qualifier instead interprets string values as the path location of the input file and automatically converts to a file object.\n\n(process-input-path)=\n\n### Input type `path`\n\nThe `path` qualifier allows you to provide input files to the process execution context. Nextflow will stage the files into the process execution directory, and they can be accessed in the script by using the specified input name. For example:\n\n```groovy\nprocess blastThemAll {\n  input:\n  path query_file\n\n  \"blastp -query ${query_file} -db nr\"\n}\n\nworkflow {\n  def proteins = Channel.fromPath( '/some/path/*.fa' )\n  blastThemAll(proteins)\n}\n```\n\nIn the above example, all the files ending with the suffix `.fa` are sent over the channel `proteins`. These files are received by the process, which executes a BLAST query on each of them.\n\nIt's worth noting that in the above example, the name of the file in the file-system is not used. You can access the file without even knowing its name, because you can reference it in the process script by the input name.\n\nThere may be cases where your task needs to use a file whose name is fixed, i.e. it does not have to change along with the actual provided file. In this case, you can specify a fixed name with the `name` attribute in the input file parameter definition, as shown in the following example:\n\n```groovy\ninput:\npath query_file, name: 'query.fa'\n```\n\nor, using a shorter syntax:\n\n```groovy\ninput:\npath 'query.fa'\n```\n\nThe previous example can be re-written as shown below:\n\n```groovy\nprocess blastThemAll {\n  input:\n  path 'query.fa'\n\n  \"blastp -query query.fa -db nr\"\n}\n\nworkflow {\n  def proteins = Channel.fromPath( '/some/path/*.fa' )\n  blastThemAll(proteins)\n}\n```\n\nIn this example, each file received by the process is staged with the name `query.fa` in a different execution context (i.e. the folder where a task is executed).\n\n:::{tip}\nThis feature allows you to execute the process command multiple times without worrying about the file names changing. In other words, Nextflow helps you write pipeline tasks that are self-contained and decoupled from the execution environment. As a best practice, you should avoid referencing files in your process script other than those defined in your input block.\n:::\n\nChannel factories like `Channel.fromPath` produce file objects, but a `path` input can also accept a string literal path. The string value should be an absolute path, i.e. it must be prefixed with a `/` character or a supported URI protocol (`file://`, `http://`, `s3://`, etc), and it cannot contain special characters (`\\n`, etc).\n\n```groovy\nprocess foo {\n  input:\n  path x\n\n  \"\"\"\n  your_command --in", "start_char_idx": 13498, "end_char_idx": 16980, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "964776da-1750-475e-9ff5-efe227f3cb04": {"__data__": {"id_": "964776da-1750-475e-9ff5-efe227f3cb04", "embedding": null, "metadata": {"file_path": "docs/process.md", "file_name": "process.md"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "db7a594acaaabbb6b84352a6719c7045b36a31c3", "node_type": null, "metadata": {"file_path": "docs/process.md", "file_name": "process.md"}, "hash": "50cb2af6615db266b968212168f623fa82945be6d88a86b90d19c62bf36f4b94"}, "2": {"node_id": "21251270-23dc-4a6f-b29f-b9c13a48c226", "node_type": null, "metadata": {"file_path": "docs/process.md", "file_name": "process.md"}, "hash": "724a826940b825ac87b695cb9b59747a0f8d63b579d403134446134d5f8b18bc"}, "3": {"node_id": "44945d41-5a12-491a-924a-b97e239af1ba", "node_type": null, "metadata": {"file_path": "docs/process.md", "file_name": "process.md"}, "hash": "c03d98e35c956443404c27d03b02627b7ccfc6d3960515997b10e86e393df375"}}, "hash": "9cd49e8d94a095c38ddfd897716757bfa58d7aaee32362b26b11e274f6cbb78b", "text": "foo {\n  input:\n  path x\n\n  \"\"\"\n  your_command --in $x\n  \"\"\"\n}\n\nworkflow {\n  foo('/some/data/file.txt')\n}\n```\n\nThe `stageAs` option allows you to control how the file should be named in the task work directory. You can provide a specific name or a pattern as described in the [Multiple input files](#multiple-input-files) section:\n\n```groovy\nprocess foo {\n  input:\n  path x, stageAs: 'data.txt'\n\n  \"\"\"\n  your_command --in data.txt\n  \"\"\"\n}\n\nworkflow {\n  foo('/some/data/file.txt')\n}\n```\n\n### Multiple input files\n\nA `path` input can also accept a collection of files instead of a single value. In this case, the input variable will be a Groovy list, and you can use it as such.\n\nWhen the input has a fixed file name and a collection of files is received by the process, the file name will be appended with a numerical suffix representing its ordinal position in the list. For example:\n\n```groovy\nprocess blastThemAll {\n    input:\n    path 'seq'\n\n    \"echo seq*\"\n}\n\nworkflow {\n    def fasta = Channel.fromPath( \"/some/path/*.fa\" ).buffer(size: 3)\n    blastThemAll(fasta)\n}\n```\n\nwill output:\n\n```\nseq1 seq2 seq3\nseq1 seq2 seq3\n...\n```\n\nThe target input file name may contain the `*` and `?` wildcards, which can be used to control the name of staged files. The following table shows how the wildcards are replaced depending on the cardinality of the received input collection.\n\n| Cardinality | Name pattern | Staged file names                                                                                       |\n| ----------- | ------------ | ------------------------------------------------------------------------------------------------------- |\n| any         | `*`          | named as the source file                                                                                |\n| 1           | `file*.ext`  | `file.ext`                                                                                              |\n| 1           | `file?.ext`  | `file1.ext`                                                                                             |\n| 1           | `file??.ext` | `file01.ext`                                                                           ", "start_char_idx": 17009, "end_char_idx": 19186, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "44945d41-5a12-491a-924a-b97e239af1ba": {"__data__": {"id_": "44945d41-5a12-491a-924a-b97e239af1ba", "embedding": null, "metadata": {"file_path": "docs/process.md", "file_name": "process.md"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "db7a594acaaabbb6b84352a6719c7045b36a31c3", "node_type": null, "metadata": {"file_path": "docs/process.md", "file_name": "process.md"}, "hash": "50cb2af6615db266b968212168f623fa82945be6d88a86b90d19c62bf36f4b94"}, "2": {"node_id": "964776da-1750-475e-9ff5-efe227f3cb04", "node_type": null, "metadata": {"file_path": "docs/process.md", "file_name": "process.md"}, "hash": "9cd49e8d94a095c38ddfd897716757bfa58d7aaee32362b26b11e274f6cbb78b"}, "3": {"node_id": "1ed85396-1fe1-4c6b-ace0-dab134705495", "node_type": null, "metadata": {"file_path": "docs/process.md", "file_name": "process.md"}, "hash": "c5a67f87512b4c6821bcb43746c25a21eba07c1ac6fd934f7aa5024cb3fdf2b5"}}, "hash": "c03d98e35c956443404c27d03b02627b7ccfc6d3960515997b10e86e393df375", "text": "                                    |\n| many        | `file*.ext`  | `file1.ext`, `file2.ext`, `file3.ext`, ..                                                               |\n| many        | `file?.ext`  | `file1.ext`, `file2.ext`, `file3.ext`, ..                                                               |\n| many        | `file??.ext` | `file01.ext`, `file02.ext`, `file03.ext`, ..                                                            |\n| many        | `dir/*`      | named as the source file, created in `dir` subdirectory                                                 |\n| many        | `dir??/*`    | named as the source file, created in a progressively indexed subdirectory e.g. `dir01/`, `dir02/`, etc. |\n| many        | `dir*/*`     | (as above)                                                                                              |\n\nThe following example shows how a wildcard can be used in the input file definition:\n\n```groovy\nprocess blastThemAll {\n    input:\n    path 'seq?.fa'\n\n    \"cat seq1.fa seq2.fa seq3.fa\"\n}\n\nworkflow {\n    def fasta = Channel.fromPath( \"/some/path/*.fa\" ).buffer(size: 3)\n    blastThemAll(fasta)\n}\n```\n\n:::{note}\nRewriting input file names according to a named pattern is an extra feature and not at all required. The normal file input syntax introduced in the {ref}`process-input-path` section is valid for collections of multiple files as well. To handle multiple input files while preserving the original file names, use a variable identifier or the `*` wildcard.\n:::\n\n### Dynamic input file names\n\nWhen the input file name is specified by using the `name` option or a string literal, you can also use other input values as variables in the file name string. For example:\n\n```groovy\nprocess simpleCount {\n  input:\n  val x\n  path \"${x}.fa\"\n\n  \"\"\"\n  cat ${x}.fa | grep '>'\n  \"\"\"\n}\n```\n\nIn the above example, the input file name is determined by the current value of the `x` input value.\n\nThis approach allows input files to be staged in the task directory with a name that is coherent with the current execution context.\n\n:::{tip}\nIn most cases, you won't need to use dynamic file names, because each task is executed in its own directory, and input files are automatically staged into this directory by Nextflow. This", "start_char_idx": 19226, "end_char_idx": 21502, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "1ed85396-1fe1-4c6b-ace0-dab134705495": {"__data__": {"id_": "1ed85396-1fe1-4c6b-ace0-dab134705495", "embedding": null, "metadata": {"file_path": "docs/process.md", "file_name": "process.md"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "db7a594acaaabbb6b84352a6719c7045b36a31c3", "node_type": null, "metadata": {"file_path": "docs/process.md", "file_name": "process.md"}, "hash": "50cb2af6615db266b968212168f623fa82945be6d88a86b90d19c62bf36f4b94"}, "2": {"node_id": "44945d41-5a12-491a-924a-b97e239af1ba", "node_type": null, "metadata": {"file_path": "docs/process.md", "file_name": "process.md"}, "hash": "c03d98e35c956443404c27d03b02627b7ccfc6d3960515997b10e86e393df375"}, "3": {"node_id": "e23489cd-f252-48fb-ace5-bfce5c2f3251", "node_type": null, "metadata": {"file_path": "docs/process.md", "file_name": "process.md"}, "hash": "d12d7346ed74796017d1e57fcbae87c6df32fd0fcdfb54e4cb9e2147a5e96fb1"}}, "hash": "c5a67f87512b4c6821bcb43746c25a21eba07c1ac6fd934f7aa5024cb3fdf2b5", "text": "own directory, and input files are automatically staged into this directory by Nextflow. This behavior guarantees that input files with the same name won't overwrite each other.\n\nAn example of when you may have to deal with that is when you have many input files in a task, and some of these files may have the same filename. In this case, a solution would be to use the `stageAs` option.\n:::\n\n### Input type `env`\n\nThe `env` qualifier allows you to define an environment variable in the process execution context based on the input value. For example:\n\n```groovy\nprocess printEnv {\n    input:\n    env HELLO\n\n    '''\n    echo $HELLO world!\n    '''\n}\n\nworkflow {\n    Channel.of('hello', 'hola', 'bonjour', 'ciao') | printEnv\n}\n```\n\n```\nhello world!\nciao world!\nbonjour world!\nhola world!\n```\n\n### Input type `stdin`\n\nThe `stdin` qualifier allows you to forward the input value to the [standard input](http://en.wikipedia.org/wiki/Standard_streams#Standard_input_.28stdin.29) of the process script. For example:\n\n```groovy\nprocess printAll {\n  input:\n  stdin str\n\n  \"\"\"\n  cat -\n  \"\"\"\n}\n\nworkflow {\n  Channel.of('hello', 'hola', 'bonjour', 'ciao')\n    | map { it + '\\n' }\n    | printAll\n}\n```\n\nwill output:\n\n```\nhola\nbonjour\nciao\nhello\n```\n\n(process-input-set)=\n\n### Input type `set`\n\n:::{deprecated} 19.08.1-edge\nUse `tuple` instead.\n:::\n\n(process-input-tuple)=\n\n### Input type `tuple`\n\nThe `tuple` qualifier allows you to group multiple values into a single input definition. It can be useful when a channel emits tuples of values that need to be handled separately. Each element in the tuple is associated with a corresponding element in the `tuple` definition. For example:\n\n```groovy\nprocess tupleExample {\n    input:\n    tuple val(x), path('latin.txt')\n\n    \"\"\"\n    echo \"Processing $x\"\n    cat - latin.txt > copy\n    \"\"\"\n}\n\nworkflow {\n  Channel.of( [1, 'alpha'], [2, 'beta'], [3, 'delta'] ) | tupleExample\n}\n```\n\nIn the above example, the `tuple` input consists of the value `x` and the file `latin.txt`.\n\nA `tuple` definition may contain any of the following qualifiers, as previously described: `val`, `env`, `path` and `stdin`. Files specified with the `path` qualifier are treated exactly the same as standalone `path` inputs.\n\n### Input repeaters (`each`)\n\nThe `each` qualifier allows you to repeat the execution of a process for each item in a collection, each time a new value is received. For example:\n\n```groovy\nprocess alignSequences {\n  input:\n  path seq\n  each mode\n\n  \"\"\"\n  t_coffee -in $seq -mode $mode > result\n  \"\"\"\n}\n\nworkflow {\n  sequences = Channel.fromPath('*.fa')\n  methods = ['regular', 'espresso', 'psicoffee']\n\n  alignSequences(sequences, methods)\n}\n```\n\nIn the above example, each time a file of sequences is emitted from the `sequences` channel, the process executes *three* tasks, each running a T-coffee alignment with a different value for the `mode` parameter. This behavior is useful when you need to repeat the same task over a given set of parameters.\n\nInput repeaters can be applied to files as well. For example:\n\n```groovy\nprocess alignSequences", "start_char_idx": 21423, "end_char_idx": 24508, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "e23489cd-f252-48fb-ace5-bfce5c2f3251": {"__data__": {"id_": "e23489cd-f252-48fb-ace5-bfce5c2f3251", "embedding": null, "metadata": {"file_path": "docs/process.md", "file_name": "process.md"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "db7a594acaaabbb6b84352a6719c7045b36a31c3", "node_type": null, "metadata": {"file_path": "docs/process.md", "file_name": "process.md"}, "hash": "50cb2af6615db266b968212168f623fa82945be6d88a86b90d19c62bf36f4b94"}, "2": {"node_id": "1ed85396-1fe1-4c6b-ace0-dab134705495", "node_type": null, "metadata": {"file_path": "docs/process.md", "file_name": "process.md"}, "hash": "c5a67f87512b4c6821bcb43746c25a21eba07c1ac6fd934f7aa5024cb3fdf2b5"}, "3": {"node_id": "8e6ad538-2e89-48a3-854c-f7a8abe351ee", "node_type": null, "metadata": {"file_path": "docs/process.md", "file_name": "process.md"}, "hash": "b7e05bbf3c406063477f5621f3610776e62d3c17a3761d90f3edeca1bf2c4ac0"}}, "hash": "d12d7346ed74796017d1e57fcbae87c6df32fd0fcdfb54e4cb9e2147a5e96fb1", "text": "to files as well. For example:\n\n```groovy\nprocess alignSequences {\n  input:\n  path seq\n  each mode\n  each path(lib)\n\n  \"\"\"\n  t_coffee -in $seq -mode $mode -lib $lib > result\n  \"\"\"\n}\n\nworkflow {\n  sequences = Channel.fromPath('*.fa')\n  methods = ['regular', 'espresso']\n  libraries = [ file('PQ001.lib'), file('PQ002.lib'), file('PQ003.lib') ]\n\n  alignSequences(sequences, methods, libraries)\n}\n```\n\nIn the above example, each sequence input file emitted by the `sequences` channel triggers six alignment tasks, three with the `regular` method against each library file, and three with the `espresso` method.\n\n:::{note}\nWhen multiple repeaters are defined, the process is executed for each *combination* of them.\n:::\n\n:::{note}\nInput repeaters currently do not support tuples. However, you can emulate an input repeater on a channel of tuples by using the {ref}`operator-combine` or {ref}`operator-cross` operator with other input channels to produce all of the desired input combinations.\n:::\n\n(process-multiple-input-channels)=\n\n### Multiple input channels\n\nA key feature of processes is the ability to handle inputs from multiple channels.\n\nWhen two or more channels are declared as process inputs, the process waits until there is a complete input configuration, i.e. until it receives a value from each input channel. When this condition is satisfied, the process consumes a value from each channel and launches a new task, repeating this logic until one or more channels are empty.\n\nAs a result, channel values are consumed sequentially and any empty channel will cause the process to wait, even if the other channels have values.\n\nFor example:\n\n```groovy\nprocess foo {\n  input:\n  val x\n  val y\n\n  script:\n  \"\"\"\n  echo $x and $y\n  \"\"\"\n}\n\nworkflow {\n  x = Channel.of(1, 2)\n  y = Channel.of('a', 'b', 'c')\n  foo(x, y)\n}\n```\n\nThe process `foo` is executed two times because the `x` channel emits only two values, therefore the `c` element is discarded. It outputs:\n\n```\n1 and a\n2 and b\n```\n\nA different semantic is applied when using a {ref}`value channel <channel-type-value>`. This kind of channel is created by the {ref}`Channel.value <channel-value>` factory method or implicitly when a process is invoked with an argument that is not a channel. By definition, a value channel is bound to a single value and it can be read an unlimited number of times without consuming its content. Therefore, when mixing a value channel with one or more (queue) channels, it does not affect the process termination because the underlying value is applied repeatedly.\n\nTo better understand this behavior, compare the previous example with the following one:\n\n```groovy\nprocess bar {\n  input:\n  val x\n  val y\n\n  script:\n  \"\"\"\n  echo $x and $y\n  \"\"\"\n}\n\nworkflow {\n  x = Channel.value(1)\n  y = Channel.of('a', 'b', 'c')\n  foo(x, y)\n}\n```\n\nThe above example executes the `bar` process three times because `x` is a value channel, therefore its value can be read as many times as needed. The process termination is determined by the contents of `y`. It outputs:\n\n```\n1 and a\n1 and b\n1 and c\n```\n\n:::{note}\nIn general, multiple input channels should be used to process *combinations* of different inputs, using the `each` qualifier or value channels. Having multiple queue channels as inputs is equivalent to using the `merge` operator, which is not recommended as it may lead to inputs being combined in a non-deterministic", "start_char_idx": 24531, "end_char_idx": 27938, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "8e6ad538-2e89-48a3-854c-f7a8abe351ee": {"__data__": {"id_": "8e6ad538-2e89-48a3-854c-f7a8abe351ee", "embedding": null, "metadata": {"file_path": "docs/process.md", "file_name": "process.md"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "db7a594acaaabbb6b84352a6719c7045b36a31c3", "node_type": null, "metadata": {"file_path": "docs/process.md", "file_name": "process.md"}, "hash": "50cb2af6615db266b968212168f623fa82945be6d88a86b90d19c62bf36f4b94"}, "2": {"node_id": "e23489cd-f252-48fb-ace5-bfce5c2f3251", "node_type": null, "metadata": {"file_path": "docs/process.md", "file_name": "process.md"}, "hash": "d12d7346ed74796017d1e57fcbae87c6df32fd0fcdfb54e4cb9e2147a5e96fb1"}, "3": {"node_id": "3a4bc64a-71be-4851-aa46-f9163e424db4", "node_type": null, "metadata": {"file_path": "docs/process.md", "file_name": "process.md"}, "hash": "1838bb5ddf818ae8554a704b7e6ea96b4f92fc0cc23c0e793e5bec4fceca7098"}}, "hash": "b7e05bbf3c406063477f5621f3610776e62d3c17a3761d90f3edeca1bf2c4ac0", "text": "not recommended as it may lead to inputs being combined in a non-deterministic way.\n:::\n\nSee also: {ref}`channel-types`.\n\n(process-output)=\n\n## Outputs\n\nThe `output` block allows you to define the output channels of a process, similar to function outputs. A process may have at most one output block, and it must contain at least one output.\n\nThe output block follows the syntax shown below:\n\n```\noutput:\n  <output qualifier> <output name> [, <option>: <option value>]\n```\n\nAn output definition consists of a *qualifier* and a *name*. Some optional attributes can also be specified.\n\nWhen a process is invoked, each process output is returned as a channel. The examples provided in the following sections demonstrate how to access the output channels of a process.\n\nThe following output qualifiers are available:\n\n- `val`: Emit the variable with the specified name.\n- `file`: (DEPRECATED) Emit a file produced by the process with the specified name.\n- `path`: Emit a file produced by the process with the specified name.\n- `env`: Emit the variable defined in the process environment with the specified name.\n- `stdout`: Emit the `stdout` of the executed process.\n- `tuple`: Emit multiple values.\n\n### Output type `val`\n\nThe `val` qualifier allows you to output any Nextflow variable defined in the process. A common use case is to output a variable that was defined in the `input` block, as shown in the following example:\n\n```groovy\nprocess foo {\n  input:\n  each x\n\n  output:\n  val x\n\n  \"\"\"\n  echo $x > file\n  \"\"\"\n}\n\nworkflow {\n  methods = ['prot', 'dna', 'rna']\n\n  receiver = foo(methods)\n  receiver.view { \"Received: $it\" }\n}\n```\n\nThe output value can be a value literal, an input variable, any other Nextflow variable in the process scope, or a value expression. For example:\n\n```groovy\nprocess foo {\n  input:\n  path infile\n\n  output:\n  val x\n  val 'BB11'\n  val \"${infile.baseName}.out\"\n\n  script:\n  x = infile.name\n  \"\"\"\n  cat $x > file\n  \"\"\"\n}\n\nworkflow {\n  ch_dummy = Channel.fromPath('*').first()\n  (ch_var, ch_str, ch_exp) = foo(ch_dummy)\n\n  ch_var.view { \"ch_var: $it\" }\n  ch_str.view { \"ch_str: $it\" }\n  ch_exp.view { \"ch_exp: $it\" }\n}\n```\n\n### Output type `file`\n\n:::{note}\nThe `file` qualifier was the standard way to handle input files prior to Nextflow 19.10.0. In later versions of Nextflow, the `path` qualifier should be preferred over `file`.\n:::\n\nThe `file` qualifier is similar to `path`, but with some differences. The `file` qualifier interprets `:` as a path separator, therefore `file 'foo:bar'` captures two files named `foo` and `bar`, whereas `path 'foo:bar'` captures a single file named `foo:bar`. Additionally, `file` does not support all of the extra options provided by `path`.\n\n### Output type `path`\n\nThe `path` qualifier allows you to output one or more files produced by the process. For example:\n\n```groovy\nprocess randomNum {\n  output:\n  path 'result.txt'\n\n  '''\n  echo $RANDOM > result.txt\n  '''\n}\n\nworkflow {\n  numbers = randomNum()\n  numbers.view { \"Received: ${it.text}\" }\n}\n```\n\nIn the above example, the `randomNum` process creates a file named `result.txt` which contains a random number. Since a `path` output with the same name is declared, that file", "start_char_idx": 27931, "end_char_idx": 31129, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "3a4bc64a-71be-4851-aa46-f9163e424db4": {"__data__": {"id_": "3a4bc64a-71be-4851-aa46-f9163e424db4", "embedding": null, "metadata": {"file_path": "docs/process.md", "file_name": "process.md"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "db7a594acaaabbb6b84352a6719c7045b36a31c3", "node_type": null, "metadata": {"file_path": "docs/process.md", "file_name": "process.md"}, "hash": "50cb2af6615db266b968212168f623fa82945be6d88a86b90d19c62bf36f4b94"}, "2": {"node_id": "8e6ad538-2e89-48a3-854c-f7a8abe351ee", "node_type": null, "metadata": {"file_path": "docs/process.md", "file_name": "process.md"}, "hash": "b7e05bbf3c406063477f5621f3610776e62d3c17a3761d90f3edeca1bf2c4ac0"}, "3": {"node_id": "2670d1e3-f2c4-4e6a-a234-545c9ca4f909", "node_type": null, "metadata": {"file_path": "docs/process.md", "file_name": "process.md"}, "hash": "80ee066930a3d04a31c854653f0236cbf71d13cfe38d65889fadba60cb5dc0cb"}}, "hash": "1838bb5ddf818ae8554a704b7e6ea96b4f92fc0cc23c0e793e5bec4fceca7098", "text": "a random number. Since a `path` output with the same name is declared, that file is emitted by the corresponding output channel. A downstream process with a compatible input channel will be able to receive it.\n\nAvailable options:\n\n`followLinks`\n: When `true` target files are return in place of any matching symlink (default: `true`)\n\n`glob`\n: When `true` the specified name is interpreted as a glob pattern (default: `true`)\n\n`hidden`\n: When `true` hidden files are included in the matching output files (default: `false`)\n\n`includeInputs`\n: When `true` any input files matching an output file glob pattern are included.\n\n`maxDepth`\n: Maximum number of directory levels to visit (default: no limit)\n\n`type`\n: Type of paths returned, either `file`, `dir` or `any` (default: `any`, or `file` if the specified file name pattern contains a double star (`**`))\n\n### Multiple output files\n\nWhen an output file name contains a `*` or `?` wildcard character, it is interpreted as a [glob][glob] path matcher. This allows you to capture multiple files into a list and emit the list as a single value. For example:\n\n```groovy\nprocess splitLetters {\n    output:\n    path 'chunk_*'\n\n    '''\n    printf 'Hola' | split -b 1 - chunk_\n    '''\n}\n\nworkflow {\n    splitLetters\n        | flatten\n        | view { \"File: ${it.name} => ${it.text}\" }\n}\n```\n\nIt prints:\n\n```\nFile: chunk_aa => H\nFile: chunk_ab => o\nFile: chunk_ac => l\nFile: chunk_ad => a\n```\n\nBy default, all the files matching the specified glob pattern are emitted as a single list. However, as the above example demonstrates, the {ref}`operator-flatten` operator can be used to transform the list of files into a channel that emits each file individually.\n\nSome caveats on glob pattern behavior:\n\n- Input files are not included (unless `includeInputs` is `true`)\n- Directories are included, unless the `**` pattern is used to recurse through directories\n\n:::{warning}\nAlthough the input files matching a glob output declaration are not included in the resulting output channel, these files may still be transferred from the task scratch directory to the original task work directory. Therefore, to avoid unnecessary file copies, avoid using loose wildcards when defining output files, e.g. `path '*'`. Instead, use a prefix or a suffix to restrict the set of matching files to only the expected ones, e.g. `path 'prefix_*.sorted.bam'`.\n:::\n\nRead more about glob syntax at the following link [What is a glob?][what is a glob?]\n\n### Dynamic output file names\n\nWhen an output file name needs to be expressed dynamically, it is possible to define it using a dynamic string which references variables in the `input` block or in the script global context. For example:\n\n```groovy\nprocess align {\n  input:\n  val species\n  path seq\n\n  output:\n  path \"${species}.aln\"\n\n  \"\"\"\n  t_coffee -in $seq > ${species}.aln\n  \"\"\"\n}\n```\n\nIn the above example, each process execution produces an alignment file whose name depends on the actual value of the `species` input.\n\n:::{tip}\nThe management of output files in Nextflow is often misunderstood.\n\nWith other tools it is generally necessary to organize the output files into some kind of directory structure or to guarantee a unique file name scheme, so that result files don't overwrite each other and so they can be referenced unequivocally by downstream tasks.\n\nWith Nextflow, in most cases, you don't need to manage the naming of", "start_char_idx": 31130, "end_char_idx": 34541, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "2670d1e3-f2c4-4e6a-a234-545c9ca4f909": {"__data__": {"id_": "2670d1e3-f2c4-4e6a-a234-545c9ca4f909", "embedding": null, "metadata": {"file_path": "docs/process.md", "file_name": "process.md"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "db7a594acaaabbb6b84352a6719c7045b36a31c3", "node_type": null, "metadata": {"file_path": "docs/process.md", "file_name": "process.md"}, "hash": "50cb2af6615db266b968212168f623fa82945be6d88a86b90d19c62bf36f4b94"}, "2": {"node_id": "3a4bc64a-71be-4851-aa46-f9163e424db4", "node_type": null, "metadata": {"file_path": "docs/process.md", "file_name": "process.md"}, "hash": "1838bb5ddf818ae8554a704b7e6ea96b4f92fc0cc23c0e793e5bec4fceca7098"}, "3": {"node_id": "c907ebdd-ec57-495c-b258-3bbae9ff432f", "node_type": null, "metadata": {"file_path": "docs/process.md", "file_name": "process.md"}, "hash": "cb73c5c6ba93630f049c2c01eddeb0385d9074c357e2e3b856df03a5d5c1909d"}}, "hash": "80ee066930a3d04a31c854653f0236cbf71d13cfe38d65889fadba60cb5dc0cb", "text": "Nextflow, in most cases, you don't need to manage the naming of output files, because each task is executed in its own unique directory, so files produced by different tasks can't overwrite each other. Also, metadata can be associated with outputs by using the {ref}`tuple output <process-out-tuple>` qualifier, instead of including them in the output file name.\n\nOne example in which you'd need to manage the naming of output files is when you use the `publishDir` directive to have output files also in a specific path of your choice. If two tasks have the same filename for their output and you want them to be in the same path specified by `publishDir`, the last task to finish will overwrite the output of the task that finished before. You can dynamically change that by adding the `saveAs` option to your `publishDir` directive.\n\nTo sum up, the use of output files with static names over dynamic ones is preferable whenever possible, because it will result in simpler and more portable code.\n:::\n\n(process-env)=\n\n### Output type `env`\n\nThe `env` qualifier allows you to output a variable defined in the process execution environment:\n\n```groovy\nprocess myTask {\n    output:\n    env FOO\n\n    script:\n    '''\n    FOO=$(ls -la)\n    '''\n}\n\nworkflow {\n    myTask | view { \"directory contents: $it\" }\n}\n```\n\n(process-stdout)=\n\n### Output type `stdout`\n\nThe `stdout` qualifier allows you to output the `stdout` of the executed process:\n\n```groovy\nprocess sayHello {\n    output:\n    stdout\n\n    \"\"\"\n    echo Hello world!\n    \"\"\"\n}\n\nworkflow {\n    sayHello | view { \"I say... $it\" }\n}\n```\n\n(process-set)=\n\n### Output type `set`\n\n:::{deprecated} 19.08.1-edge\nUse `tuple` instead.\n:::\n\n(process-out-tuple)=\n\n### Output type `tuple`\n\nThe `tuple` qualifier allows you to output multiple values in a single channel. It is useful when you need to associate outputs with metadata, for example:\n\n```groovy\nprocess blast {\n  input:\n    val species\n    path query\n\n  output:\n    tuple val(species), path('result')\n\n  script:\n    \"\"\"\n    blast -db nr -query $query > result\n    \"\"\"\n}\n\nworkflow {\n  ch_species = Channel.from('human', 'cow', 'horse')\n  ch_query = Channel.fromPath('*.fa')\n\n  blast(ch_species, ch_query)\n}\n```\n\nIn the above example, a `blast` task is executed for each pair of `species` and `query` that are received. Each task produces a new tuple containing the value for `species` and the file `result`.\n\nA `tuple` definition may contain any of the following qualifiers, as previously described: `val`, `path`, `env` and `stdout`. Files specified with the `path` qualifier are treated exactly the same as standalone `path` inputs.\n\n:::{note}\nWhile parentheses for input and output qualifiers are generally optional, they are required when specifying elements in an input/output tuple.\n\nHere's an example with a single path output (parentheses optional):\n\n```groovy\nprocess foo {\n    output:\n    path 'result.txt', hidden: true\n\n    '''\n    echo 'another new line' >> result.txt\n    '''\n}\n```\n\nAnd here's an example with a tuple output (parentheses required):\n\n```groovy\nprocess foo {\n    output:\n    tuple path('last_result.txt'), path('result.txt', hidden: true)\n\n    '''\n    echo 'another new line' >>", "start_char_idx": 34556, "end_char_idx": 37763, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "c907ebdd-ec57-495c-b258-3bbae9ff432f": {"__data__": {"id_": "c907ebdd-ec57-495c-b258-3bbae9ff432f", "embedding": null, "metadata": {"file_path": "docs/process.md", "file_name": "process.md"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "db7a594acaaabbb6b84352a6719c7045b36a31c3", "node_type": null, "metadata": {"file_path": "docs/process.md", "file_name": "process.md"}, "hash": "50cb2af6615db266b968212168f623fa82945be6d88a86b90d19c62bf36f4b94"}, "2": {"node_id": "2670d1e3-f2c4-4e6a-a234-545c9ca4f909", "node_type": null, "metadata": {"file_path": "docs/process.md", "file_name": "process.md"}, "hash": "80ee066930a3d04a31c854653f0236cbf71d13cfe38d65889fadba60cb5dc0cb"}, "3": {"node_id": "7dd06df2-2281-4ee7-bffa-637628a09c0c", "node_type": null, "metadata": {"file_path": "docs/process.md", "file_name": "process.md"}, "hash": "d4f79c0735088a4573055d0e6a2e2a3ebded6cde082b29495db20115a7083910"}}, "hash": "cb73c5c6ba93630f049c2c01eddeb0385d9074c357e2e3b856df03a5d5c1909d", "text": "true)\n\n    '''\n    echo 'another new line' >> result.txt\n    echo 'another new line' > last_result.txt\n    '''\n}\n```\n:::\n\n### Optional outputs\n\nIn most cases, a process is expected to produce an output for each output definition. However, there are situations where it is valid for a process to not generate output. In these cases, `optional: true` may be added to the output definition, which tells Nextflow not to fail the process if the declared output is not produced:\n\n```groovy\noutput:\n    path(\"output.txt\"), optional: true\n```\n\nIn this example, the process is normally expected to produce an `output.txt` file, but in the cases where the file is legitimately missing, the process does not fail. The output channel will only contain values for those processes that produce `output.txt`.\n\n## When\n\nThe `when` block allows you to define a condition that must be satisfied in order to execute the process. The condition can be any expression that returns a boolean value.\n\nIt can be useful to enable/disable the process execution depending on the state of various inputs and parameters. For example:\n\n```groovy\nprocess find {\n  input:\n  path proteins\n  val dbtype\n\n  when:\n  proteins.name =~ /^BB11.*/ && dbtype == 'nr'\n\n  script:\n  \"\"\"\n  blastp -query $proteins -db nr\n  \"\"\"\n}\n```\n\n:::{tip}\nAs a best practice, it is better to define such control flow logic in the workflow block, i.e. with an `if` statement or with channel operators, to make the process more portable.\n:::\n\n(process-directives)=\n\n## Directives\n\nDirectives are optional settings that affect the execution of the current process.\n\nThey must be entered at the top of the process body, before any other declaration blocks (`input`, `output`, etc), and have the following syntax:\n\n```\nname value [, value2 [,..]]\n```\n\nSome directives are generally available to all processes, while others depend on the `executor` currently defined.\n\n(process-accelerator)=\n\n### accelerator\n\n:::{versionadded} 19.09.0-edge\n:::\n\nThe `accelerator` directive allows you to request hardware accelerators (e.g. GPUs) for the task execution. For example:\n\n```groovy\nprocess foo {\n    accelerator 4, type: 'nvidia-tesla-k80'\n\n    script:\n    \"\"\"\n    your_gpu_enabled --command --line\n    \"\"\"\n}\n```\n\nThe above examples will request 4 GPUs of type `nvidia-tesla-k80`.\n\n:::{note}\nThis directive is only used by certain executors. Refer to the {ref}`executor-page` page to see which executors support this directive.\n:::\n\n:::{note}\nThe accelerator `type` option depends on the target execution platform. Refer to the platform-specific documentation for details on the available accelerators:\n\n- [Google Cloud](https://cloud.google.com/compute/docs/gpus/)\n- [Kubernetes](https://kubernetes.io/docs/tasks/manage-gpus/scheduling-gpus/#clusters-containing-different-types-of-gpus)\n\nThe accelerator `type` option is not supported for AWS Batch. You can control the accelerator type indirectly through the allowed instance types in your Compute Environment. See the [AWS Batch FAQs](https://aws.amazon.com/batch/faqs/?#GPU_Scheduling_) for more information.\n:::\n\n(process-afterscript)=\n\n### afterScript\n\nThe `afterScript` directive allows you to execute a custom (Bash) snippet immediately *after* the main process has run. This may be useful to clean up your staging area.\n\n:::{note}\nWhen combined", "start_char_idx": 37783, "end_char_idx": 41116, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "7dd06df2-2281-4ee7-bffa-637628a09c0c": {"__data__": {"id_": "7dd06df2-2281-4ee7-bffa-637628a09c0c", "embedding": null, "metadata": {"file_path": "docs/process.md", "file_name": "process.md"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "db7a594acaaabbb6b84352a6719c7045b36a31c3", "node_type": null, "metadata": {"file_path": "docs/process.md", "file_name": "process.md"}, "hash": "50cb2af6615db266b968212168f623fa82945be6d88a86b90d19c62bf36f4b94"}, "2": {"node_id": "c907ebdd-ec57-495c-b258-3bbae9ff432f", "node_type": null, "metadata": {"file_path": "docs/process.md", "file_name": "process.md"}, "hash": "cb73c5c6ba93630f049c2c01eddeb0385d9074c357e2e3b856df03a5d5c1909d"}, "3": {"node_id": "6b96c19c-e8a7-4e6f-bbd4-e7691ddf6117", "node_type": null, "metadata": {"file_path": "docs/process.md", "file_name": "process.md"}, "hash": "2e1412a0acebfd9a76b680e4247a48dac59bb168309ef8682cc4134e5e0a99b3"}}, "hash": "d4f79c0735088a4573055d0e6a2e2a3ebded6cde082b29495db20115a7083910", "text": "useful to clean up your staging area.\n\n:::{note}\nWhen combined with the {ref}`container directive <process-container>`, the `afterScript` will be executed outside the specified container. In other words, the `afterScript` is always executed in the host environment.\n:::\n\n(process-arch)=\n\n### arch\n\nThe `arch` directive allows you to define the CPU architecture to build the software in use by the process' task. For example:\n\n```groovy\nprocess cpu_task {\n    spack 'blast-plus@2.13.0'\n    arch 'linux/x86_64', target: 'cascadelake'\n\n    \"\"\"\n    blastp -query input_sequence -num_threads ${task.cpus}\n    \"\"\"\n}\n```\n\nThe example above declares that the CPU generic architecture is `linux/x86_64` (X86 64 bit), and more specifically that the microarchitecture is `cascadelake` (a specific generation of Intel CPUs).\n\nThis directive is currently used by the following Nextflow functionalities:\n\n- by the [spack](#spack) directive, to build microarchitecture-optimised applications;\n- by the {ref}`wave-page` service, to build containers for one of the generic families of CPU architectures (see below);\n- by the `spack` strategy within {ref}`wave-page`, to optimise the container builds for specific CPU microarchitectures.\n\nAllowed values for the `arch` directive are as follows, grouped by equivalent family (choices available for the sake of compatibility):\n- X86 64 bit: `linux/x86_64`, `x86_64`, `linux/amd64`, `amd64`\n- ARM 64 bit: `linux/aarch64`, `aarch64`, `linux/arm64`, `arm64`, `linux/arm64/v8`\n- ARM 64 bit, older generation: `linux/arm64/v7`\n\nExamples of values for the architecture `target` option are `cascadelake`, `icelake`, `zen2` and `zen3`. See the Spack documentation for the full and up-to-date [list of meaningful targets](https://spack.readthedocs.io/en/latest/basic_usage.html#support-for-specific-microarchitectures).\n\n(process-beforescript)=\n\n### beforeScript\n\nThe `beforeScript` directive allows you to execute a custom (Bash) snippet *before* the main process script is run. This may be useful to initialise the underlying cluster environment or for other custom initialisation.\n\nFor example:\n\n```groovy\nprocess foo {\n  beforeScript 'source /cluster/bin/setup'\n\n  \"\"\"\n  echo bar\n  \"\"\"\n}\n```\n\n:::{note}\nWhen combined with the {ref}`container directive <process-container>`, the `beforeScript` will be executed outside the specified container. In other words, the `beforeScript` is always executed in the host environment.\n:::\n\n(process-cache)=\n\n### cache\n\nThe `cache` directive allows you to store the process results to a local cache. When the cache is enabled *and* the pipeline is launched with the {ref}`resume <getstarted-resume>` option, any following attempt to execute the process, along with the same inputs, will cause the process execution to be skipped, producing the stored data as the actual results.\n\nThe caching feature generates a unique key by indexing the process script and inputs. This key is used to identify univocally the outputs produced by the process execution.\n\nThe cache is enabled by default, you can disable it for a specific process by setting the `cache` directive to `false`. For example:\n\n```groovy\nprocess noCacheThis {\n  cache false\n\n  script:\n  <your command string here>\n}\n```\n\nThe following values are available:\n\n`false`\n: Disable caching.\n\n`true` (default)\n: Enable caching. Cache keys are created indexing input files", "start_char_idx": 41095, "end_char_idx": 44480, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "6b96c19c-e8a7-4e6f-bbd4-e7691ddf6117": {"__data__": {"id_": "6b96c19c-e8a7-4e6f-bbd4-e7691ddf6117", "embedding": null, "metadata": {"file_path": "docs/process.md", "file_name": "process.md"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "db7a594acaaabbb6b84352a6719c7045b36a31c3", "node_type": null, "metadata": {"file_path": "docs/process.md", "file_name": "process.md"}, "hash": "50cb2af6615db266b968212168f623fa82945be6d88a86b90d19c62bf36f4b94"}, "2": {"node_id": "7dd06df2-2281-4ee7-bffa-637628a09c0c", "node_type": null, "metadata": {"file_path": "docs/process.md", "file_name": "process.md"}, "hash": "d4f79c0735088a4573055d0e6a2e2a3ebded6cde082b29495db20115a7083910"}, "3": {"node_id": "91d12dfc-dd9f-4c13-abf6-eeacb0925fa4", "node_type": null, "metadata": {"file_path": "docs/process.md", "file_name": "process.md"}, "hash": "85e6cc4f1b87c5684baa2d8ed3b4e5dd0043f61866f73b690b0c7f689ac7b434"}}, "hash": "2e1412a0acebfd9a76b680e4247a48dac59bb168309ef8682cc4134e5e0a99b3", "text": "(default)\n: Enable caching. Cache keys are created indexing input files meta-data information (name, size and last update timestamp attributes).\n\n`'deep'`\n: Enable caching. Cache keys are created indexing input files content.\n\n`'lenient'`\n: Enable caching. Cache keys are created indexing input files path and size attributes (this policy provides a workaround for incorrect caching invalidation observed on shared file systems due to inconsistent files timestamps).\n\n(process-clusteroptions)=\n\n### clusterOptions\n\nThe `clusterOptions` directive allows the usage of any native configuration option accepted by your cluster submit command. You can use it to request non-standard resources or use settings that are specific to your cluster and not supported out of the box by Nextflow.\n\n:::{note}\nThis directive is only used by grid executors. Refer to the {ref}`executor-page` page to see which executors support this directive.\n:::\n\n(process-conda)=\n\n### conda\n\nThe `conda` directive allows for the definition of the process dependencies using the [Conda](https://conda.io) package manager.\n\nNextflow automatically sets up an environment for the given package names listed by in the `conda` directive. For example:\n\n```groovy\nprocess foo {\n  conda 'bwa=0.7.15'\n\n  '''\n  your_command --here\n  '''\n}\n```\n\nMultiple packages can be specified separating them with a blank space e.g. `bwa=0.7.15 fastqc=0.11.5`. The name of the channel from where a specific package needs to be downloaded can be specified using the usual Conda notation i.e. prefixing the package with the channel name as shown here `bioconda::bwa=0.7.15`.\n\nThe `conda` directive also allows the specification of a Conda environment file path or the path of an existing environment directory. See the {ref}`conda-page` page for further details.\n\n(process-container)=\n\n### container\n\nThe `container` directive allows you to execute the process script in a [Docker](http://docker.io) container.\n\nIt requires the Docker daemon to be running in machine where the pipeline is executed, i.e. the local machine when using the *local* executor or the cluster nodes when the pipeline is deployed through a *grid* executor.\n\nFor example:\n\n```groovy\nprocess runThisInDocker {\n  container 'dockerbox:tag'\n\n  \"\"\"\n  <your holy script here>\n  \"\"\"\n}\n```\n\nSimply replace in the above script `dockerbox:tag` with the name of the Docker image you want to use.\n\n:::{tip}\nContainers are a very useful way to execute your scripts in a reproducible self-contained environment or to run your pipeline in the cloud.\n:::\n\n:::{note}\nThis directive is ignored for processes that are {ref}`executed natively <process-native>`.\n:::\n\n(process-containeroptions)=\n\n### containerOptions\n\nThe `containerOptions` directive allows you to specify any container execution option supported by the underlying container engine (ie. Docker, Singularity, etc). This can be useful to provide container settings only for a specific process e.g. mount a custom path:\n\n```groovy\nprocess runThisWithDocker {\n    container 'busybox:latest'\n    containerOptions '--volume /data/db:/db'\n\n    output:\n    path 'output.txt'\n\n    '''\n    your_command --data /db > output.txt\n    '''\n}\n```\n\n:::{warning}\nThis feature is not supported by the {ref}`k8s-executor` and {ref}`google-lifesciences-executor` executors.\n:::\n\n(process-cpus)=\n\n### cpus\n\nThe `cpus` directive allows you to define the number of (logical) CPU required by the process' task. For", "start_char_idx": 44474, "end_char_idx": 47927, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "91d12dfc-dd9f-4c13-abf6-eeacb0925fa4": {"__data__": {"id_": "91d12dfc-dd9f-4c13-abf6-eeacb0925fa4", "embedding": null, "metadata": {"file_path": "docs/process.md", "file_name": "process.md"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "db7a594acaaabbb6b84352a6719c7045b36a31c3", "node_type": null, "metadata": {"file_path": "docs/process.md", "file_name": "process.md"}, "hash": "50cb2af6615db266b968212168f623fa82945be6d88a86b90d19c62bf36f4b94"}, "2": {"node_id": "6b96c19c-e8a7-4e6f-bbd4-e7691ddf6117", "node_type": null, "metadata": {"file_path": "docs/process.md", "file_name": "process.md"}, "hash": "2e1412a0acebfd9a76b680e4247a48dac59bb168309ef8682cc4134e5e0a99b3"}, "3": {"node_id": "023b4e6a-1f76-4f8b-b548-863c6dc27d3b", "node_type": null, "metadata": {"file_path": "docs/process.md", "file_name": "process.md"}, "hash": "31c85af89dc63bd19e2a577c600c842b8e4c4b52cc42cd1b8c56166403ddfa06"}}, "hash": "85e6cc4f1b87c5684baa2d8ed3b4e5dd0043f61866f73b690b0c7f689ac7b434", "text": "allows you to define the number of (logical) CPU required by the process' task. For example:\n\n```groovy\nprocess big_job {\n  cpus 8\n  executor 'sge'\n\n  \"\"\"\n  blastp -query input_sequence -num_threads ${task.cpus}\n  \"\"\"\n}\n```\n\nThis directive is required for tasks that execute multi-process or multi-threaded commands/tools and it is meant to reserve enough CPUs when a pipeline task is executed through a cluster resource manager.\n\nSee also: [penv](#penv), [memory](#memory), [time](#time), [queue](#queue), [maxForks](#maxforks)\n\n(process-debug)=\n\n### debug\n\nBy default the `stdout` produced by the commands executed in all processes is ignored. By setting the `debug` directive to `true`, you can forward the process `stdout` to the current top running process `stdout` file, showing it in the shell terminal.\n\nFor example:\n\n```groovy\nprocess sayHello {\n  debug true\n\n  script:\n  \"echo Hello\"\n}\n```\n\n```\nHello\n```\n\nWithout specifying `debug true`, you won't see the `Hello` string printed out when executing the above example.\n\n(process-disk)=\n\n### disk\n\nThe `disk` directive allows you to define how much local disk storage the process is allowed to use. For example:\n\n```groovy\nprocess big_job {\n    disk '2 GB'\n    executor 'cirrus'\n\n    \"\"\"\n    your task script here\n    \"\"\"\n}\n```\n\nThe following memory unit suffix can be used when specifying the disk value:\n\n| Unit | Description |\n| ---- | ----------- |\n| B    | Bytes       |\n| KB   | Kilobytes   |\n| MB   | Megabytes   |\n| GB   | Gigabytes   |\n| TB   | Terabytes   |\n\nSee {ref}`implicit-classes-memoryunit` for more information.\n\n:::{note}\nThis directive is only used by certain executors. Refer to the {ref}`executor-page` page to see which executors support this directive.\n:::\n\nSee also: [cpus](#cpus), [memory](#memory) [time](#time), [queue](#queue) and [Dynamic computing resources](#dynamic-computing-resources).\n\n(process-echo)=\n\n### echo\n\n:::{deprecated} 22.04.0\nUse `debug` instead\n:::\n\n(process-error-strategy)=\n\n### errorStrategy\n\nThe `errorStrategy` directive allows you to define how an error condition is managed by the process. By default when an error status is returned by the executed script, the process stops immediately. This in turn forces the entire pipeline to terminate.\n\nThe following error strategies are available:\n\n`terminate` (default)\n: Terminate the execution as soon as an error condition is reported. Pending jobs are killed.\n\n`finish`\n: Initiate an orderly pipeline shutdown when an error condition is raised, waiting for the completion of any submitted jobs.\n\n`ignore`\n: Ignore process execution errors.\n\n`retry`\n: Re-submit any process that returns an error condition.\n\nWhen setting the `errorStrategy` directive to `ignore` the process doesn't stop on an error condition, it just reports a message notifying you of the error event.\n\nFor example:\n\n```groovy\nprocess ignoreAnyError {\n  errorStrategy 'ignore'\n\n  script:\n  <your command string here>\n}\n```\n\n:::{note}\nBy definition, a command script fails when it ends with a non-zero exit status.\n:::\n\nThe `retry` error strategy allows you to re-submit for execution a process returning an error condition. For example:\n\n```groovy\nprocess retryIfFail {\n  errorStrategy 'retry'\n\n  script:\n", "start_char_idx": 47921, "end_char_idx": 51154, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "023b4e6a-1f76-4f8b-b548-863c6dc27d3b": {"__data__": {"id_": "023b4e6a-1f76-4f8b-b548-863c6dc27d3b", "embedding": null, "metadata": {"file_path": "docs/process.md", "file_name": "process.md"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "db7a594acaaabbb6b84352a6719c7045b36a31c3", "node_type": null, "metadata": {"file_path": "docs/process.md", "file_name": "process.md"}, "hash": "50cb2af6615db266b968212168f623fa82945be6d88a86b90d19c62bf36f4b94"}, "2": {"node_id": "91d12dfc-dd9f-4c13-abf6-eeacb0925fa4", "node_type": null, "metadata": {"file_path": "docs/process.md", "file_name": "process.md"}, "hash": "85e6cc4f1b87c5684baa2d8ed3b4e5dd0043f61866f73b690b0c7f689ac7b434"}, "3": {"node_id": "767b9b4f-6546-4376-8122-1aec03371b68", "node_type": null, "metadata": {"file_path": "docs/process.md", "file_name": "process.md"}, "hash": "8210489c05ab74f6bcecdcc080c394de47565ab6f0f20755753ba93c92d89553"}}, "hash": "31c85af89dc63bd19e2a577c600c842b8e4c4b52cc42cd1b8c56166403ddfa06", "text": "retryIfFail {\n  errorStrategy 'retry'\n\n  script:\n  <your command string here>\n}\n```\n\nThe number of times a failing process is re-executed is defined by the [maxRetries](#maxretries) and [maxErrors](#maxerrors) directives.\n\n:::{tip}\nMore complex strategies depending on the task exit status or other parametric values can be defined using a dynamic `errorStrategy`. See the [Dynamic directives](#dynamic-directives) section for details.\n:::\n\nSee also: [maxErrors](#maxerrors), [maxRetries](#maxretries) and [Dynamic computing resources](#dynamic-computing-resources).\n\n(process-executor)=\n\n### executor\n\nThe `executor` defines the underlying system where processes are executed. By default a process uses the executor defined globally in the `nextflow.config` file.\n\nThe `executor` directive allows you to configure what executor has to be used by the process, overriding the default configuration.\n\nThe following executors are available:\n\n| Name                  | Executor                                                                                    |\n| --------------------- | ------------------------------------------------------------------------------------------- |\n| `awsbatch`            | [AWS Batch](https://aws.amazon.com/batch/) service                                          |\n| `azurebatch`          | [Azure Batch](https://azure.microsoft.com/en-us/services/batch/) service                    |\n| `condor`              | [HTCondor](https://research.cs.wisc.edu/htcondor/) job scheduler                            |\n| `google-lifesciences` | [Google Genomics Pipelines](https://cloud.google.com/life-sciences) service                 |\n| `ignite`              | [Apache Ignite](https://ignite.apache.org/) cluster                                         |\n| `k8s`                 | [Kubernetes](https://kubernetes.io/) cluster                                                |\n| `local`               | The computer where `Nextflow` is launched                                                   |\n| `lsf`                 | [Platform LSF](http://en.wikipedia.org/wiki/Platform_LSF) job scheduler                     |\n| `moab`                |", "start_char_idx": 51181, "end_char_idx": 53345, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "767b9b4f-6546-4376-8122-1aec03371b68": {"__data__": {"id_": "767b9b4f-6546-4376-8122-1aec03371b68", "embedding": null, "metadata": {"file_path": "docs/process.md", "file_name": "process.md"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "db7a594acaaabbb6b84352a6719c7045b36a31c3", "node_type": null, "metadata": {"file_path": "docs/process.md", "file_name": "process.md"}, "hash": "50cb2af6615db266b968212168f623fa82945be6d88a86b90d19c62bf36f4b94"}, "2": {"node_id": "023b4e6a-1f76-4f8b-b548-863c6dc27d3b", "node_type": null, "metadata": {"file_path": "docs/process.md", "file_name": "process.md"}, "hash": "31c85af89dc63bd19e2a577c600c842b8e4c4b52cc42cd1b8c56166403ddfa06"}, "3": {"node_id": "0ed1fe50-3b49-4022-b72f-f684715aa191", "node_type": null, "metadata": {"file_path": "docs/process.md", "file_name": "process.md"}, "hash": "afa9c1cd66ef069b8f26da976ae9ea7ef6983399ea6a1e6e77c7b6b2db989a2b"}}, "hash": "8210489c05ab74f6bcecdcc080c394de47565ab6f0f20755753ba93c92d89553", "text": "`moab`                | [Moab](http://www.adaptivecomputing.com/moab-hpc-basic-edition/) job scheduler              |\n| `nqsii`               | [NQSII](https://www.rz.uni-kiel.de/en/our-portfolio/hiperf/nec-linux-cluster) job scheduler |\n| `oge`                 | Alias for the `sge` executor                                                                |\n| `pbs`                 | [PBS/Torque](http://en.wikipedia.org/wiki/Portable_Batch_System) job scheduler              |\n| `pbspro`              | [PBS Pro](https://www.pbsworks.com/) job scheduler                                          |\n| `sge`                 | Sun Grid Engine / [Open Grid Engine](http://gridscheduler.sourceforge.net/)                 |\n| `slurm`               | [SLURM](https://en.wikipedia.org/wiki/Slurm_Workload_Manager) workload manager              |\n| `tes`                 | [GA4GH TES](https://github.com/ga4gh/task-execution-schemas) service                        |\n| `uge`                 | Alias for the `sge` executor                                                                |\n\nThe following example shows how to set the process's executor:\n\n```groovy\nprocess doSomething {\n  executor 'sge'\n\n  script:\n  <your script here>\n}\n```\n\n:::{note}\nEach executor supports additional directives and `executor` configuration options. Refer to the {ref}`executor-page` page to see what each executor supports.\n:::\n\n(process-ext)=\n\n### ext\n\nThe `ext` is a special directive used as *namespace* for user custom process directives. This can be useful for advanced configuration options. For example:\n\n```groovy\nprocess mapping {\n  container \"biocontainers/star:${task.ext.version}\"\n\n  input:\n  path genome\n  tuple val(sampleId), path(reads)\n\n  \"\"\"\n  STAR --genomeDir $genome --readFilesIn $reads\n  \"\"\"\n}\n```\n\nIn the above example, the process uses a container whose version is controlled by the `ext.version` property. This can be defined in the `nextflow.config` file as shown below:\n\n```groovy\nprocess.ext.version = '2.5.3'\n```\n\n(process-fair)=\n\n### fair\n\n:::{versionadded} 22.12.0-edge\n:::\n\nThe `fair` directive, when enabled, guarantees that process outputs will", "start_char_idx": 53382, "end_char_idx": 55534, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "0ed1fe50-3b49-4022-b72f-f684715aa191": {"__data__": {"id_": "0ed1fe50-3b49-4022-b72f-f684715aa191", "embedding": null, "metadata": {"file_path": "docs/process.md", "file_name": "process.md"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "db7a594acaaabbb6b84352a6719c7045b36a31c3", "node_type": null, "metadata": {"file_path": "docs/process.md", "file_name": "process.md"}, "hash": "50cb2af6615db266b968212168f623fa82945be6d88a86b90d19c62bf36f4b94"}, "2": {"node_id": "767b9b4f-6546-4376-8122-1aec03371b68", "node_type": null, "metadata": {"file_path": "docs/process.md", "file_name": "process.md"}, "hash": "8210489c05ab74f6bcecdcc080c394de47565ab6f0f20755753ba93c92d89553"}, "3": {"node_id": "6da8aa39-9c5d-4a3b-8a63-9157b7c1a241", "node_type": null, "metadata": {"file_path": "docs/process.md", "file_name": "process.md"}, "hash": "9e50a1cd5e0b41cab855ab67bf855cd7f40512a08ecfe9cae43e4143a04b293a"}}, "hash": "afa9c1cd66ef069b8f26da976ae9ea7ef6983399ea6a1e6e77c7b6b2db989a2b", "text": "`fair` directive, when enabled, guarantees that process outputs will be emitted in the order in which they were received. For example:\n\n```groovy\nprocess foo {\n    fair true\n\n    input:\n    val x\n    output:\n    tuple val(task.index), val(x)\n\n    script:\n    \"\"\"\n    sleep \\$((RANDOM % 3))\n    \"\"\"\n}\n\nworkflow {\n    channel.of('A','B','C','D') | foo | view\n}\n```\n\nThe above example produces:\n\n```\n[1, A]\n[2, B]\n[3, C]\n[4, D]\n```\n\n(process-label)=\n\n### label\n\nThe `label` directive allows the annotation of processes with mnemonic identifier of your choice. For example:\n\n```groovy\nprocess bigTask {\n  label 'big_mem'\n\n  '''\n  <task script>\n  '''\n}\n```\n\nThe same label can be applied to more than a process and multiple labels can be applied to the same process using the `label` directive more than one time.\n\n:::{note}\nA label must consist of alphanumeric characters or `_`, must start with an alphabetic character and must end with an alphanumeric character.\n:::\n\nLabels are useful to organise workflow processes in separate groups which can be referenced in the configuration file to select and configure subset of processes having similar computing requirements. See the {ref}`config-process-selectors` documentation for details.\n\nSee also: [resourceLabels](#resourcelabels)\n\n(process-machinetype)=\n\n### machineType\n\n:::{versionadded} 19.07.0\n:::\n\nThe `machineType` can be used to specify a predefined Google Compute Platform [machine type](https://cloud.google.com/compute/docs/machine-types) when running using the {ref}`Google Life Sciences <google-lifesciences-executor>` executor.\n\nThis directive is optional and if specified overrides the cpus and memory directives:\n\n```groovy\nprocess foo {\n  machineType 'n1-highmem-8'\n\n  \"\"\"\n  <your script here>\n  \"\"\"\n}\n```\n\nSee also: [cpus](#cpus) and [memory](#memory).\n\n(process-maxerrors)=\n\n### maxErrors\n\nThe `maxErrors` directive allows you to specify the maximum number of times a process can fail when using the `retry` error strategy. By default this directive is disabled, you can set it as shown in the example below:\n\n```groovy\nprocess retryIfFail {\n  errorStrategy 'retry'\n  maxErrors 5\n\n  \"\"\"\n  echo 'do this as that .. '\n  \"\"\"\n}\n```\n\n:::{note}\nThis setting considers the **total** errors accumulated for a given process, across all instances. If you want to control the number of times a process **instance** (aka task) can fail, use `maxRetries`.\n:::\n\nSee also: [errorStrategy](#errorstrategy) and [maxRetries](#maxretries).\n\n(process-maxforks)=\n\n### maxForks\n\nThe `maxForks` directive allows you to define the maximum number of process instances that can be executed in parallel. By default this value is equals to the number of CPU cores available minus 1.\n\nIf you want to execute a process in a sequential manner, set this directive to one. For example:\n\n```groovy\nprocess doNotParallelizeIt {\n  maxForks 1\n\n  '''\n  <your script here>\n  '''\n}\n```\n\n(process-maxretries)=\n\n### maxRetries\n\nThe `maxRetries` directive allows you to define the maximum number of times a process instance can be re-submitted in case of failure. This value is applied only when using the `retry` error strategy. By default only one retry is", "start_char_idx": 55482, "end_char_idx": 58664, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "6da8aa39-9c5d-4a3b-8a63-9157b7c1a241": {"__data__": {"id_": "6da8aa39-9c5d-4a3b-8a63-9157b7c1a241", "embedding": null, "metadata": {"file_path": "docs/process.md", "file_name": "process.md"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "db7a594acaaabbb6b84352a6719c7045b36a31c3", "node_type": null, "metadata": {"file_path": "docs/process.md", "file_name": "process.md"}, "hash": "50cb2af6615db266b968212168f623fa82945be6d88a86b90d19c62bf36f4b94"}, "2": {"node_id": "0ed1fe50-3b49-4022-b72f-f684715aa191", "node_type": null, "metadata": {"file_path": "docs/process.md", "file_name": "process.md"}, "hash": "afa9c1cd66ef069b8f26da976ae9ea7ef6983399ea6a1e6e77c7b6b2db989a2b"}, "3": {"node_id": "adf110de-6ee7-40eb-a6e4-656d82d04f05", "node_type": null, "metadata": {"file_path": "docs/process.md", "file_name": "process.md"}, "hash": "ad0a64dfbe0ddc229a5159e46f7f481b113911cb6692da72d6b1676c6ff6c3bd"}}, "hash": "9e50a1cd5e0b41cab855ab67bf855cd7f40512a08ecfe9cae43e4143a04b293a", "text": "only when using the `retry` error strategy. By default only one retry is allowed, you can increase this value as shown below:\n\n```groovy\nprocess retryIfFail {\n    errorStrategy 'retry'\n    maxRetries 3\n\n    \"\"\"\n    echo 'do this as that .. '\n    \"\"\"\n}\n```\n\n:::{note}\nThere is a subtle but important difference between `maxRetries` and the `maxErrors` directive. The latter defines the total number of errors that are allowed during the process execution (the same process can launch different execution instances), while the `maxRetries` defines the maximum number of times the same process execution can be retried in case of an error.\n:::\n\nSee also: [errorStrategy](#errorstrategy) and [maxErrors](#maxerrors).\n\n(process-memory)=\n\n### memory\n\nThe `memory` directive allows you to define how much memory the process is allowed to use. For example:\n\n```groovy\nprocess big_job {\n    memory '2 GB'\n    executor 'sge'\n\n    \"\"\"\n    your task script here\n    \"\"\"\n}\n```\n\nThe following memory unit suffix can be used when specifying the memory value:\n\n| Unit | Description |\n| ---- | ----------- |\n| B    | Bytes       |\n| KB   | Kilobytes   |\n| MB   | Megabytes   |\n| GB   | Gigabytes   |\n| TB   | Terabytes   |\n\nSee {ref}`implicit-classes-memoryunit` for more information.\n\nSee also: [cpus](#cpus), [time](#time), [queue](#queue) and [Dynamic computing resources](#dynamic-computing-resources).\n\n(process-module)=\n\n### module\n\n[Environment Modules](http://modules.sourceforge.net/) is a package manager that allows you to dynamically configure your execution environment and easily switch between multiple versions of the same software tool.\n\nIf it is available in your system you can use it with Nextflow in order to configure the processes execution environment in your pipeline.\n\nIn a process definition you can use the `module` directive to load a specific module version to be used in the process execution environment. For example:\n\n```groovy\nprocess basicExample {\n  module 'ncbi-blast/2.2.27'\n\n  \"\"\"\n  blastp -query <etc..>\n  \"\"\"\n}\n```\n\nYou can repeat the `module` directive for each module you need to load. Alternatively multiple modules can be specified in a single `module` directive by separating all the module names by using a `:` (colon) character as shown below:\n\n```groovy\n process manyModules {\n\n   module 'ncbi-blast/2.2.27:t_coffee/10.0:clustalw/2.1'\n\n   \"\"\"\n   blastp -query <etc..>\n   \"\"\"\n}\n```\n\n(process-penv)=\n\n### penv\n\nThe `penv` directive allows you to define the parallel environment to be used when submitting a parallel task to the {ref}`SGE <sge-executor>` resource manager. For example:\n\n```groovy\nprocess big_job {\n  cpus 4\n  penv 'smp'\n  executor 'sge'\n\n  \"\"\"\n  blastp -query input_sequence -num_threads ${task.cpus}\n  \"\"\"\n}\n```\n\nThis configuration depends on the parallel environment provided by your grid engine installation. Refer to your cluster documentation or contact your admin to learn more about this.\n\nSee also: [cpus](#cpus), [memory](#memory), [time](#time)\n\n(process-pod)=\n\n### pod\n\nThe `pod` directive allows the definition of pods specific settings, such as environment variables, secrets and config maps when using the {ref}`k8s-executor` executor.\n\nFor example:\n\n```groovy\nprocess your_task", "start_char_idx": 58665, "end_char_idx": 61903, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "adf110de-6ee7-40eb-a6e4-656d82d04f05": {"__data__": {"id_": "adf110de-6ee7-40eb-a6e4-656d82d04f05", "embedding": null, "metadata": {"file_path": "docs/process.md", "file_name": "process.md"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "db7a594acaaabbb6b84352a6719c7045b36a31c3", "node_type": null, "metadata": {"file_path": "docs/process.md", "file_name": "process.md"}, "hash": "50cb2af6615db266b968212168f623fa82945be6d88a86b90d19c62bf36f4b94"}, "2": {"node_id": "6da8aa39-9c5d-4a3b-8a63-9157b7c1a241", "node_type": null, "metadata": {"file_path": "docs/process.md", "file_name": "process.md"}, "hash": "9e50a1cd5e0b41cab855ab67bf855cd7f40512a08ecfe9cae43e4143a04b293a"}, "3": {"node_id": "3ef9a297-63d2-4050-bfb4-3b8a46d1aea4", "node_type": null, "metadata": {"file_path": "docs/process.md", "file_name": "process.md"}, "hash": "5eb2068caa6d9ae9878dfad03f3263988d13df13bfb8561a7f177938ded91170"}}, "hash": "ad0a64dfbe0ddc229a5159e46f7f481b113911cb6692da72d6b1676c6ff6c3bd", "text": "executor.\n\nFor example:\n\n```groovy\nprocess your_task {\n  pod env: 'FOO', value: 'bar'\n\n  '''\n  echo $FOO\n  '''\n}\n```\n\nThe above snippet defines an environment variable named `FOO` which value is `bar`.\n\nWhen defined in the Nextflow configuration file, a pod setting can be defined using the canonical associative array syntax. For example:\n\n```groovy\nprocess {\n  pod = [env: 'FOO', value: 'bar']\n}\n```\n\nWhen more than one setting needs to be provides they must be enclosed in a list definition as shown below:\n\n```groovy\nprocess {\n  pod = [ [env: 'FOO', value: 'bar'], [secret: 'my-secret/key1', mountPath: '/etc/file.txt'] ]\n}\n```\n\nThe `pod` directive supports the following options:\n\n`affinity: <V>`\n: :::{versionadded} 22.01.0-edge\n  :::\n: Specifies affinity for which nodes the process should run on. See [Kubernetes affinity](https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#affinity-and-anti-affinity) for details.\n\n`annotation: <K>, value: <V>`\n: *Can be specified multiple times*\n: Defines a pod annotation with key `K` and value `V`.\n\n`automountServiceAccountToken: <V>`\n: :::{versionadded} 22.01.0-edge\n  :::\n: Specifies whether to [automount service account token](https://kubernetes.io/docs/tasks/configure-pod-container/configure-service-account/) into process pods. If `V` is true, service account token is automounted into task pods (default).\n\n`config: <C/K>, mountPath: </absolute/path>`\n: *Can be specified multiple times*\n: Mounts a [ConfigMap](https://kubernetes.io/docs/tasks/configure-pod-container/configure-pod-configmap/) with name `C` with key `K` to the path `/absolute/path`. When the key component is omitted the path is interpreted as a directory and all the `ConfigMap` entries are exposed in that path.\n\n`csi: <V>, mountPath: </absolute/path>`\n: :::{versionadded} 22.11.0-edge\n  :::\n: *Can be specified multiple times*\n: Mounts a [CSI ephemeral volume](https://kubernetes.io/docs/concepts/storage/ephemeral-volumes/#csi-ephemeral-volumes) with config `V`to the path `/absolute/path`.\n\n`emptyDir: <V>, mountPath: </absolute/path>`\n: :::{versionadded} 22.11.0-edge\n  :::\n: *Can be specified multiple times*\n: Mounts an [emptyDir](https://kubernetes.io/docs/concepts/storage/volumes/#emptydir) with configuration `V` to the path `/absolute/path`.\n\n`env: <E>, config: <C/K>`\n: *Can be specified multiple times*\n: Defines an environment variable with name `E` and whose value is given by the entry associated to the key with name `K` in the [ConfigMap](https://kubernetes.io/docs/tasks/configure-pod-container/configure-pod-configmap/) with name `C`.\n\n`env: <E>, fieldPath: <V>`\n: :::{versionadded} 21.09.1-edge\n  :::\n: *Can be specified multiple times*\n: Defines an environment variable with name `E` and whose value is given by the `V` [field path](https://kubernetes.io/docs/tasks/inject-data-application/environment-variable-expose-pod-information/).\n\n`env: <E>, secret: <S/K>`\n: *Can be specified multiple times*\n:", "start_char_idx": 61914, "end_char_idx": 64884, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "3ef9a297-63d2-4050-bfb4-3b8a46d1aea4": {"__data__": {"id_": "3ef9a297-63d2-4050-bfb4-3b8a46d1aea4", "embedding": null, "metadata": {"file_path": "docs/process.md", "file_name": "process.md"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "db7a594acaaabbb6b84352a6719c7045b36a31c3", "node_type": null, "metadata": {"file_path": "docs/process.md", "file_name": "process.md"}, "hash": "50cb2af6615db266b968212168f623fa82945be6d88a86b90d19c62bf36f4b94"}, "2": {"node_id": "adf110de-6ee7-40eb-a6e4-656d82d04f05", "node_type": null, "metadata": {"file_path": "docs/process.md", "file_name": "process.md"}, "hash": "ad0a64dfbe0ddc229a5159e46f7f481b113911cb6692da72d6b1676c6ff6c3bd"}, "3": {"node_id": "cad50b16-721e-4045-949f-2ec998dc9b3b", "node_type": null, "metadata": {"file_path": "docs/process.md", "file_name": "process.md"}, "hash": "a181be733d03156e1a9bf241df24fccdd7635d8424bec7d3f51b1f4a275f520d"}}, "hash": "5eb2068caa6d9ae9878dfad03f3263988d13df13bfb8561a7f177938ded91170", "text": "secret: <S/K>`\n: *Can be specified multiple times*\n: Defines an environment variable with name `E` and whose value is given by the entry associated to the key with name `K` in the [Secret](https://kubernetes.io/docs/concepts/configuration/secret/) with name `S`.\n\n`env: <E>, value: <V>`\n: *Can be specified multiple times*\n: Defines an environment variable with name `E` and whose value is given by the `V` string.\n\n`imagePullPolicy: <V>`\n: Specifies the strategy to be used to pull the container image e.g. `imagePullPolicy: 'Always'`.\n\n`imagePullSecret: <V>`\n: Specifies the secret name to access a private container image registry. See [Kubernetes documentation](https://kubernetes.io/docs/concepts/containers/images/#specifying-imagepullsecrets-on-a-pod) for details.\n\n`label: <K>, value: <V>`\n: *Can be specified multiple times*\n: Defines a pod label with key `K` and value `V`.\n\n`nodeSelector: <V>`\n: Specifies which node the process will run on. See [Kubernetes nodeSelector](https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#nodeselector) for details.\n\n`priorityClassName: <V>`\n: :::{versionadded} 22.01.0-edge\n  :::\n: Specifies the [priority class name](https://kubernetes.io/docs/concepts/scheduling-eviction/pod-priority-preemption/) for pods.\n\n`privileged: <B>`\n: :::{versionadded} 22.05.0-edge\n  :::\n: Whether the process task should run as a *privileged* container (default: `false`)\n\n`runAsUser: <UID>`\n: Specifies the user ID to be used to run the container. Shortcut for the `securityContext` option.\n\n`secret: <S/K>, mountPath: </absolute/path>`\n: *Can be specified multiple times*\n: Mounts a [Secret](https://kubernetes.io/docs/concepts/configuration/secret/) with name `S` with key `K` to the path `/absolute/path`. When the key component is omitted the path is interpreted as a directory and all the `Secret` entries are exposed in that path.\n\n`securityContext: <V>`\n: Specifies the pod security context. See [Kubernetes security context](https://kubernetes.io/docs/tasks/configure-pod-container/security-context/) for details.\n\n`toleration: <V>`\n: :::{versionadded} 22.04.0\n  :::\n: *Can be specified multiple times*\n: Specifies a toleration for a node taint. See [Taints and Tolerations](https://kubernetes.io/docs/concepts/scheduling-eviction/taint-and-toleration/) for details.\n\n`volumeClaim: <V>, mountPath: </absolute/path>`\n: *Can be specified multiple times*\n: Mounts a [Persistent volume claim](https://kubernetes.io/docs/concepts/storage/persistent-volumes/) with name `V` to the specified path location. Use the optional `subPath` parameter to mount a directory inside the referenced volume instead of its root. The volume may be mounted with `readOnly: true`, but is read/write by default.\n\n(process-publishdir)=\n\n### publishDir\n\nThe `publishDir` directive allows you to publish the process output files to a specified folder. For example:\n\n```groovy\nprocess foo {\n    publishDir '/data/chunks'\n\n    output:\n    path 'chunk_*'\n\n    '''\n    printf 'Hola' | split -b 1 - chunk_\n    '''\n}\n```\n\nThe above example splits the string `Hola` into file chunks of a single", "start_char_idx": 64889, "end_char_idx": 68006, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "cad50b16-721e-4045-949f-2ec998dc9b3b": {"__data__": {"id_": "cad50b16-721e-4045-949f-2ec998dc9b3b", "embedding": null, "metadata": {"file_path": "docs/process.md", "file_name": "process.md"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "db7a594acaaabbb6b84352a6719c7045b36a31c3", "node_type": null, "metadata": {"file_path": "docs/process.md", "file_name": "process.md"}, "hash": "50cb2af6615db266b968212168f623fa82945be6d88a86b90d19c62bf36f4b94"}, "2": {"node_id": "3ef9a297-63d2-4050-bfb4-3b8a46d1aea4", "node_type": null, "metadata": {"file_path": "docs/process.md", "file_name": "process.md"}, "hash": "5eb2068caa6d9ae9878dfad03f3263988d13df13bfb8561a7f177938ded91170"}, "3": {"node_id": "1958a5c9-7550-484c-96a3-942056c58345", "node_type": null, "metadata": {"file_path": "docs/process.md", "file_name": "process.md"}, "hash": "68e380dd0600a7c6e09ce8f4ec8aa6a4f4150a3e839e0a18a6a326755e6c7c8a"}}, "hash": "a181be733d03156e1a9bf241df24fccdd7635d8424bec7d3f51b1f4a275f520d", "text": "above example splits the string `Hola` into file chunks of a single byte. When complete the `chunk_*` output files are published into the `/data/chunks` folder.\n\n:::{note}\nOnly files that match the declaration in the `output` block are published, not all the outputs of the process.\n:::\n\n:::{tip}\nThe `publishDir` directive can be specified more than once in order to publish output files to different target directories based on different rules.\n:::\n\nBy default files are published to the target folder creating a *symbolic link* for each process output that links the file produced into the process working directory. This behavior can be modified using the `mode` option, for example:\n\n```groovy\nprocess foo {\n    publishDir '/data/chunks', mode: 'copy', overwrite: false\n\n    output:\n    path 'chunk_*'\n\n    '''\n    printf 'Hola' | split -b 1 - chunk_\n    '''\n}\n```\n\n:::{warning}\nFiles are copied into the specified directory in an *asynchronous* manner, so they may not be immediately available in the publish directory at the end of the process execution. For this reason, downstream processes should not try to access output files through the publish directory, but through channels.\n:::\n\nAvailable options:\n\n`contentType`\n: :::{versionadded} 22.10.0\n  :::\n: *Experimental: currently only supported for S3.*\n: Allow specifying the media content type of the published file a.k.a. [MIME type](https://developer.mozilla.org/en-US/docs/Web/HTTP/Basics_of_HTTP/MIME_Types). If set to `true`, the content type is inferred from the file extension (default: `false`).\n\n`enabled`\n: Enable or disable the publish rule depending on the boolean value specified (default: `true`).\n\n`failOnError`\n: When `true` abort the execution if some file can't be published to the specified target directory or bucket for any cause (default: `false`)\n\n`mode`\n: The file publishing method. Can be one of the following values:\n\n  - `'copy'`: Copies the output files into the publish directory.\n  - `'copyNoFollow'`: Copies the output files into the publish directory without following symlinks ie. copies the links themselves.\n  - `'link'`: Creates a hard link in the publish directory for each output file.\n  - `'move'`: Moves the output files into the publish directory. **Note**: this is only supposed to be used for a *terminal* process i.e. a process whose output is not consumed by any other downstream process.\n  - `'rellink'`: Creates a relative symbolic link in the publish directory for each output file.\n  - `'symlink'`: Creates an absolute symbolic link in the publish directory for each output file (default).\n\n`overwrite`\n: When `true` any existing file in the specified folder will be overridden (default: `true` during normal pipeline execution and `false` when pipeline execution is `resumed`).\n\n`path`\n: Specifies the directory where files need to be published. **Note**: the syntax `publishDir '/some/dir'` is a shortcut for `publishDir path: '/some/dir'`.\n\n`pattern`\n: Specifies a [glob][glob] file pattern that selects which files to publish from the overall set of output files.\n\n`saveAs`\n: A closure which, given the name of the file being published, returns the actual file name or a full path where the file is required to be stored. This can be used to rename or change the destination directory of the published files dynamically by using a custom strategy. Return the value `null` from the closure to *not* publish a file. This is useful when the process has multiple output files, but you want to publish only some of them.\n\n`storageClass`\n: :::{versionadded}", "start_char_idx": 67997, "end_char_idx": 71565, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "1958a5c9-7550-484c-96a3-942056c58345": {"__data__": {"id_": "1958a5c9-7550-484c-96a3-942056c58345", "embedding": null, "metadata": {"file_path": "docs/process.md", "file_name": "process.md"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "db7a594acaaabbb6b84352a6719c7045b36a31c3", "node_type": null, "metadata": {"file_path": "docs/process.md", "file_name": "process.md"}, "hash": "50cb2af6615db266b968212168f623fa82945be6d88a86b90d19c62bf36f4b94"}, "2": {"node_id": "cad50b16-721e-4045-949f-2ec998dc9b3b", "node_type": null, "metadata": {"file_path": "docs/process.md", "file_name": "process.md"}, "hash": "a181be733d03156e1a9bf241df24fccdd7635d8424bec7d3f51b1f4a275f520d"}, "3": {"node_id": "28409c52-9258-4335-941d-ac167bcaa839", "node_type": null, "metadata": {"file_path": "docs/process.md", "file_name": "process.md"}, "hash": "d442943f51c77f80cbf380fb3941693d60327a1df337de6ee67738c699699615"}}, "hash": "68e380dd0600a7c6e09ce8f4ec8aa6a4f4150a3e839e0a18a6a326755e6c7c8a", "text": "publish only some of them.\n\n`storageClass`\n: :::{versionadded} 22.12.0-edge\n  :::\n: *Experimental: currently only supported for S3.*\n: Allow specifying the storage class to be used for the published file.\n\n`tags`\n: :::{versionadded} 21.12.0-edge\n  :::\n: *Experimental: currently only supported for S3.*\n: Allow the association of arbitrary tags with the published file e.g. `tags: [FOO: 'Hello world']`.\n\n(process-queue)=\n\n### queue\n\nThe `queue` directive allows you to set the `queue` where jobs are scheduled when using a grid based executor in your pipeline. For example:\n\n```groovy\nprocess grid_job {\n    queue 'long'\n    executor 'sge'\n\n    \"\"\"\n    your task script here\n    \"\"\"\n}\n```\n\nMultiple queues can be specified by separating their names with a comma for example:\n\n```groovy\nprocess grid_job {\n    queue 'short,long,cn-el6'\n    executor 'sge'\n\n    \"\"\"\n    your task script here\n    \"\"\"\n}\n```\n\n:::{note}\nThis directive is only used by certain executors. Refer to the {ref}`executor-page` page to see which executors support this directive.\n:::\n\n(process-resourcelabels)=\n\n### resourceLabels\n\n:::{versionadded} 22.09.1-edge\n:::\n\nThe `resourceLabels` directive allows you to specify custom name-value pairs that Nextflow applies to the computing resource used to carry out the process execution. Resource labels can be specified using the syntax shown below:\n\n```groovy\nprocess my_task {\n    resourceLabels region: 'some-region', user: 'some-username'\n\n    '''\n    <task script>\n    '''\n}\n```\n\nThe limits and the syntax of the corresponding cloud provider should be taken into consideration when using resource labels.\n\n:::{note}\nResource labels are currently only supported by the {ref}`awsbatch-executor`, {ref}`google-lifesciences-executor`, Google Cloud Batch and {ref}`k8s-executor` executors.\n:::\n\nSee also: [label](#label)\n\n(process-scratch)=\n\n### scratch\n\nThe `scratch` directive allows you to execute the process in a temporary folder that is local to the execution node.\n\nThis is useful when your pipeline is launched by using a grid executor, because it allows you to decrease the NFS overhead by running the pipeline processes in a temporary directory in the local disk of the actual execution node. Only the files declared as output in the process definition will be copied in the pipeline working area.\n\nIn its basic form simply specify `true` at the directive value, as shown below:\n\n```groovy\nprocess simpleTask {\n  scratch true\n\n  output:\n  path 'data_out'\n\n  '''\n  <task script>\n  '''\n}\n```\n\nBy doing this, it tries to execute the script in the directory defined by the variable `$TMPDIR` in the execution node. If this variable does not exist, it will create a new temporary directory by using the Linux command `mktemp`.\n\n:::{note}\nCloud-based executors use `scratch = true` by default, since the work directory resides in object storage.\n:::\n\nThe following values are supported:\n\n`false`\n: Do not use a scratch directory.\n\n`true`\n: Create a scratch directory in the directory defined by the `$TMPDIR` environment variable, or `$(mktemp /tmp)` if `$TMPDIR` is not set.\n\n`'$YOUR_VAR'`\n: Create a scratch directory in the directory defined by the given environment variable, or `$(mktemp /tmp)` if that variable is not set. The value", "start_char_idx": 71565, "end_char_idx": 74825, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "28409c52-9258-4335-941d-ac167bcaa839": {"__data__": {"id_": "28409c52-9258-4335-941d-ac167bcaa839", "embedding": null, "metadata": {"file_path": "docs/process.md", "file_name": "process.md"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "db7a594acaaabbb6b84352a6719c7045b36a31c3", "node_type": null, "metadata": {"file_path": "docs/process.md", "file_name": "process.md"}, "hash": "50cb2af6615db266b968212168f623fa82945be6d88a86b90d19c62bf36f4b94"}, "2": {"node_id": "1958a5c9-7550-484c-96a3-942056c58345", "node_type": null, "metadata": {"file_path": "docs/process.md", "file_name": "process.md"}, "hash": "68e380dd0600a7c6e09ce8f4ec8aa6a4f4150a3e839e0a18a6a326755e6c7c8a"}, "3": {"node_id": "a3f69744-aecd-4d85-86aa-dc85a3654336", "node_type": null, "metadata": {"file_path": "docs/process.md", "file_name": "process.md"}, "hash": "ec20c2ec767152b77fa204cfc7029c632c6660f46b244e076c307341aec3e347"}}, "hash": "d442943f51c77f80cbf380fb3941693d60327a1df337de6ee67738c699699615", "text": "or `$(mktemp /tmp)` if that variable is not set. The value must use single quotes, otherwise the environment variable will be evaluated in the pipeline script context.\n\n`'/my/tmp/path'`\n: Create a scratch directory in the specified directory.\n\n`'ram-disk'`\n: Create a scratch directory in the RAM disk `/dev/shm/`.\n\n(process-directive-shell)=\n\n### shell\n\nThe `shell` directive allows you to define a custom shell command for process scripts. By default, script blocks are executed with `/bin/bash -ue`.\n\n```groovy\nprocess doMoreThings {\n    shell '/bin/bash', '-euo', 'pipefail'\n\n    '''\n    your_command_here\n    '''\n}\n```\n\nThe same directive could be specified in your Nextflow configuration as follows:\n\n```groovy\nprocess.shell = ['/bin/bash', '-euo', 'pipefail']\n```\n\n(process-spack)=\n\n### spack\n\nThe `spack` directive allows for the definition of the process dependencies using the [Spack](https://spack.io) package manager.\n\nNextflow automatically sets up an environment for the given package names listed by in the `spack` directive. For example:\n\n```groovy\nprocess foo {\n    spack 'bwa@0.7.15'\n\n    '''\n    your_command --here\n    '''\n}\n```\n\nMultiple packages can be specified separating them with a blank space, e.g. `bwa@0.7.15 fastqc@0.11.5`.\n\nThe `spack` directive also allows the specification of a Spack environment file path or the path of an existing environment directory. See the {ref}`spack-page` page for further details.\n\n(process-stageinmode)=\n\n### stageInMode\n\nThe `stageInMode` directive defines how input files are staged into the process work directory. The following values are allowed:\n\n`'copy'`\n: Input files are staged in the process work directory by creating a copy.\n\n`'link'`\n: Input files are staged in the process work directory by creating a hard link for each of them.\n\n`'rellink'`\n: Input files are staged in the process work directory by creating a symbolic link with a relative path for each of them.\n\n`'symlink'`\n: Input files are staged in the process work directory by creating a symbolic link with an absolute path for each of them (default).\n\n(process-stageoutmode)=\n\n### stageOutMode\n\nThe `stageOutMode` directive defines how output files are staged out from the scratch directory to the process work directory. The following values are allowed:\n\n`'copy'`\n: Output files are copied from the scratch directory to the work directory.\n\n`'fcp'`\n: :::{versionadded} 23.02.0-edge\n  :::\n: Output files are copied from the scratch directory to the work directory by using the [fcp](https://github.com/Svetlitski/fcp) utility (note: it must be available in your cluster computing nodes).\n\n`'move'`\n: Output files are moved from the scratch directory to the work directory.\n\n`'rclone'`\n: :::{versionadded} 23.01.0-edge\n  :::\n: Output files are copied from the scratch directory to the work directory by using the [rclone](https://rclone.org) utility (note: it must be available in your cluster computing nodes).\n\n`'rsync'`\n: Output files are copied from the scratch directory to the work directory by using the `rsync` utility.\n\nSee also: [scratch](#scratch).\n\n(process-storedir)=\n\n### storeDir\n\nThe `storeDir` directive allows you to define a directory that is used as a *permanent* cache for your process results.\n\nIn more detail, it affects the process execution in two main", "start_char_idx": 74835, "end_char_idx": 78148, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "a3f69744-aecd-4d85-86aa-dc85a3654336": {"__data__": {"id_": "a3f69744-aecd-4d85-86aa-dc85a3654336", "embedding": null, "metadata": {"file_path": "docs/process.md", "file_name": "process.md"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "db7a594acaaabbb6b84352a6719c7045b36a31c3", "node_type": null, "metadata": {"file_path": "docs/process.md", "file_name": "process.md"}, "hash": "50cb2af6615db266b968212168f623fa82945be6d88a86b90d19c62bf36f4b94"}, "2": {"node_id": "28409c52-9258-4335-941d-ac167bcaa839", "node_type": null, "metadata": {"file_path": "docs/process.md", "file_name": "process.md"}, "hash": "d442943f51c77f80cbf380fb3941693d60327a1df337de6ee67738c699699615"}, "3": {"node_id": "565938cc-1933-4079-bdfc-35ef73169e62", "node_type": null, "metadata": {"file_path": "docs/process.md", "file_name": "process.md"}, "hash": "c76c246ffeda757f42c0cd4a48201cc8ca549259ee7fd0f60c25874a9efb469d"}}, "hash": "ec20c2ec767152b77fa204cfc7029c632c6660f46b244e076c307341aec3e347", "text": "process results.\n\nIn more detail, it affects the process execution in two main ways:\n\n1. The process is executed only if the files declared in the `output` block do not exist in the directory specified by the `storeDir` directive. When the files exist the process execution is skipped and these files are used as the actual process result.\n2. Whenever a process is successfully completed the files listed in the `output` block are moved into the directory specified by the `storeDir` directive.\n\nThe following example shows how to use the `storeDir` directive to create a directory containing a BLAST database for each species specified by an input parameter:\n\n```groovy\nprocess formatBlastDatabases {\n  storeDir '/db/genomes'\n\n  input:\n  path species\n\n  output:\n  path \"${dbName}.*\"\n\n  script:\n  dbName = species.baseName\n  \"\"\"\n  makeblastdb -dbtype nucl -in ${species} -out ${dbName}\n  \"\"\"\n}\n```\n\n:::{warning}\nThe `storeDir` directive is meant for long-term process caching and should not be used to publish output files or organize outputs into a semantic directory structure. In those cases, use the [publishDir](#publishdir) directive instead.\n:::\n\n:::{note}\nThe use of AWS S3 paths is supported, however it requires the installation of the [AWS CLI](https://aws.amazon.com/cli/) (i.e. `aws`) in the target compute node.\n:::\n\n(process-tag)=\n\n### tag\n\nThe `tag` directive allows you to associate each process execution with a custom label, so that it will be easier to identify them in the log file or in the trace execution report. For example:\n\n```groovy\nprocess foo {\n  tag \"$code\"\n\n  input:\n  val code\n\n  \"\"\"\n  echo $code\n  \"\"\"\n}\n\nworkflow {\n  Channel.of('alpha', 'gamma', 'omega') | foo\n}\n```\n\nThe above snippet will print a log similar to the following one, where process names contain the tag value:\n\n```\n[6e/28919b] Submitted process > foo (alpha)\n[d2/1c6175] Submitted process > foo (gamma)\n[1c/3ef220] Submitted process > foo (omega)\n```\n\nSee also {ref}`Trace execution report <trace-report>`\n\n(process-time)=\n\n### time\n\nThe `time` directive allows you to define how long a process is allowed to run. For example:\n\n```groovy\nprocess big_job {\n    time '1h'\n\n    \"\"\"\n    your task script here\n    \"\"\"\n}\n```\n\nThe following time unit suffixes can be used when specifying the duration value:\n\n| Unit                            | Description  |\n| ------------------------------- | ------------ |\n| `ms`, `milli`, `millis`         | Milliseconds |\n| `s`, `sec`, `second`, `seconds` | Seconds      |\n| `m`, `min`, `minute`, `minutes` | Minutes      |\n| `h`, `hour`, `hours`            | Hours        |\n| `d`, `day`, `days`              | Days         |\n\nMultiple units can be used in a single declaration, for example: `'1day 6hours 3minutes 30seconds'`\n\nSee {ref}`implicit-classes-duration` for more information.\n\n:::{note}\nThis directive is only used by certain executors. Refer to the {ref}`executor-page` page to see which executors support this directive.\n:::\n\nSee also: [cpus](#cpus), [memory](#memory),", "start_char_idx": 78130, "end_char_idx": 81146, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "565938cc-1933-4079-bdfc-35ef73169e62": {"__data__": {"id_": "565938cc-1933-4079-bdfc-35ef73169e62", "embedding": null, "metadata": {"file_path": "docs/process.md", "file_name": "process.md"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "db7a594acaaabbb6b84352a6719c7045b36a31c3", "node_type": null, "metadata": {"file_path": "docs/process.md", "file_name": "process.md"}, "hash": "50cb2af6615db266b968212168f623fa82945be6d88a86b90d19c62bf36f4b94"}, "2": {"node_id": "a3f69744-aecd-4d85-86aa-dc85a3654336", "node_type": null, "metadata": {"file_path": "docs/process.md", "file_name": "process.md"}, "hash": "ec20c2ec767152b77fa204cfc7029c632c6660f46b244e076c307341aec3e347"}, "3": {"node_id": "92a3ac49-2d11-4c91-8adb-2f2a3de43477", "node_type": null, "metadata": {"file_path": "docs/process.md", "file_name": "process.md"}, "hash": "f192bb0e7c569df0052a7c3994e27b16cf43e033cf8a6969f8d2b33c8e2fd092"}}, "hash": "c76c246ffeda757f42c0cd4a48201cc8ca549259ee7fd0f60c25874a9efb469d", "text": "also: [cpus](#cpus), [memory](#memory), [queue](#queue) and [Dynamic computing resources](#dynamic-computing-resources).\n\n### Dynamic directives\n\nA directive can be assigned *dynamically*, during the process execution, so that its actual value can be evaluated based on the process inputs.\n\nIn order to be defined in a dynamic manner, the directive's value needs to be expressed using a {ref}`closure <script-closure>`, as in the following example:\n\n```groovy\nprocess foo {\n  executor 'sge'\n  queue { entries > 100 ? 'long' : 'short' }\n\n  input:\n  tuple val(entries), path('data.txt')\n\n  script:\n  \"\"\"\n  < your job here >\n  \"\"\"\n}\n```\n\nIn the above example, the [queue](#queue) directive is evaluated dynamically, depending on the input value `entries`. When it is larger than 100, jobs will be submitted to the `long` queue, otherwise the `short` queue will be used.\n\nAll directives can be assigned a dynamic value except the following:\n\n- [executor](#executor)\n- [label](#label)\n- [maxForks](#maxforks)\n\n:::{tip}\nAssigning a string value with one or more variables is always resolved in a dynamic manner, and therefore is equivalent to the above syntax. For example, the above directive can also be written as:\n\n```groovy\nqueue \"${ entries > 100 ? 'long' : 'short' }\"\n```\n\nNote, however, that the latter syntax can be used both for a directive's main argument (as in the above example) and for a directive's optional named attributes, whereas the closure syntax is only resolved dynamically for a directive's main argument.\n:::\n\n:::{tip}\nYou can retrieve the current value of a dynamic directive in the process script by using the implicit variable `task`, which holds the directive values defined in the current task. For example:\n\n```groovy\nprocess foo {\n  queue { entries > 100 ? 'long' : 'short' }\n\n  input:\n  tuple val(entries), path('data.txt')\n\n  script:\n  \"\"\"\n  echo Current queue: ${task.queue}\n  \"\"\"\n}\n```\n:::\n\n### Dynamic computing resources\n\nIt's a very common scenario that different instances of the same process may have very different needs in terms of computing resources. In such situations requesting, for example, an amount of memory too low will cause some tasks to fail. Instead, using a higher limit that fits all the tasks in your execution could significantly decrease the execution priority of your jobs.\n\nThe [Dynamic directives](#dynamic-directives) evaluation feature can be used to modify the amount of computing resources requested in case of a process failure and try to re-execute it using a higher limit. For example:\n\n```groovy\nprocess foo {\n    memory { 2.GB * task.attempt }\n    time { 1.hour * task.attempt }\n\n    errorStrategy { task.exitStatus in 137..140 ? 'retry' : 'terminate' }\n    maxRetries 3\n\n    script:\n    <your job here>\n}\n```\n\nIn the above example the [memory](#memory) and execution [time](#time) limits are defined dynamically. The first time the process is executed the `task.attempt` is set to `1`, thus it will request a two GB of memory and one hour of maximum execution time.\n\nIf the task execution fail reporting an exit status in the range between 137 and 140, the task is re-submitted (otherwise terminates immediately). This time the value of `task.attempt` is `2`, thus increasing the amount of the memory to four GB and the time to 2 hours, and so on.\n\nThe directive [maxRetries](#maxretries) set the maximum number of time the same task can be re-executed.\n\n### Dynamic Retry with backoff\n\nThere", "start_char_idx": 81177, "end_char_idx": 84639, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "92a3ac49-2d11-4c91-8adb-2f2a3de43477": {"__data__": {"id_": "92a3ac49-2d11-4c91-8adb-2f2a3de43477", "embedding": null, "metadata": {"file_path": "docs/process.md", "file_name": "process.md"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "db7a594acaaabbb6b84352a6719c7045b36a31c3", "node_type": null, "metadata": {"file_path": "docs/process.md", "file_name": "process.md"}, "hash": "50cb2af6615db266b968212168f623fa82945be6d88a86b90d19c62bf36f4b94"}, "2": {"node_id": "565938cc-1933-4079-bdfc-35ef73169e62", "node_type": null, "metadata": {"file_path": "docs/process.md", "file_name": "process.md"}, "hash": "c76c246ffeda757f42c0cd4a48201cc8ca549259ee7fd0f60c25874a9efb469d"}}, "hash": "f192bb0e7c569df0052a7c3994e27b16cf43e033cf8a6969f8d2b33c8e2fd092", "text": "task can be re-executed.\n\n### Dynamic Retry with backoff\n\nThere are cases in which the required execution resources may be temporary unavailable e.g. network congestion. In these cases immediately re-executing the task will likely result in the identical error. A retry with an exponential backoff delay can better recover these error conditions:\n\n```groovy\nprocess foo {\n  errorStrategy { sleep(Math.pow(2, task.attempt) * 200 as long); return 'retry' }\n  maxRetries 5\n\n  script:\n  '''\n  your_command --here\n  '''\n}\n```\n\n[glob]: http://docs.oracle.com/javase/tutorial/essential/io/fileOps.html#glob\n[what is a glob?]: http://docs.oracle.com/javase/tutorial/essential/io/fileOps.html#glob", "start_char_idx": 84613, "end_char_idx": 85301, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "c5c76d6d-dc87-42f6-a4f4-f60200c7089a": {"__data__": {"id_": "c5c76d6d-dc87-42f6-a4f4-f60200c7089a", "embedding": null, "metadata": {"file_path": "docs/script.md", "file_name": "script.md"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "3778126c3f8d6c114a4804ff3e62747973115392", "node_type": null, "metadata": {"file_path": "docs/script.md", "file_name": "script.md"}, "hash": "fda433296dd0f686911ec7c1134cf06b71cc668b7a3655785b3647709f1b8920"}, "3": {"node_id": "5f38ced3-f532-4e1e-8e2e-a157da62bd55", "node_type": null, "metadata": {"file_path": "docs/script.md", "file_name": "script.md"}, "hash": "69fc0e4bceeab64b631d145a2ac2d632781e6630eb49cc07427f52f0fe34fb26"}}, "hash": "abcab50dc176c94019e8004f26e997bc3a157d73e42c78d5390e76e132937066", "text": "(script-page)=\n\n# Scripts\n\nThe Nextflow scripting language is an extension of the Groovy programming language. Groovy is a powerful programming language for the Java virtual machine. The Nextflow syntax has been specialized to ease the writing of computational pipelines in a declarative manner.\n\nNextflow can execute any piece of Groovy code or use any library for the JVM platform.\n\nFor a detailed description of the Groovy programming language, reference these links:\n\n- [Groovy User Guide](http://groovy-lang.org/documentation.html)\n- [Groovy Cheat sheet](http://www.cheat-sheets.org/saved-copy/rc015-groovy_online.pdf)\n- [Groovy in Action](http://www.manning.com/koenig2/)\n\nBelow you can find a crash course in the most important language constructs used in the Nextflow scripting language.\n\n:::{warning}\nNextflow uses UTF-8 as the default character encoding for source files. Make sure to use UTF-8 encoding when editing Nextflow scripts with your preferred text editor.\n:::\n\n:::{warning}\nNextflow scripts have a maximum size of 64 KiB. To avoid this limit for large pipelines, consider moving pipeline components into separate files and including them as modules.\n:::\n\n## Groovy basics\n\n### Hello world\n\nTo print something is as easy as using one of the `print` or `println` methods.\n\n```groovy\nprintln \"Hello, World!\"\n```\n\nThe only difference between the two is that the `println` method implicitly appends a newline character to the printed string.\n\n### Variables\n\nTo define a variable, simply assign a value to it:\n\n```groovy\nx = 1\nprintln x\n\nx = new java.util.Date()\nprintln x\n\nx = -3.1499392\nprintln x\n\nx = false\nprintln x\n\nx = \"Hi\"\nprintln x\n```\n\n### Lists\n\nA List object can be defined by placing the list items in square brackets:\n\n```groovy\nmyList = [1776, -1, 33, 99, 0, 928734928763]\n```\n\nYou can access a given item in the list with square-bracket notation (indexes start at 0):\n\n```groovy\nprintln myList[0]\n```\n\nIn order to get the length of the list use the `size` method:\n\n```groovy\nprintln myList.size()\n```\n\nLearn more about lists:\n\n- [Groovy Lists tutorial](http://groovy-lang.org/groovy-dev-kit.html#Collections-Lists)\n- [Groovy List API](http://docs.groovy-lang.org/latest/html/groovy-jdk/java/util/List.html)\n- [Java List API](https://docs.oracle.com/en/java/javase/11/docs/api/java.base/java/util/List.html)\n\n### Maps\n\nMaps are used to store *associative arrays* (also known as *dictionaries*). They are unordered collections of heterogeneous, named data:\n\n```groovy\nscores = [ \"Brett\":100, \"Pete\":\"Did not finish\", \"Andrew\":86.87934 ]\n```\n\nNote that each of the values stored in the map can be of a different type. `Brett` is an integer, `Pete` is a string, and `Andrew` is a floating-point number.\n\nWe can access the values in a map in two main ways:\n\n```groovy\nprintln scores[\"Pete\"]\nprintln scores.Pete\n```\n\nTo add data to or modify a map, the syntax is similar to adding values to list:\n\n```groovy\nscores[\"Pete\"] = 3\nscores[\"Cedric\"] = 120\n```\n\nYou can also use the `+` operator to add two maps together:\n\n```groovy\nnew_scores = scores + [\"Pete\": 3, \"Cedric\": 120]\n```\n\nWhen adding two maps, the first", "start_char_idx": 0, "end_char_idx": 3135, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "5f38ced3-f532-4e1e-8e2e-a157da62bd55": {"__data__": {"id_": "5f38ced3-f532-4e1e-8e2e-a157da62bd55", "embedding": null, "metadata": {"file_path": "docs/script.md", "file_name": "script.md"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "3778126c3f8d6c114a4804ff3e62747973115392", "node_type": null, "metadata": {"file_path": "docs/script.md", "file_name": "script.md"}, "hash": "fda433296dd0f686911ec7c1134cf06b71cc668b7a3655785b3647709f1b8920"}, "2": {"node_id": "c5c76d6d-dc87-42f6-a4f4-f60200c7089a", "node_type": null, "metadata": {"file_path": "docs/script.md", "file_name": "script.md"}, "hash": "abcab50dc176c94019e8004f26e997bc3a157d73e42c78d5390e76e132937066"}, "3": {"node_id": "191609f6-dcb7-46b3-8759-31889a56726b", "node_type": null, "metadata": {"file_path": "docs/script.md", "file_name": "script.md"}, "hash": "8e614fc462be1e0c3575934522ab43d6c11465f4aa1d43f2a3e5bd36380f4549"}}, "hash": "69fc0e4bceeab64b631d145a2ac2d632781e6630eb49cc07427f52f0fe34fb26", "text": "\"Cedric\": 120]\n```\n\nWhen adding two maps, the first map is copied and then appended with the keys from the second map. Any conflicting keys are overwritten by the second map.\n\n:::{tip}\nAppending an \"update\" map is a safer way to modify maps in Nextflow, specifically when passing maps through channels. This way, any references to the original map elsewhere in the pipeline won't be modified.\n:::\n\nLearn more about maps:\n\n- [Groovy Maps tutorial](http://groovy-lang.org/groovy-dev-kit.html#Collections-Maps)\n- [Groovy Map API](http://docs.groovy-lang.org/latest/html/groovy-jdk/java/util/Map.html)\n- [Java Map API](https://docs.oracle.com/en/java/javase/11/docs/api/java.base/java/util/Map.html)\n\n(script-multiple-assignment)=\n\n### Multiple assignment\n\nAn array or a list object can used to assign to multiple variables at once:\n\n```groovy\n(a, b, c) = [10, 20, 'foo']\nassert a == 10 && b == 20 && c == 'foo'\n```\n\nThe three variables on the left of the assignment operator are initialized by the corresponding item in the list.\n\nRead more about [Multiple assignment](http://www.groovy-lang.org/semantics.html#_multiple_assignment) in the Groovy documentation.\n\n### Conditional Execution\n\nOne of the most important features of any programming language is the ability to execute different code under different conditions. The simplest way to do this is to use the `if` construct:\n\n```groovy\nx = Math.random()\nif( x < 0.5 ) {\n    println \"You lost.\"\n}\nelse {\n    println \"You won!\"\n}\n```\n\n### Strings\n\nStrings can be defined by enclosing text in single or double quotes (`'` or `\"` characters):\n\n```groovy\nprintln \"he said 'cheese' once\"\nprintln 'he said \"cheese!\" again'\n```\n\nStrings can be concatenated with `+`:\n\n```groovy\na = \"world\"\nprint \"hello \" + a + \"\\n\"\n```\n\n(string-interpolation)=\n\n### String interpolation\n\nThere is an important difference between single-quoted and double-quoted strings: Double-quoted strings support variable interpolations, while single-quoted strings do not.\n\nIn practice, double-quoted strings can contain the value of an arbitrary variable by prefixing its name with the `$` character, or the value of any expression by using the `${expression}` syntax, similar to Bash/shell scripts:\n\n```groovy\nfoxtype = 'quick'\nfoxcolor = ['b', 'r', 'o', 'w', 'n']\nprintln \"The $foxtype ${foxcolor.join()} fox\"\n\nx = 'Hello'\nprintln '$x + $y'\n```\n\nThis code prints:\n\n```\nThe quick brown fox\n$x + $y\n```\n\n### Multi-line strings\n\nA block of text that span multiple lines can be defined by delimiting it with triple single or double quotes:\n\n```groovy\ntext = \"\"\"\n    hello there James\n    how are you today?\n    \"\"\"\n```\n\n:::{note}\nLike before, multi-line strings inside double quotes support variable interpolation, while single-quoted multi-line strings do not.\n:::\n\nAs in Bash/shell scripts, terminating a line in a multi-line string with a `\\` character prevents a newline character from separating that line from the one that follows:\n\n```groovy\nmyLongCmdline = \"\"\"\n    blastp \\\n    -in $input_query \\\n    -out $output_file \\\n    -db $blast_database \\\n    -html\n    \"\"\"\n\nresult =", "start_char_idx": 3091, "end_char_idx": 6188, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "191609f6-dcb7-46b3-8759-31889a56726b": {"__data__": {"id_": "191609f6-dcb7-46b3-8759-31889a56726b", "embedding": null, "metadata": {"file_path": "docs/script.md", "file_name": "script.md"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "3778126c3f8d6c114a4804ff3e62747973115392", "node_type": null, "metadata": {"file_path": "docs/script.md", "file_name": "script.md"}, "hash": "fda433296dd0f686911ec7c1134cf06b71cc668b7a3655785b3647709f1b8920"}, "2": {"node_id": "5f38ced3-f532-4e1e-8e2e-a157da62bd55", "node_type": null, "metadata": {"file_path": "docs/script.md", "file_name": "script.md"}, "hash": "69fc0e4bceeab64b631d145a2ac2d632781e6630eb49cc07427f52f0fe34fb26"}, "3": {"node_id": "21988fc6-30c1-4e66-8fd0-3bdaa70ee9ad", "node_type": null, "metadata": {"file_path": "docs/script.md", "file_name": "script.md"}, "hash": "05f7b9bfcafd9df532a22abb0100a8cb66684ba06fabfeba163b53af35f7b07c"}}, "hash": "8e614fc462be1e0c3575934522ab43d6c11465f4aa1d43f2a3e5bd36380f4549", "text": "$blast_database \\\n    -html\n    \"\"\"\n\nresult = myLongCmdline.execute().text\n```\n\nIn the preceding example, `blastp` and its `-in`, `-out`, `-db` and `-html` switches and their arguments are effectively a single line.\n\n(script-regexp)=\n\n### Regular expressions\n\nRegular expressions are the Swiss Army knife of text processing. They provide the programmer with the ability to match and extract patterns from strings.\n\nRegular expressions are available via the `~/pattern/` syntax and the `=~` and `==~` operators.\n\nUse `=~` to check whether a given pattern occurs anywhere in a string:\n\n```groovy\nassert 'foo' =~ /foo/       // return TRUE\nassert 'foobar' =~ /foo/    // return TRUE\n```\n\nUse `==~` to check whether a string matches a given regular expression pattern exactly.\n\n```groovy\nassert 'foo' ==~ /foo/       // return TRUE\nassert 'foobar' ==~ /foo/    // return FALSE\n```\n\nIt is worth noting that the `~` operator creates a Java `Pattern` object from the given string, while the `=~` operator creates a Java `Matcher` object.\n\n```groovy\nx = ~/abc/\nprintln x.class\n// prints java.util.regex.Pattern\n\ny = 'some string' =~ /abc/\nprintln y.class\n// prints java.util.regex.Matcher\n```\n\nRegular expression support is imported from Java. Java's regular expression language and API is documented in the [Pattern](https://docs.oracle.com/en/java/javase/11/docs/api/java.base/java/util/regex/Pattern.html) class.\n\nYou may also be interested in this post: [Groovy: Don't Fear the RegExp](https://web.archive.org/web/20170621185113/http://www.naleid.com/blog/2008/05/19/dont-fear-the-regexp).\n\n#### String replacement\n\nTo replace pattern occurrences in a given string, use the `replaceFirst` and `replaceAll` methods:\n\n```groovy\nx = \"colour\".replaceFirst(/ou/, \"o\")\nprintln x\n// prints: color\n\ny = \"cheesecheese\".replaceAll(/cheese/, \"nice\")\nprintln y\n// prints: nicenice\n```\n\n#### Capturing groups\n\nYou can match a pattern that includes groups. First create a matcher object with the `=~` operator. Then, you can index the matcher object to find the matches: `matcher[0]` returns a list representing the first match of the regular expression in the string. The first list element is the string that matches the entire regular expression, and the remaining elements are the strings that match each group.\n\nHere's how it works:\n\n```groovy\nprogramVersion = '2.7.3-beta'\nm = programVersion =~ /(\\d+)\\.(\\d+)\\.(\\d+)-?(.+)/\n\nassert m[0] == ['2.7.3-beta', '2', '7', '3', 'beta']\nassert m[0][1] == '2'\nassert m[0][2] == '7'\nassert m[0][3] == '3'\nassert m[0][4] == 'beta'\n```\n\nApplying some syntactic sugar, you can do the same in just one line of code:\n\n```groovy\nprogramVersion = '2.7.3-beta'\n(full, major, minor, patch, flavor) = (programVersion =~ /(\\d+)\\.(\\d+)\\.(\\d+)-?(.+)/)[0]\n\nprintln full    // 2.7.3-beta\nprintln major   // 2\nprintln minor   // 7\nprintln patch   // 3\nprintln flavor  // beta\n```\n\n#### Removing part of a string\n\nYou can remove part of a `String`", "start_char_idx": 6199, "end_char_idx": 9155, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "21988fc6-30c1-4e66-8fd0-3bdaa70ee9ad": {"__data__": {"id_": "21988fc6-30c1-4e66-8fd0-3bdaa70ee9ad", "embedding": null, "metadata": {"file_path": "docs/script.md", "file_name": "script.md"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "3778126c3f8d6c114a4804ff3e62747973115392", "node_type": null, "metadata": {"file_path": "docs/script.md", "file_name": "script.md"}, "hash": "fda433296dd0f686911ec7c1134cf06b71cc668b7a3655785b3647709f1b8920"}, "2": {"node_id": "191609f6-dcb7-46b3-8759-31889a56726b", "node_type": null, "metadata": {"file_path": "docs/script.md", "file_name": "script.md"}, "hash": "8e614fc462be1e0c3575934522ab43d6c11465f4aa1d43f2a3e5bd36380f4549"}, "3": {"node_id": "5bdedd96-7a5b-4418-8c1d-ad10f33d2b70", "node_type": null, "metadata": {"file_path": "docs/script.md", "file_name": "script.md"}, "hash": "8cd97738b366848766633e9652c1e3c7458be329900e1e32a2cf8227a0a6ade4"}}, "hash": "05f7b9bfcafd9df532a22abb0100a8cb66684ba06fabfeba163b53af35f7b07c", "text": "Removing part of a string\n\nYou can remove part of a `String` value using a regular expression pattern. The first match found is replaced with an empty String:\n\n```groovy\n// define the regexp pattern\nwordStartsWithGr = ~/(?i)\\s+Gr\\w+/\n\n// apply and verify the result\n('Hello Groovy world!' - wordStartsWithGr) == 'Hello world!'\n('Hi Grails users' - wordStartsWithGr) == 'Hi users'\n```\n\nRemove the first 5-character word from a string:\n\n```groovy\nassert ('Remove first match of 5 letter word' - ~/\\b\\w{5}\\b/) == 'Remove match of 5 letter word'\n```\n\nRemove the first number with its trailing whitespace from a string:\n\n```groovy\nassert ('Line contains 20 characters' - ~/\\d+\\s+/) == 'Line contains characters'\n```\n\n(script-closure)=\n\n### Closures\n\nBriefly, a closure is a block of code that can be passed as an argument to a function. Thus, you can define a chunk of code and then pass it around as if it were a string or an integer.\n\nMore formally, you can create functions that are defined as *first-class objects*.\n\n```groovy\nsquare = { it * it }\n```\n\nThe curly brackets around the expression `it * it` tells the script interpreter to treat this expression as code. The `it` identifier is an implicit variable that represents the value that is passed to the function when it is invoked.\n\nOnce compiled the function object is assigned to the variable `square` as any other variable assignments shown previously. Now we can do something like this:\n\n```groovy\nprintln square(9)\n```\n\nand get the value 81.\n\nThis is not very interesting until we find that we can pass the function `square` as an argument to other functions or methods. Some built-in functions take a function like this as an argument. One example is the `collect` method on lists:\n\n```groovy\n[ 1, 2, 3, 4 ].collect(square)\n```\n\nThis expression says: Create an array with the values 1, 2, 3 and 4, then call its `collect` method, passing in the closure we defined above. The `collect` method runs through each item in the array, calls the closure on the item, then puts the result in a new array, resulting in:\n\n```groovy\n[ 1, 4, 9, 16 ]\n```\n\nFor more methods that you can call with closures as arguments, see the [Groovy GDK documentation](http://docs.groovy-lang.org/latest/html/groovy-jdk/).\n\nBy default, closures take a single parameter called `it`, but you can also create closures with multiple, custom-named parameters. For example, the method `Map.each()` can take a closure with two arguments, to which it binds the `key` and the associated `value` for each key-value pair in the `Map`. Here, we use the obvious variable names `key` and `value` in our closure:\n\n```groovy\nprintMapClosure = { key, value ->\n    println \"$key = $value\"\n}\n\n[ \"Yue\" : \"Wu\", \"Mark\" : \"Williams\", \"Sudha\" : \"Kumari\" ].each(printMapClosure)\n```\n\nPrints:\n\n```\nYue = Wu\nMark = Williams\nSudha = Kumari\n```\n\nA closure has two other important features. First, it can access variables in the scope where it is defined, so that it can interact with them.\n\nSecond, a closure can be defined in an anonymous manner, meaning that it is not given a name, and is defined in the place where it needs to be used.\n\nAs an example showing both these features, see the following code fragment:\n\n```groovy\nmyMap = [\"China\": 1 , \"India\" : 2, \"USA\" : 3]\n\nresult = 0\nmyMap.keySet().each( { result+= myMap[it] }", "start_char_idx": 9141, "end_char_idx": 12474, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "5bdedd96-7a5b-4418-8c1d-ad10f33d2b70": {"__data__": {"id_": "5bdedd96-7a5b-4418-8c1d-ad10f33d2b70", "embedding": null, "metadata": {"file_path": "docs/script.md", "file_name": "script.md"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "3778126c3f8d6c114a4804ff3e62747973115392", "node_type": null, "metadata": {"file_path": "docs/script.md", "file_name": "script.md"}, "hash": "fda433296dd0f686911ec7c1134cf06b71cc668b7a3655785b3647709f1b8920"}, "2": {"node_id": "21988fc6-30c1-4e66-8fd0-3bdaa70ee9ad", "node_type": null, "metadata": {"file_path": "docs/script.md", "file_name": "script.md"}, "hash": "05f7b9bfcafd9df532a22abb0100a8cb66684ba06fabfeba163b53af35f7b07c"}, "3": {"node_id": "bf8d2480-0ae0-4cf0-adf6-7abf6215d9aa", "node_type": null, "metadata": {"file_path": "docs/script.md", "file_name": "script.md"}, "hash": "03920edeeac1a302eb1ddbff58dc6cc4125b05ad27684bd751d4bec4c0d1a39b"}}, "hash": "8cd97738b366848766633e9652c1e3c7458be329900e1e32a2cf8227a0a6ade4", "text": "= 0\nmyMap.keySet().each( { result+= myMap[it] } )\n\nprintln result\n```\n\nLearn more about closures in the [Groovy documentation](http://groovy-lang.org/closures.html)\n\n(implicit-variables)=\n\n## Implicit variables\n\n### Script implicit variables\n\nThe following variables are implicitly defined in the script global execution scope:\n\n`baseDir`\n: :::{deprecated} 20.04.0\n  Use `projectDir` instead\n  :::\n: The directory where the main workflow script is located.\n\n`launchDir`\n: :::{versionadded} 20.04.0\n  :::\n: The directory where the workflow is run.\n\n`moduleDir`\n: :::{versionadded} 20.04.0\n  :::\n: The directory where a module script is located for DSL2 modules or the same as `projectDir` for a non-module script.\n\n`nextflow`\n: Dictionary like object representing nextflow runtime information (see {ref}`metadata-nextflow`).\n\n`params`\n: Dictionary like object holding workflow parameters specifying in the config file or as command line options.\n\n`projectDir`\n: :::{versionadded} 20.04.0\n  :::\n: The directory where the main script is located.\n\n`workDir`\n: The directory where tasks temporary files are created.\n\n`workflow`\n: Dictionary like object representing workflow runtime information (see {ref}`metadata-workflow`).\n\n### Configuration implicit variables\n\nThe following variables are implicitly defined in the Nextflow configuration file:\n\n`baseDir`\n: :::{deprecated} 20.04.0\n  Use `projectDir` instead\n  :::\n: The directory where the main workflow script is located.\n\n`launchDir`\n: :::{versionadded} 20.04.0\n  :::\n: The directory where the workflow is run.\n\n`projectDir`\n: :::{versionadded} 20.04.0\n  :::\n: The directory where the main script is located.\n\n### Process implicit variables\n\nThe following variables are implicitly defined in the `task` object of each process:\n\n`attempt`\n: The current task attempt\n\n`hash`\n: *Available only in `exec:` blocks*\n: The task unique hash ID\n\n`index`\n: The task index (corresponds to `task_id` in the execution trace)\n\n`name`\n: *Available only in `exec:` blocks*\n: The current task name\n\n`process`\n: The current process name\n\n`workDir`\n: *Available only in `exec:` blocks*\n: The task unique directory\n\nThe `task` object also contains the values of all process directives for the given task, which allows you to access these settings at runtime. For examples:\n\n```groovy\nprocess foo {\n  script:\n  \"\"\"\n  some_tool --cpus $task.cpus --mem $task.memory\n  \"\"\"\n}\n```\n\nIn the above snippet the `task.cpus` holds the value for the {ref}`cpus directive<process-cpus>` and the `task.memory` the current value for {ref}`memory directive<process-memory>` depending on the actual setting given in the workflow configuration file.\n\nSee {ref}`Process directives <process-directives>` for details.\n\n(implicit-functions)=\n\n## Implicit functions\n\nThe following functions are available in Nextflow scripts:\n\n`branchCriteria( closure )`\n: Create a branch criteria to use with the {ref}`operator-branch` operator.\n\n`error( message = null )`\n: Throw a script runtime error with an optional error message.\n\n`exit( exitCode = 0, message = null )`\n: :::{deprecated} 22.06.0-edge\n  Use `error()` instead\n  :::\n: Stop the pipeline execution and return an exit code and optional error message.\n\n`file( filePattern, options = [:] )`\n: Get one or more files from", "start_char_idx": 12483, "end_char_idx": 15761, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "bf8d2480-0ae0-4cf0-adf6-7abf6215d9aa": {"__data__": {"id_": "bf8d2480-0ae0-4cf0-adf6-7abf6215d9aa", "embedding": null, "metadata": {"file_path": "docs/script.md", "file_name": "script.md"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "3778126c3f8d6c114a4804ff3e62747973115392", "node_type": null, "metadata": {"file_path": "docs/script.md", "file_name": "script.md"}, "hash": "fda433296dd0f686911ec7c1134cf06b71cc668b7a3655785b3647709f1b8920"}, "2": {"node_id": "5bdedd96-7a5b-4418-8c1d-ad10f33d2b70", "node_type": null, "metadata": {"file_path": "docs/script.md", "file_name": "script.md"}, "hash": "8cd97738b366848766633e9652c1e3c7458be329900e1e32a2cf8227a0a6ade4"}, "3": {"node_id": "9f9fdf29-987f-484d-9f71-984d46de8d01", "node_type": null, "metadata": {"file_path": "docs/script.md", "file_name": "script.md"}, "hash": "132bfd3f8146466fe732cb753e688e056b2183ccc07d7262a3fafb4f00e77fcd"}}, "hash": "03920edeeac1a302eb1ddbff58dc6cc4125b05ad27684bd751d4bec4c0d1a39b", "text": "filePattern, options = [:] )`\n: Get one or more files from a path or glob pattern. Returns a [Path](https://docs.oracle.com/en/java/javase/11/docs/api/java.base/java/nio/file/Path.html) or list of Paths if there are multiple files. See [Files and I/O](#files-and-io).\n\n`files( filePattern, options = [:] )`\n: Convenience method for `file()` that always returns a list.\n\n`groupKey( key, size )`\n: Create a grouping key to use with the {ref}`operator-grouptuple` operator.\n\n`multiMapCriteria( closure )`\n: Create a multi-map criteria to use with the {ref}`operator-multiMap` operator.\n\n`sendMail( params )`\n: Send an email. See {ref}`mail-page`.\n\n`tuple( collection )`\n: Create a tuple object from the given collection.\n\n`tuple( ... args )`\n: Create a tuple object from the given arguments.\n\n(implicit-classes)=\n\n## Implicit classes\n\nThe following classes are imported by default in Nextflow scripts:\n\n- `java.lang.*`\n- `java.util.*`\n- `java.io.*`\n- `java.net.*`\n- `groovy.lang.*`\n- `groovy.util.*`\n- `java.math.BigInteger`\n- `java.math.BigDecimal`\n- `java.nio.file.Path`\n\nAdditionally, Nextflow imports several new classes which are described below.\n\n### Channel\n\nThe `Channel` class provides the channel factory methods. See {ref}`channel-factory` for more information.\n\n(implicit-classes-duration)=\n\n### Duration\n\nA `Duration` represents some duration of time.\n\nYou can create a duration by adding a time unit suffix to an integer, e.g. `1.h`. The following suffixes are available:\n\n| Unit                            | Description  |\n| ------------------------------- | ------------ |\n| `ms`, `milli`, `millis`         | Milliseconds |\n| `s`, `sec`, `second`, `seconds` | Seconds      |\n| `m`, `min`, `minute`, `minutes` | Minutes      |\n| `h`, `hour`, `hours`            | Hours        |\n| `d`, `day`, `days`              | Days         |\n\nYou can also create a duration with `Duration.of()`:\n\n```groovy\n// integer value (milliseconds)\noneSecond = Duration.of(1000)\n\n// simple string value\noneHour = Duration.of('1h')\n\n// complex string value\ncomplexDuration = Duration.of('1day 6hours 3minutes 30seconds')\n```\n\nDurations can be compared like numbers, and they support basic arithmetic operations:\n\n```groovy\na = 1.h\nb = 2.h\n\nassert a < b\nassert a + a == b\nassert b - a == a\nassert a * 2 == b\nassert b / 2 == a\n```\n\nThe following methods are available for a `Duration` object:\n\n`getDays()`, `toDays()`\n: Get the duration value in days (rounded down).\n\n`getHours()`, `toHours()`\n: Get the duration value in hours (rounded down).\n\n`getMillis()`, `toMillis()`\n: Get the duration value in milliseconds.\n\n`getMinutes()`, `toMinutes()`\n: Get the duration value in minutes (rounded down).\n\n`getSeconds()`, `toSeconds()`\n: Get the duration value in seconds (rounded down).\n\n(implicit-classes-memoryunit)=\n\n### MemoryUnit\n\nA `MemoryUnit` represents a quantity of", "start_char_idx": 15756, "end_char_idx": 18611, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "9f9fdf29-987f-484d-9f71-984d46de8d01": {"__data__": {"id_": "9f9fdf29-987f-484d-9f71-984d46de8d01", "embedding": null, "metadata": {"file_path": "docs/script.md", "file_name": "script.md"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "3778126c3f8d6c114a4804ff3e62747973115392", "node_type": null, "metadata": {"file_path": "docs/script.md", "file_name": "script.md"}, "hash": "fda433296dd0f686911ec7c1134cf06b71cc668b7a3655785b3647709f1b8920"}, "2": {"node_id": "bf8d2480-0ae0-4cf0-adf6-7abf6215d9aa", "node_type": null, "metadata": {"file_path": "docs/script.md", "file_name": "script.md"}, "hash": "03920edeeac1a302eb1ddbff58dc6cc4125b05ad27684bd751d4bec4c0d1a39b"}, "3": {"node_id": "0e1ff258-fcaf-4962-92af-fbd7a8475a7a", "node_type": null, "metadata": {"file_path": "docs/script.md", "file_name": "script.md"}, "hash": "21145a4d1efdee493a6b7fa8d1b958ab11ec85f43d5df4dfe60dc2a7fb259526"}}, "hash": "132bfd3f8146466fe732cb753e688e056b2183ccc07d7262a3fafb4f00e77fcd", "text": "MemoryUnit\n\nA `MemoryUnit` represents a quantity of bytes.\n\nYou can create a memory unit by adding a unit suffix to an integer, e.g. `1.GB`. The following suffixes are available:\n\n| Unit | Description |\n| ---- | ----------- |\n| `B`  | Bytes       |\n| `KB` | Kilobytes   |\n| `MB` | Megabytes   |\n| `GB` | Gigabytes   |\n| `TB` | Terabytes   |\n| `PB` | Petabytes   |\n| `EB` | Exabytes    |\n| `ZB` | Zettabytes  |\n\n:::{note}\nTechnically speaking, a kilobyte is equal to 1000 bytes, whereas 1024 bytes is called a \"kibibyte\" and abbreviated as \"KiB\", and so on for the other units. In practice, however, kilobyte is commonly understood to mean 1024 bytes, and Nextflow follows this convention in its implementation as well as this documentation.\n:::\n\nYou can also create a memory unit with `MemoryUnit.of()`:\n\n```groovy\n// integer value (bytes)\noneKilobyte = MemoryUnit.of(1024)\n\n// string value\noneGigabyte = MemoryUnit.of('1 GB')\n```\n\nMemory units can be compared like numbers, and they support basic arithmetic operations:\n\n```groovy\na = 1.GB\nb = 2.GB\n\nassert a < b\nassert a + a == b\nassert b - a == a\nassert a * 2 == b\nassert b / 2 == a\n```\n\nThe following methods are available for a `MemoryUnit` object:\n\n`getBytes()`, `toBytes()`\n: Get the memory value in bytes (B).\n\n`getGiga()`, `toGiga()`\n: Get the memory value in gigabytes (rounded down), where 1 GB = 1024 MB.\n\n`getKilo()`, `toKilo()`\n: Get the memory value in kilobytes (rounded down), where 1 KB = 1024 B.\n\n`getMega()`, `toMega()`\n: Get the memory value in megabytes (rounded down), where 1 MB = 1024 KB.\n\n`toUnit( unit )`\n: Get the memory value in terms of a given unit (rounded down). The unit can be one of: `'B'`, `'KB'`, `'MB'`, `'GB'`, `'TB'`, `'PB'`, `'EB'`, `'ZB'`.\n\n### ValueObject\n\n`ValueObject` is an AST transformation for classes and enums, which simply combines [AutoClone](http://docs.groovy-lang.org/latest/html/gapi/groovy/transform/AutoClone.html) and [Immutable](https://docs.groovy-lang.org/latest/html/gapi/groovy/transform/Immutable.html). It is useful for defining custom \"record\" types.\n\n(script-file-io)=\n\n## Files and I/O\n\n### Opening files\n\nTo access and work with files, use the `file()` method, which returns a file system object given a file path string:\n\n```groovy\nmyFile = file('some/path/to/my_file.file')\n```\n\nThe `file()` method can reference both files and directories, depending on what the string path refers to in the file system.\n\nWhen using the wildcard characters `*`, `?`, `[]` and `{}`, the argument is interpreted as a [glob](http://docs.oracle.com/javase/tutorial/essential/io/fileOps.html#glob) path matcher and the `file()` method returns a list object holding the paths of files whose names match the specified pattern, or an empty list if no match is found:\n\n```groovy\nlistOfFiles = file('some/path/*.fa')\n```\n\n:::{note}\nThe `file()` method does not return a list if only one file is matched. Use the `files()` method to always return a list.\n:::\n\n:::{note}\nA double", "start_char_idx": 18614, "end_char_idx": 21589, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "0e1ff258-fcaf-4962-92af-fbd7a8475a7a": {"__data__": {"id_": "0e1ff258-fcaf-4962-92af-fbd7a8475a7a", "embedding": null, "metadata": {"file_path": "docs/script.md", "file_name": "script.md"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "3778126c3f8d6c114a4804ff3e62747973115392", "node_type": null, "metadata": {"file_path": "docs/script.md", "file_name": "script.md"}, "hash": "fda433296dd0f686911ec7c1134cf06b71cc668b7a3655785b3647709f1b8920"}, "2": {"node_id": "9f9fdf29-987f-484d-9f71-984d46de8d01", "node_type": null, "metadata": {"file_path": "docs/script.md", "file_name": "script.md"}, "hash": "132bfd3f8146466fe732cb753e688e056b2183ccc07d7262a3fafb4f00e77fcd"}, "3": {"node_id": "21bca994-6699-410b-a1a9-b732721bd33b", "node_type": null, "metadata": {"file_path": "docs/script.md", "file_name": "script.md"}, "hash": "23218ecee789a641a8bcbfbcc321a2f86fa4190a3d36c440a3808e3ea13ca03c"}}, "hash": "21145a4d1efdee493a6b7fa8d1b958ab11ec85f43d5df4dfe60dc2a7fb259526", "text": "method to always return a list.\n:::\n\n:::{note}\nA double asterisk (`**`) in a glob pattern works like `*` but also searches through subdirectories.\n:::\n\nBy default, wildcard characters do not match directories or hidden files. For example, if you want to include hidden files in the result list, enable the `hidden` option:\n\n```groovy\nlistWithHidden = file('some/path/*.fa', hidden: true)\n```\n\n:::{note}\nTo compose paths, instead of string interpolation, use the `resolve()` method or the `/` operator:\n\n```groovy\ndef dir = file('s3://bucket/some/data/path')\ndef sample1 = dir.resolve('sample.bam')         // correct\ndef sample2 = dir / 'sample.bam'\ndef sample3 = file(\"$dir/sample.bam\")           // correct (but verbose)\ndef sample4 = \"$dir/sample.bam\"                 // incorrect\n```\n:::\n\nThe following options are available:\n\n`checkIfExists`\n: When `true`, throws an exception if the specified path does not exist in the file system (default: `false`)\n\n`followLinks`\n: When `true`, follows symbolic links when traversing a directory tree, otherwise treats them as files (default: `true`)\n\n`glob`\n: When `true`, interprets characters `*`, `?`, `[]` and `{}` as glob wildcards, otherwise handles them as normal characters (default: `true`)\n\n`hidden`\n: When `true`, includes hidden files in the resulting paths (default: `false`)\n\n`maxDepth`\n: Maximum number of directory levels to visit (default: *no limit*)\n\n`type`\n: Type of paths returned, can be `'file'`, `'dir'` or `'any'` (default: `'file'`)\n\nSee also: {ref}`Channel.fromPath <channel-path>`.\n\n### Getting file attributes\n\nThe `file()` method returns a [Path](https://docs.oracle.com/en/java/javase/11/docs/api/java.base/java/nio/file/Path.html), so any method defined for Path can also be used in a Nextflow script.\n\nAdditionally, the following methods are also defined for Paths in Nextflow:\n\n`exists()`\n: Returns `true` if the file exists.\n\n`getBaseName()`\n: Gets the file name without its extension, e.g. `/some/path/file.tar.gz` -> `file.tar`.\n\n`getExtension()`\n: Gets the file extension, e.g. `/some/path/file.txt` -> `txt`.\n\n`getName()`\n: Gets the file name, e.g. `/some/path/file.txt` -> `file.txt`.\n\n`getSimpleName()`\n: Gets the file name without any extension, e.g. `/some/path/file.tar.gz` -> `file`.\n\n`getParent()`\n: Gets the file parent path, e.g. `/some/path/file.txt` -> `/some/path`.\n\n`getScheme()`\n: Gets the file URI scheme, e.g. `s3://some-bucket/foo.txt` -> `s3`.\n\n`isDirectory()`\n: Returns `true` if the file is a directory.\n\n`isEmpty()`\n: Returns `true` if the file is empty or does not exist.\n\n`isFile()`\n: Returns `true` if it is a regular file (i.e. not a directory).\n\n`isHidden()`\n: Returns `true` if the file is hidden.\n\n`isLink()`\n: Returns `true` if the file is a symbolic link.\n\n`lastModified()`\n: Returns the file last modified", "start_char_idx": 21587, "end_char_idx": 24405, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "21bca994-6699-410b-a1a9-b732721bd33b": {"__data__": {"id_": "21bca994-6699-410b-a1a9-b732721bd33b", "embedding": null, "metadata": {"file_path": "docs/script.md", "file_name": "script.md"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "3778126c3f8d6c114a4804ff3e62747973115392", "node_type": null, "metadata": {"file_path": "docs/script.md", "file_name": "script.md"}, "hash": "fda433296dd0f686911ec7c1134cf06b71cc668b7a3655785b3647709f1b8920"}, "2": {"node_id": "0e1ff258-fcaf-4962-92af-fbd7a8475a7a", "node_type": null, "metadata": {"file_path": "docs/script.md", "file_name": "script.md"}, "hash": "21145a4d1efdee493a6b7fa8d1b958ab11ec85f43d5df4dfe60dc2a7fb259526"}, "3": {"node_id": "92288b6a-332b-4c21-804a-68e1301d538f", "node_type": null, "metadata": {"file_path": "docs/script.md", "file_name": "script.md"}, "hash": "424ad83bb050f512193668bd71cccfc3168e4cbc424685ca9fba773727440319"}}, "hash": "23218ecee789a641a8bcbfbcc321a2f86fa4190a3d36c440a3808e3ea13ca03c", "text": "symbolic link.\n\n`lastModified()`\n: Returns the file last modified timestamp in Unix time (i.e. milliseconds since January 1, 1970).\n\n`size()`\n: Gets the file size in bytes.\n\n`toUriString()`\n: Gets the file path along with the protocol scheme:\n  ```groovy\n  def ref = file('s3://some-bucket/foo.txt')\n\n  assert ref.toString() == '/some-bucket/foo.txt'\n  assert \"$ref\" == '/some-bucket/foo.txt'\n  assert ref.toUriString() == 's3://some-bucket/foo.txt'\n  ```\n\n:::{tip}\nIn Groovy, any method that looks like `get*()` can also be accessed as a field. For example, `myFile.getName()` is equivalent to `myFile.name`, `myFile.getBaseName()` is equivalent to `myFile.baseName`, and so on.\n:::\n\n### Reading and writing\n\n#### Reading and writing an entire file\n\nGiven a file variable, created with the `file()` method as shown previously, reading a file is as easy as getting the file's `text` property, which returns the file content as a string:\n\n```groovy\nprint myFile.text\n```\n\nSimilarly, you can save a string to a file by assigning it to the file's `text` property:\n\n```groovy\nmyFile.text = 'Hello world!'\n```\n\nBinary data can managed in the same way, just using the file property `bytes` instead of `text`. Thus, the following example reads the file and returns its content as a byte array:\n\n```groovy\nbinaryContent = myFile.bytes\n```\n\nOr you can save a byte array to a file:\n\n```groovy\nmyFile.bytes = binaryContent\n```\n\n:::{note}\nThe above assignment overwrites any existing file contents, and implicitly creates the file if it doesn't exist.\n:::\n\n:::{warning}\nThe above methods read and write the **entire** file contents at once, in a single variable or buffer. For this reason, when dealing with large files it is recommended that you use a more memory efficient approach, such as reading/writing a file line by line or using a fixed size buffer.\n:::\n\n#### Appending to a file\n\nIn order to append a string value to a file without erasing existing content, you can use the `append()` method:\n\n```groovy\nmyFile.append('Add this line\\n')\n```\n\nOr use the left shift operator, a more idiomatic way to append text content to a file:\n\n```groovy\nmyFile << 'Add a line more\\n'\n```\n\n#### Reading a file line by line\n\nIn order to read a text file line by line you can use the method `readLines()` provided by the file object, which returns the file content as a list of strings:\n\n```groovy\nmyFile = file('some/my_file.txt')\nallLines = myFile.readLines()\nfor( line : allLines ) {\n    println line\n}\n```\n\nThis can also be written in a more idiomatic syntax:\n\n```groovy\nfile('some/my_file.txt')\n    .readLines()\n    .each { println it }\n```\n\n:::{warning}\nThe method `readLines()` reads the **entire** file at once and returns a list containing all the lines. For this reason, do not use it to read big files.\n:::\n\nTo process a big file, use the method `eachLine()`, which reads only a single line at a time into memory:\n\n```groovy\ncount = 0\nmyFile.eachLine { str ->\n    println \"line ${count++}: $str\"\n}\n```\n\n#### Advanced file reading\n\nThe classes `Reader` and `InputStream` provide fine-grained control for reading text and binary files, respectively.\n\nThe method `newReader()` creates a", "start_char_idx": 24396, "end_char_idx": 27571, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "92288b6a-332b-4c21-804a-68e1301d538f": {"__data__": {"id_": "92288b6a-332b-4c21-804a-68e1301d538f", "embedding": null, "metadata": {"file_path": "docs/script.md", "file_name": "script.md"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "3778126c3f8d6c114a4804ff3e62747973115392", "node_type": null, "metadata": {"file_path": "docs/script.md", "file_name": "script.md"}, "hash": "fda433296dd0f686911ec7c1134cf06b71cc668b7a3655785b3647709f1b8920"}, "2": {"node_id": "21bca994-6699-410b-a1a9-b732721bd33b", "node_type": null, "metadata": {"file_path": "docs/script.md", "file_name": "script.md"}, "hash": "23218ecee789a641a8bcbfbcc321a2f86fa4190a3d36c440a3808e3ea13ca03c"}, "3": {"node_id": "35041c69-67b4-46b1-9093-01f260631494", "node_type": null, "metadata": {"file_path": "docs/script.md", "file_name": "script.md"}, "hash": "b9a461dbcdd1761b287adb08d14e95348ab24aceb4afd5398f57b1db1236d9cf"}}, "hash": "424ad83bb050f512193668bd71cccfc3168e4cbc424685ca9fba773727440319", "text": "text and binary files, respectively.\n\nThe method `newReader()` creates a [Reader](https://docs.oracle.com/en/java/javase/11/docs/api/java.base/java/io/Reader.html) object for the given file that allows you to read the content as single characters, lines or arrays of characters:\n\n```groovy\nmyReader = myFile.newReader()\nString line\nwhile( line = myReader.readLine() ) {\n    println line\n}\nmyReader.close()\n```\n\nThe method `withReader()` works similarly, but automatically calls the `close()` method for you when you have finished processing the file. So, the previous example can be written more simply as:\n\n```groovy\nmyFile.withReader {\n    String line\n    while( line = it.readLine() ) {\n        println line\n    }\n}\n```\n\nThe methods `newInputStream()` and `withInputStream()` work similarly. The main difference is that they create an [InputStream](https://docs.oracle.com/en/java/javase/11/docs/api/java.base/java/io/InputStream.html) object useful for writing binary data.\n\nThe following methods are useful for reading files:\n\n`eachByte( closure )`\n: Iterates over the file byte by byte, applying the specified {ref}`closure <script-closure>`.\n\n`eachLine( closure )`\n: Iterates over the file line by line, applying the specified {ref}`closure <script-closure>`.\n\n`getBytes()`\n: Returns the file content as a byte array.\n\n`getText()`\n: Returns the file content as a string value.\n\n`newInputStream()`\n: Returns an [InputStream](https://docs.oracle.com/en/java/javase/11/docs/api/java.base/java/io/InputStream.html) object to read a binary file.\n\n`newReader()`\n: Returns a [Reader](https://docs.oracle.com/en/java/javase/11/docs/api/java.base/java/io/Reader.html) object to read a text file.\n\n`readLines()`\n: Reads the file line by line and returns the content as a list of strings.\n\n`withInputStream( closure )`\n: Opens a file for reading and lets you access it with an [InputStream](https://docs.oracle.com/en/java/javase/11/docs/api/java.base/java/io/InputStream.html) object.\n\n`withReader( closure )`\n: Opens a file for reading and lets you access it with a [Reader](https://docs.oracle.com/en/java/javase/11/docs/api/java.base/java/io/Reader.html) object.\n\n#### Advanced file writing\n\nThe `Writer` and `OutputStream` classes provide fine-grained control for writing text and binary files, respectively, including low-level operations for single characters or bytes, and support for big files.\n\nFor example, given two file objects `sourceFile` and `targetFile`, the following code copies the first file's content into the second file, replacing all `U` characters with `X`:\n\n```groovy\nsourceFile.withReader { source ->\n    targetFile.withWriter { target ->\n        String line\n        while( line=source.readLine() ) {\n            target << line.replaceAll('U','X')\n        }\n    }\n}\n```\n\nThe following methods are available for writing to files:\n\n`append( text )`\n: Appends a string value to a file without replacing existing content.\n\n`newOutputStream()`\n: Creates an [OutputStream](https://docs.oracle.com/en/java/javase/11/docs/api/java.base/java/io/OutputStream.html) object that allows you to write binary", "start_char_idx": 27567, "end_char_idx": 30684, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "35041c69-67b4-46b1-9093-01f260631494": {"__data__": {"id_": "35041c69-67b4-46b1-9093-01f260631494", "embedding": null, "metadata": {"file_path": "docs/script.md", "file_name": "script.md"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "3778126c3f8d6c114a4804ff3e62747973115392", "node_type": null, "metadata": {"file_path": "docs/script.md", "file_name": "script.md"}, "hash": "fda433296dd0f686911ec7c1134cf06b71cc668b7a3655785b3647709f1b8920"}, "2": {"node_id": "92288b6a-332b-4c21-804a-68e1301d538f", "node_type": null, "metadata": {"file_path": "docs/script.md", "file_name": "script.md"}, "hash": "424ad83bb050f512193668bd71cccfc3168e4cbc424685ca9fba773727440319"}, "3": {"node_id": "0cecd8ba-77b1-4051-98a7-01627b961b02", "node_type": null, "metadata": {"file_path": "docs/script.md", "file_name": "script.md"}, "hash": "eb3ece60962a2d6c380b6c43fe1549fc3ef471a41539657879908d5fde55a626"}}, "hash": "b9a461dbcdd1761b287adb08d14e95348ab24aceb4afd5398f57b1db1236d9cf", "text": "object that allows you to write binary data to a file.\n\n`newPrintWriter()`\n: Creates a [PrintWriter](https://docs.oracle.com/en/java/javase/11/docs/api/java.base/java/io/PrintWriter.html) object that allows you to write formatted text to a file.\n\n`newWriter()`\n: Creates a [Writer](https://docs.oracle.com/en/java/javase/11/docs/api/java.base/java/io/Writer.html) object that allows you to save text data to a file.\n\n`setBytes( bytes )`\n: Writes a byte array to a file. Equivalent to setting the `bytes` property.\n\n`setText( text )`\n: Writes a string value to a file. Equivalent to setting the `text` property.\n\n`withOutputStream( closure )`\n: Applies the specified closure to an [OutputStream](https://docs.oracle.com/en/java/javase/11/docs/api/java.base/java/io/OutputStream.html) object, closing it when finished.\n\n`withPrintWriter( closure )`\n: Applies the specified closure to a [PrintWriter](https://docs.oracle.com/en/java/javase/11/docs/api/java.base/java/io/PrintWriter.html) object, closing it when finished.\n\n`withWriter( closure )`\n: Applies the specified closure to a [Writer](https://docs.oracle.com/en/java/javase/11/docs/api/java.base/java/io/Writer.html) object, closing it when finished.\n\n`write( text )`\n: Writes a string to a file, replacing any existing content.\n\n### Filesystem operations\n\nThe following methods are available for manipulating files and directories in a filesystem:\n\n`copyTo( target )`\n: Copies a source file or directory to a target file or directory.\n\n: *When copying a file to another file:* if the target file already exists, it will be replaced.\n\n  ```groovy\n  file('/some/path/my_file.txt').copyTo('/another/path/new_file.txt')\n  ```\n\n: *When copying a file to a directory:* the file will be copied into the directory, replacing any file with the same name.\n\n  ```groovy\n  file('/some/path/my_file.txt').copyTo('/another/path')\n  ```\n\n: *When copying a directory to another directory:* if the target directory already exists, the source directory will be copied into the target directory, replacing any sub-directory with the same name. If the target path does not exist, it will be created automatically.\n\n  ```groovy\n  file('/any/dir_a').moveTo('/any/dir_b')\n  ```\n\n  The result of the above example depends on the existence of the target directory. If the target directory exists, the source is moved into the target directory, resulting in the path `/any/dir_b/dir_a`. If the target directory does not exist, the source is just renamed to the target name, resulting in the path `/any/dir_b`.\n\n: :::{note}\n  The `copyTo()` method follows the semantics of the Linux command `cp -r <source> <target>`, with the following caveat: while Linux tools often treat paths ending with a slash (e.g. `/some/path/name/`) as directories, and those not (e.g. `/some/path/name`) as regular files, Nextflow (due to its use of the Java files API) views both of these paths as the same file system object. If the path exists, it is handled according to its actual type (i.e. as a regular file or as a directory). If the path does not exist, it is treated as a regular file, with any missing parent directories created automatically.\n  :::\n\n`delete()`\n: Deletes the file or directory at the given path, returning `true` if the operation succeeds, and `false`", "start_char_idx": 30717, "end_char_idx": 34003, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "0cecd8ba-77b1-4051-98a7-01627b961b02": {"__data__": {"id_": "0cecd8ba-77b1-4051-98a7-01627b961b02", "embedding": null, "metadata": {"file_path": "docs/script.md", "file_name": "script.md"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "3778126c3f8d6c114a4804ff3e62747973115392", "node_type": null, "metadata": {"file_path": "docs/script.md", "file_name": "script.md"}, "hash": "fda433296dd0f686911ec7c1134cf06b71cc668b7a3655785b3647709f1b8920"}, "2": {"node_id": "35041c69-67b4-46b1-9093-01f260631494", "node_type": null, "metadata": {"file_path": "docs/script.md", "file_name": "script.md"}, "hash": "b9a461dbcdd1761b287adb08d14e95348ab24aceb4afd5398f57b1db1236d9cf"}, "3": {"node_id": "d0141c92-fc27-4d55-b4d1-94076acfdce8", "node_type": null, "metadata": {"file_path": "docs/script.md", "file_name": "script.md"}, "hash": "4cc12e1fb57c037daa9314c6c055de138b5ece7700c078b8b9b6f11a342dee15"}}, "hash": "eb3ece60962a2d6c380b6c43fe1549fc3ef471a41539657879908d5fde55a626", "text": "given path, returning `true` if the operation succeeds, and `false` otherwise:\n\n  ```groovy\n  myFile = file('some/file.txt')\n  result = myFile.delete()\n  println result ? \"OK\" : \"Cannot delete: $myFile\"\n  ```\n\n  If a directory is not empty, it will not be deleted and `delete()` will return `false`.\n\n`deleteDir()`\n: Deletes a directory and all of its contents.\n\n  ```groovy\n  file('any/path').deleteDir()\n  ```\n\n`getPermissions()`\n: Returns a file's permissions using the [symbolic notation](http://en.wikipedia.org/wiki/File_system_permissions#Symbolic_notation), e.g. `'rw-rw-r--'`.\n\n`list()`\n: Returns the first-level elements (files and directories) of a directory as a list of strings.\n\n`listFiles()`\n: Returns the first-level elements (files and directories) of a directory as a list of Paths.\n\n`mkdir()`\n: Creates a directory at the given path, returning `true` if the directory is created successfully, and `false` otherwise:\n\n  ```groovy\n  myDir = file('any/path')\n  result = myDir.mkdir()\n  println result ? \"OK\" : \"Cannot create directory: $myDir\"\n  ```\n\n  If the parent directories do not exist, the directory will not be created and `mkdir()` will return `false`.\n\n`mkdirs()`\n: Creates a directory at the given path, including any nonexistent parent directories:\n\n  ```groovy\n  file('any/path').mkdirs()\n  ```\n\n`mklink( linkName, options = [:] )`\n: Creates a *filesystem link* to a given path:\n\n  ```groovy\n  myFile = file('/some/path/file.txt')\n  myFile.mklink('/user/name/link-to-file.txt')\n  ```\n\n  Available options:\n\n  `hard`\n  : When `true`, creates a *hard* link, otherwise creates a *soft* (aka *symbolic*) link (default: `false`).\n\n  `overwrite`\n  : When `true`, overwrites any existing file with the same name, otherwise throws a [FileAlreadyExistsException](https://docs.oracle.com/en/java/javase/11/docs/api/java.base/java/nio/file/FileAlreadyExistsException.html) (default: `false`).\n\n`moveTo( target )`\n: Moves a source file or directory to a target file or directory. Follows the same semantics as `copyTo()`.\n\n`renameTo( target )`\n: Rename a file or directory:\n\n  ```groovy\n  file('my_file.txt').renameTo('new_file_name.txt')\n  ```\n\n`setPermissions( permissions )`\n: Sets a file's permissions using the [symbolic notation](http://en.wikipedia.org/wiki/File_system_permissions#Symbolic_notation):\n\n  ```groovy\n  myFile.setPermissions('rwxr-xr-x')\n  ```\n\n`setPermissions( owner, group, other )`\n: Sets a file's permissions using the [numeric notation](http://en.wikipedia.org/wiki/File_system_permissions#Numeric_notation), i.e. as three digits representing the **owner**, **group**, and **other** permissions:\n\n  ```groovy\n  myFile.setPermissions(7,5,5)\n  ```\n\n#### Listing directories\n\nThe simplest way to list a directory is to use `list()` or `listFiles()`, which return a collection of first-level elements (files and directories) of a directory:\n\n```groovy\nfor( def file : file('any/path').list() ) {\n    println file\n}\n```\n\nAdditionally, the `eachFile()` method allows you to", "start_char_idx": 33978, "end_char_idx": 36988, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "d0141c92-fc27-4d55-b4d1-94076acfdce8": {"__data__": {"id_": "d0141c92-fc27-4d55-b4d1-94076acfdce8", "embedding": null, "metadata": {"file_path": "docs/script.md", "file_name": "script.md"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "3778126c3f8d6c114a4804ff3e62747973115392", "node_type": null, "metadata": {"file_path": "docs/script.md", "file_name": "script.md"}, "hash": "fda433296dd0f686911ec7c1134cf06b71cc668b7a3655785b3647709f1b8920"}, "2": {"node_id": "0cecd8ba-77b1-4051-98a7-01627b961b02", "node_type": null, "metadata": {"file_path": "docs/script.md", "file_name": "script.md"}, "hash": "eb3ece60962a2d6c380b6c43fe1549fc3ef471a41539657879908d5fde55a626"}, "3": {"node_id": "baad20ff-c3fb-424a-9810-140d2ab4696a", "node_type": null, "metadata": {"file_path": "docs/script.md", "file_name": "script.md"}, "hash": "3ea20813cab9ab1272788a1971ecb51f43b08c0295144aff6e210a79cfcaf307"}}, "hash": "4cc12e1fb57c037daa9314c6c055de138b5ece7700c078b8b9b6f11a342dee15", "text": "file\n}\n```\n\nAdditionally, the `eachFile()` method allows you to iterate through the first-level elements only (just like `listFiles()`). As with other `each*()` methods, `eachFiles()` takes a closure as a parameter:\n\n```groovy\nmyDir.eachFile { item ->\n    if( item.isFile() ) {\n        println \"${item.getName()} - size: ${item.size()}\"\n    }\n    else if( item.isDirectory() ) {\n        println \"${item.getName()} - DIR\"\n    }\n}\n```\n\nThe following methods are available for listing and traversing directories:\n\n`eachDir( closure )`\n: Iterates through first-level directories only. [Read more](http://docs.groovy-lang.org/latest/html/groovy-jdk/java/io/File.html#eachDir(groovy.lang.Closure))\n\n`eachDirMatch( nameFilter, closure )`\n: Iterates through directories whose names match the given filter. [Read more](http://docs.groovy-lang.org/latest/html/groovy-jdk/java/io/File.html#eachDirMatch(java.lang.Object,%20groovy.lang.Closure))\n\n`eachDirRecurse( closure )`\n: Iterates through directories depth-first (regular files are ignored). [Read more](http://docs.groovy-lang.org/latest/html/groovy-jdk/java/io/File.html#eachDirRecurse(groovy.lang.Closure))\n\n`eachFile( closure )`\n: Iterates through first-level files and directories. [Read more](http://docs.groovy-lang.org/latest/html/groovy-jdk/java/io/File.html#eachFile(groovy.lang.Closure))\n\n`eachFileMatch( nameFilter, closure )`\n: Iterates through files and directories whose names match the given filter. [Read more](http://docs.groovy-lang.org/latest/html/groovy-jdk/java/io/File.html#eachFileMatch(java.lang.Object,%20groovy.lang.Closure))\n\n`eachFileRecurse( closure )`\n: Iterates through files and directories depth-first. [Read more](http://docs.groovy-lang.org/latest/html/groovy-jdk/java/io/File.html#eachFileRecurse(groovy.lang.Closure))\n\nSee also: {ref}`Channel.fromPath <channel-path>`.\n\n### Fetching HTTP/FTP files\n\nNextflow integrates seamlessly with the HTTP(S) and FTP protocols for handling remote resources the same as local files. Simply specify the resource URL when opening the file:\n\n```groovy\npdb = file('http://files.rcsb.org/header/5FID.pdb')\n```\n\nThen, you can access it as a local file as described previously:\n\n```groovy\nprintln pdb.text\n```\n\nThe above one-liner prints the content of the remote PDB file. Previous sections provide code examples showing how to stream or copy the content of files.\n\n:::{note}\nWrite and list operations are not supported for HTTP(S) and FTP files.\n:::\n\n### Splitting and counting records\n\nThe following methods are defined for Paths for splitting and counting records:\n\n`countFasta()`\n: Counts the number of records in a [FASTA](https://en.wikipedia.org/wiki/FASTA_format) file. See the {ref}`operator-splitfasta` operator for available options.\n\n`countFastq()`\n: Counts the number of records in a [FASTQ](https://en.wikipedia.org/wiki/FASTQ_format) file. See the {ref}`operator-splitfastq` operator for available options.\n\n`countJson()`\n: Counts the number of records in a JSON file. See the {ref}`operator-splitjson` operator for available", "start_char_idx": 36990, "end_char_idx": 40042, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "baad20ff-c3fb-424a-9810-140d2ab4696a": {"__data__": {"id_": "baad20ff-c3fb-424a-9810-140d2ab4696a", "embedding": null, "metadata": {"file_path": "docs/script.md", "file_name": "script.md"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "3778126c3f8d6c114a4804ff3e62747973115392", "node_type": null, "metadata": {"file_path": "docs/script.md", "file_name": "script.md"}, "hash": "fda433296dd0f686911ec7c1134cf06b71cc668b7a3655785b3647709f1b8920"}, "2": {"node_id": "d0141c92-fc27-4d55-b4d1-94076acfdce8", "node_type": null, "metadata": {"file_path": "docs/script.md", "file_name": "script.md"}, "hash": "4cc12e1fb57c037daa9314c6c055de138b5ece7700c078b8b9b6f11a342dee15"}}, "hash": "3ea20813cab9ab1272788a1971ecb51f43b08c0295144aff6e210a79cfcaf307", "text": "in a JSON file. See the {ref}`operator-splitjson` operator for available options.\n\n`countLines()`\n: Counts the number of lines in a text file. See the {ref}`operator-splittext` operator for available options.\n\n`splitFasta()`\n: Splits a [FASTA](https://en.wikipedia.org/wiki/FASTA_format) file into a list of records. See the {ref}`operator-splitfasta` operator for available options.\n\n`splitFastq()`\n: Splits a [FASTQ](https://en.wikipedia.org/wiki/FASTQ_format) file into a list of records. See the {ref}`operator-splitfastq` operator for available options.\n\n`splitJson()`\n: Splits a JSON file into a list of records. See the {ref}`operator-splitjson` operator for available options.\n\n`splitLines()`\n: Splits a text file into a list of lines. See the {ref}`operator-splittext` operator for available options.", "start_char_idx": 40027, "end_char_idx": 40836, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "95baefbe-e992-4876-82a3-a2e7dff9aee6": {"__data__": {"id_": "95baefbe-e992-4876-82a3-a2e7dff9aee6", "embedding": null, "metadata": {"file_path": "docs/secrets.md", "file_name": "secrets.md"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "1f6253a69f7dc9d6db384a333c1e40582b2d4cb7", "node_type": null, "metadata": {"file_path": "docs/secrets.md", "file_name": "secrets.md"}, "hash": "fa518e8e4b84a2f678e4593144bb4f1247dabe9bbd2a7f6fc7808dac394d5730"}}, "hash": "882ab05bfcc8a5faba6cf52c1030e9a8e0873619f2f917e16147036e6265674f", "text": "(secrets-page)=\n\n# Secrets\n\n:::{versionadded} 22.10.0\nPreviewed in `21.09.0-edge`.\n:::\n\nNextflow has built-in support for pipeline secrets to allow users to safely provide sensitive information to a pipeline execution.\n\n## How it works\n\nThis feature allows decoupling the use secrets in your pipelines from the pipeline code and configuration files. Secrets are instead managed by Nextflow and store separately into a local store only accessible to the secrets owner.\n\nWhen the pipeline execution is launched Nextflow inject the secrets in pipeline jobs without leaking them into temporary execution files. The secrets are accessible into the job command via environment variables.\n\n## Command line\n\nNextflow provides a command named `secrets`. This command allows four simple operations:\n\n`list`\n: List secrets available in the current store e.g. `nextflow secrets list`.\n\n`get`\n: Retrieve a secret value e.g. `nextflow secrets get FOO`.\n\n`set`\n: Create or update a secret e.g. `nextflow secrets set FOO \"Hello world\"`\n\n`delete`\n: Delete a secret e.g. `nextflow secrets delete FOO`.\n\n## Configuration file\n\nOnce create the secrets can be used in the pipeline configuration file as implicit variables using the `secrets` scope:\n\n```groovy\naws {\n  accessKey = secrets.MY_ACCESS_KEY\n  secretKey = secrets.MY_SECRET_KEY\n}\n```\n\nThe above snippet access the secrets `MY_ACCESS_KEY` and `MY_SECRET_KEY` previously and assign them to the corresponding AWS credentials settings.\n\n:::{warning}\nSecrets **cannot** be assigned to pipeline parameters.\n:::\n\n## Process secrets\n\nSecrets can be access by pipeline processes by using the `secret` directive. For example:\n\n```groovy\nprocess someJob {\n    secret 'MY_ACCESS_KEY'\n    secret 'MY_SECRET_KEY'\n\n    \"\"\"\n    your_command --access \\$MY_ACCESS_KEY --secret \\$MY_SECRET_KEY\n    \"\"\"\n}\n```\n\nThe above snippet runs a command in with the variables `MY_ACCESS_KEY` and `MY_SECRET_KEY` are injected in the process execution environment holding the values defines in the secret store.\n\n:::{warning}\nThe secrets are made available in the process context running the command script as environment variables. Therefore make sure to escape the variable name identifier with a backslash as shown in the example above, otherwise a variable with the same will be evaluated in the Nextflow script context instead of the command script.\n:::\n\n:::{note}\nThis feature is only available when using the local or grid executors (Slurm, Grid Engine, etc). The AWS Batch executor allows the use of secrets when deploying the pipeline execution via [Nextflow Tower](https://seqera.io/blog/pipeline-secrets-secure-handling-of-sensitive-information-in-tower/).\n:::", "start_char_idx": 0, "end_char_idx": 2677, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "c7c3fb4b-734c-4a9f-8a50-77f6c805f858": {"__data__": {"id_": "c7c3fb4b-734c-4a9f-8a50-77f6c805f858", "embedding": null, "metadata": {"file_path": "docs/sharing.md", "file_name": "sharing.md"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "aac7341d37e177f35e582c98b61ee22a8c002f39", "node_type": null, "metadata": {"file_path": "docs/sharing.md", "file_name": "sharing.md"}, "hash": "596c2beb2596af8ae546a165e699511a4bd6da5bb4d6827632cd34c7880048eb"}, "3": {"node_id": "32cd74a7-d777-4148-91f3-59212a5e7db2", "node_type": null, "metadata": {"file_path": "docs/sharing.md", "file_name": "sharing.md"}, "hash": "57232693f60918be055d5369090ebda09a2706d65bec4a70db5cd82f2475c474"}}, "hash": "c6c529fee97db848b02a4c911553427fb799acfc1b9241adba95cd83d0d42324", "text": "(sharing-page)=\n\n# Pipeline sharing\n\nNextflow seamlessly integrates with [BitBucket](http://bitbucket.org/) [^id2], [GitHub](http://github.com), and [GitLab](http://gitlab.com) hosted code repositories and sharing platforms. This feature allows you to manage your project code in a more consistent manner or use other people's Nextflow pipelines, published through BitBucket/GitHub/GitLab, in a quick and transparent way.\n\n## How it works\n\nWhen you launch a script execution with Nextflow, it will look for a file with the pipeline name you've specified. If that file does not exist, it will look for a public repository with the same name on GitHub (unless otherwise specified). If it is found, the repository is automatically downloaded to your computer and executed. This repository is stored in the Nextflow home directory, that is by default the `$HOME/.nextflow` path, and thus will be reused for any further executions.\n\n## Running a pipeline\n\nTo launch the execution of a pipeline project, hosted in a remote code repository, you simply need to specify its qualified name or the repository URL after the `run` command. The qualified name is formed by two parts: the `owner` name and the `repository` name separated by a `/` character.\n\nIn other words if a Nextflow project is hosted, for example, in a GitHub repository at the address `http://github.com/foo/bar`, it can be executed by entering the following command in your shell terminal:\n\n```bash\nnextflow run foo/bar\n```\n\nor using the project URL:\n\n```bash\nnextflow run http://github.com/foo/bar\n```\n\n:::{note}\nIn the first case, if your project is hosted on a service other than GitHub, you will need to specify this hosting service in the command line by using the `-hub` option. For example `-hub bitbucket` or `-hub gitlab`. In the second case, i.e. when using the project URL as name, the `-hub` option is not needed.\n:::\n\nYou can try this feature out by simply entering the following command in your shell terminal:\n\n```bash\nnextflow run nextflow-io/hello\n```\n\nIt will download a trivial `Hello` example from the repository published at the following address <http://github.com/nextflow-io/hello> and execute it in your computer.\n\nIf the `owner` part in the pipeline name is omitted, Nextflow will look for a pipeline between the ones you have already executed having a name that matches the name specified. If none is found it will try to download it using the `organisation` name defined by the environment variable `NXF_ORG` (which by default is `nextflow-io`).\n\n:::{tip}\nTo access a private repository, specify the access credentials by using the `-user` command line option, then the program will ask you to enter the password interactively. Private repository access credentials can also be defined in the [SCM configuration file](#scm-configuration-file)(#s.\n:::\n\n## Handling revisions\n\nAny Git branch, tag or commit ID defined in a project repository, can be used to specify the revision that you want to execute when launching a pipeline by adding the `-r` option to the run command line. So for example you could enter:\n\n```bash\nnextflow run nextflow-io/hello -r mybranch\n```\n\nor\n\n```bash\nnextflow run nextflow-io/hello -r v1.1\n```\n\nIt will execute two different project revisions corresponding to the Git tag/branch having that names.\n\n## Commands to manage projects\n\nThe following commands allows you to perform some basic operations that can be used to manage your projects.\n\n:::{note}\nNextflow is not meant to completely replace the [Git](https://git-scm.com/) tool. You", "start_char_idx": 0, "end_char_idx": 3552, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "32cd74a7-d777-4148-91f3-59212a5e7db2": {"__data__": {"id_": "32cd74a7-d777-4148-91f3-59212a5e7db2", "embedding": null, "metadata": {"file_path": "docs/sharing.md", "file_name": "sharing.md"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "aac7341d37e177f35e582c98b61ee22a8c002f39", "node_type": null, "metadata": {"file_path": "docs/sharing.md", "file_name": "sharing.md"}, "hash": "596c2beb2596af8ae546a165e699511a4bd6da5bb4d6827632cd34c7880048eb"}, "2": {"node_id": "c7c3fb4b-734c-4a9f-8a50-77f6c805f858", "node_type": null, "metadata": {"file_path": "docs/sharing.md", "file_name": "sharing.md"}, "hash": "c6c529fee97db848b02a4c911553427fb799acfc1b9241adba95cd83d0d42324"}, "3": {"node_id": "f24222d1-22a0-4213-8821-0bea3b01faca", "node_type": null, "metadata": {"file_path": "docs/sharing.md", "file_name": "sharing.md"}, "hash": "baecb23221caea8037e6f35f430e6d44e4ca09923ff1b9e489a9cfc94a0208b0"}}, "hash": "57232693f60918be055d5369090ebda09a2706d65bec4a70db5cd82f2475c474", "text": "to completely replace the [Git](https://git-scm.com/) tool. You may still need `git` to create new repositories or commit changes, etc.\n:::\n\n### Listing available projects\n\nThe `list` command allows you to list all the projects you have downloaded in your computer. For example:\n\n```bash\nnextflow list\n```\n\nThis prints a list similar to the following one:\n\n```\ncbcrg/ampa-nf\ncbcrg/piper-nf\nnextflow-io/hello\nnextflow-io/examples\n```\n\n### Showing project information\n\nBy using the `info` command you can show information from a downloaded project. For example:\n\n```console\n$ nextflow info hello\nproject name: nextflow-io/hello\nrepository  : http://github.com/nextflow-io/hello\nlocal path  : $HOME/.nextflow/assets/nextflow-io/hello\nmain script : main.nf\nrevisions   :\n* master (default)\n  mybranch\n  v1.1 [t]\n  v1.2 [t]\n```\n\nStarting from the top it shows: 1) the project name; 2) the Git repository URL; 3) the local folder where the project has been downloaded; 4) the script that is executed when launched; 5) the list of available revisions i.e. branches and tags. Tags are marked with a `[t]` on the right, the current checked-out revision is marked with a `*` on the left.\n\n### Pulling or updating a project\n\nThe `pull` command allows you to download a project from a GitHub repository or to update it if that repository has already been downloaded. For example:\n\n```bash\nnextflow pull nextflow-io/examples\n```\n\nAltenatively, you can use the repository URL as the name of the project to pull:\n\n```bash\nnextflow pull https://github.com/nextflow-io/examples\n```\n\nDownloaded pipeline projects are stored in the folder `$HOME/.nextflow/assets` in your computer.\n\n### Viewing the project code\n\nThe `view` command allows you to quickly show the content of the pipeline script you have downloaded. For example:\n\n```bash\nnextflow view nextflow-io/hello\n```\n\nBy adding the `-l` option to the example above it will list the content of the repository.\n\n### Cloning a project into a folder\n\nThe `clone` command allows you to copy a Nextflow pipeline project to a directory of your choice. For example:\n\n```bash\nnextflow clone nextflow-io/hello target-dir\n```\n\nIf the destination directory is omitted the specified project is cloned to a directory with the same name as the pipeline base name (e.g. `hello`) in the current folder.\n\nThe clone command can be used to inspect or modify the source code of a pipeline project. You can eventually commit and push back your changes by using the usual Git/GitHub workflow.\n\n### Deleting a downloaded project\n\nDownloaded pipelines can be deleted by using the `drop` command, as shown below:\n\n```bash\nnextflow drop nextflow-io/hello\n```\n\n(sharing-scm-file)=\n\n## SCM configuration file\n\nThe file `$HOME/.nextflow/scm` allows you to centralise the security credentials required to access private project repositories on Bitbucket, GitHub and GitLab source code management (SCM) platforms or to manage the configuration properties of private server installations (of the same platforms).\n\nThe configuration properties for each SCM platform are defined inside the `providers` section, properties for the same provider are grouped together with a common name and delimited with curly brackets as in this example:\n\n```groovy\nproviders {\n    <provider-name> {\n        property = value\n        // ...\n    }\n}\n```\n\nIn the above template replace `<provider-name>`", "start_char_idx": 3496, "end_char_idx": 6881, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "f24222d1-22a0-4213-8821-0bea3b01faca": {"__data__": {"id_": "f24222d1-22a0-4213-8821-0bea3b01faca", "embedding": null, "metadata": {"file_path": "docs/sharing.md", "file_name": "sharing.md"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "aac7341d37e177f35e582c98b61ee22a8c002f39", "node_type": null, "metadata": {"file_path": "docs/sharing.md", "file_name": "sharing.md"}, "hash": "596c2beb2596af8ae546a165e699511a4bd6da5bb4d6827632cd34c7880048eb"}, "2": {"node_id": "32cd74a7-d777-4148-91f3-59212a5e7db2", "node_type": null, "metadata": {"file_path": "docs/sharing.md", "file_name": "sharing.md"}, "hash": "57232693f60918be055d5369090ebda09a2706d65bec4a70db5cd82f2475c474"}, "3": {"node_id": "162daf80-f111-47e5-9363-d502d3e827c4", "node_type": null, "metadata": {"file_path": "docs/sharing.md", "file_name": "sharing.md"}, "hash": "96b4ccacce22f4acd61892ff84d10d66a375243312f459903d99103e3da4c8f2"}}, "hash": "baecb23221caea8037e6f35f430e6d44e4ca09923ff1b9e489a9cfc94a0208b0", "text": "the above template replace `<provider-name>` with one of the \"default\" servers (i.e. `bitbucket`, `github` or `gitlab`) or a custom identifier representing a private SCM server installation.\n\n:::{versionadded} 20.10.0\nA custom location for the SCM file can be specified using the `NXF_SCM_FILE` environment variable.\n:::\n\nThe following configuration properties are supported for each provider configuration:\n\n`providers.<provider>.user`\n: User name required to access private repositories on the SCM server.\n\n`providers.<provider>.password`\n: User password required to access private repositories on the SCM server.\n\n`providers.<provider>.token`\n: *Required only for private Gitlab servers*\n: Private API access token.\n\n`providers.<provider>.platform`\n: *Required only for private SCM servers*\n: SCM platform name, either: `github`, `gitlab` or `bitbucket`.\n\n`providers.<provider>.server`\n: *Required only for private SCM servers*\n: SCM server name including the protocol prefix e.g. `https://github.com`.\n\n`providers.<provider>.endpoint`\n: *Required only for private SCM servers*\n: SCM API `endpoint` URL e.g. `https://api.github.com` (default: the same as `providers.<provider>.server`).\n\n### BitBucket credentials\n\nCreate a `bitbucket` entry in the [SCM configuration file](#scm-configuration-file) specifying your user name and app password, as shown below:\n\n```groovy\nproviders {\n    bitbucket {\n        user = 'me'\n        password = 'my-secret'\n    }\n}\n```\n\n:::{note}\nApp passwords are substitute passwords for a user account which you can use for scripts and integrating tools in order to avoid putting your real password into configuration files. Learn more at [this link](https://support.atlassian.com/bitbucket-cloud/docs/app-passwords/).\n:::\n\n### BitBucket Server credentials\n\n[BitBucket Server](https://confluence.atlassian.com/bitbucketserver) is a self-hosted Git repository and management platform.\n\n:::{note}\nBitBucket Server uses a different API from the [BitBucket](https://bitbucket.org/) cloud service. Make sure to use the right configuration whether you are using the cloud service or a self-hosted installation.\n:::\n\nTo access your local BitBucket Server create an entry in the [SCM configuration file](#scm-configuration-file) specifying as shown below:\n\n```groovy\nproviders {\n    mybitbucket {\n        platform = 'bitbucketserver'\n        server = 'https://your.bitbucket.host.com'\n        endpoint = 'https://your.bitbucket.host.com'\n        user = 'your-user'\n        password = 'your-password or your-token'\n    }\n}\n```\n\n### GitHub credentials\n\nCreate a `github` entry in the [SCM configuration file](#scm-configuration-file) specifying your user name and access token as shown below:\n\n```groovy\nproviders {\n    github {\n        user = 'your-user-name'\n        password = 'your-personal-access-token'\n    }\n}\n```\n\nGitHub requires the use of a personal access token (PAT) in place of a password when accessing APIs. Learn more about PAT and how to create it at [this", "start_char_idx": 6899, "end_char_idx": 9893, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "162daf80-f111-47e5-9363-d502d3e827c4": {"__data__": {"id_": "162daf80-f111-47e5-9363-d502d3e827c4", "embedding": null, "metadata": {"file_path": "docs/sharing.md", "file_name": "sharing.md"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "aac7341d37e177f35e582c98b61ee22a8c002f39", "node_type": null, "metadata": {"file_path": "docs/sharing.md", "file_name": "sharing.md"}, "hash": "596c2beb2596af8ae546a165e699511a4bd6da5bb4d6827632cd34c7880048eb"}, "2": {"node_id": "f24222d1-22a0-4213-8821-0bea3b01faca", "node_type": null, "metadata": {"file_path": "docs/sharing.md", "file_name": "sharing.md"}, "hash": "baecb23221caea8037e6f35f430e6d44e4ca09923ff1b9e489a9cfc94a0208b0"}, "3": {"node_id": "5ff7a0d0-1e3e-4c78-8506-2bc7eb2a58f0", "node_type": null, "metadata": {"file_path": "docs/sharing.md", "file_name": "sharing.md"}, "hash": "878ff2ee7b0fa5a8dc34c8249afa261e96931d96a2a495f69f18535f9fa4ecb8"}}, "hash": "96b4ccacce22f4acd61892ff84d10d66a375243312f459903d99103e3da4c8f2", "text": "password when accessing APIs. Learn more about PAT and how to create it at [this link](https://docs.github.com/en/github/authenticating-to-github/keeping-your-account-and-data-secure/creating-a-personal-access-token).\n\n:::{versionadded} 23.01.0-edge\nNextflow automatically uses the `GITHUB_TOKEN` environment variable to authenticate access to the GitHub repository if no credentials are provided via the `scm` file. This is useful especially when accessing pipeline code from a GitHub Action. Read more about the token authentication in the [GitHub documentation](https://docs.github.com/en/actions/security-guides/automatic-token-authentication).\n:::\n\n### GitLab credentials\n\nCreate a `gitlab` entry in the [SCM configuration file](#scm-configuration-file) specifying the user name, password and your API access token that can be found in your GitLab [account page](https://gitlab.com/profile/account) (sign in required). For example:\n\n```groovy\nproviders {\n    gitlab {\n        user = 'me'\n        password = 'my-secret'\n        token = 'YgpR8m7viH_ZYnC8YSe8'\n    }\n}\n```\n\n:::{tip}\nThe GitLab *token* string can be used as the `password` value in the above setting. When doing that the `token` field can be omitted.\n:::\n\n### Gitea credentials\n\n[Gitea](https://gitea.io) is a Git repository server with GitHub-like GUI access. Since Gitea installation is quite easy, it is suitable for building a private development environment in your network. To access your Gitea server, you have to provide all the credential information below:\n\n```groovy\nproviders {\n    mygitea {\n        server = 'http://your-domain.org/gitea'\n        endpoint = 'http://your-domain.org/gitea/api/v1'\n        platform = 'gitea'\n        user = 'your-user'\n        password = 'your-password'\n        token = 'your-api-token'\n    }\n}\n```\n\nSee [Gitea documentation](https://docs.gitea.io/en-us/api-usage/) about how to enable API access on your server and how to issue a token.\n\n### Azure Repos credentials\n\nNextflow has a builtin support for [Azure Repos](https://azure.microsoft.com/en-us/services/devops/repos/), a Git source code management service hosted in the Azure cloud. To access your Azure Repos with Nextflow provide the repository credentials using the configuration snippet shown below:\n\n```groovy\nproviders {\n    azurerepos {\n        user = 'your-user-name'\n        password = 'your-personal-access-token'\n    }\n}\n```\n\n:::{tip}\nThe Personal access token can be generated in the repository `Clone Repository` dialog.\n:::\n\n### AWS CodeCommit credentials\n\n:::{versionadded} 22.06.0-edge\n:::\n\nNextflow supports [AWS CodeCommit](https://aws.amazon.com/codecommit/) as a Git provider to access and to share pipelines code.\n\nTo access your project hosted on AWS CodeCommit with Nextflow provide the repository credentials using the configuration snippet shown below:\n\n```groovy\nproviders {\n    my_aws_repo {\n        platform = 'codecommit'\n      ", "start_char_idx": 9868, "end_char_idx": 12794, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "5ff7a0d0-1e3e-4c78-8506-2bc7eb2a58f0": {"__data__": {"id_": "5ff7a0d0-1e3e-4c78-8506-2bc7eb2a58f0", "embedding": null, "metadata": {"file_path": "docs/sharing.md", "file_name": "sharing.md"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "aac7341d37e177f35e582c98b61ee22a8c002f39", "node_type": null, "metadata": {"file_path": "docs/sharing.md", "file_name": "sharing.md"}, "hash": "596c2beb2596af8ae546a165e699511a4bd6da5bb4d6827632cd34c7880048eb"}, "2": {"node_id": "162daf80-f111-47e5-9363-d502d3e827c4", "node_type": null, "metadata": {"file_path": "docs/sharing.md", "file_name": "sharing.md"}, "hash": "96b4ccacce22f4acd61892ff84d10d66a375243312f459903d99103e3da4c8f2"}, "3": {"node_id": "38003815-e0f2-4dd1-9af3-0d93d869a3f2", "node_type": null, "metadata": {"file_path": "docs/sharing.md", "file_name": "sharing.md"}, "hash": "781e1a2dcf3f92bf300356977b7ea1504da4a31950dc7e1d5a0c763287ef8389"}}, "hash": "878ff2ee7b0fa5a8dc34c8249afa261e96931d96a2a495f69f18535f9fa4ecb8", "text": "     platform = 'codecommit'\n        user = '<AWS ACCESS KEY>'\n        password = '<AWS SECRET KEY>'\n    }\n}\n```\n\nIn the above snippet replace `<AWS ACCESS KEY>` and `<AWS SECRET KEY>` with your AWS credentials, and `my_aws_repo` with a name of your choice.\n\n:::{tip}\nThe `user` and `password` are optional settings, if omitted the [AWS default credentials provider chain](https://docs.aws.amazon.com/sdk-for-java/v1/developer-guide/credentials.html) is used.\n:::\n\nThen the pipeline can be accessed with Nextflow as shown below:\n\n```bash\nnextflow run https://git-codecommit.eu-west-1.amazonaws.com/v1/repos/my-repo\n```\n\nIn the above example replace `my-repo` with your own repository. Note also that AWS CodeCommit has different URLs depending the region in which you are working.\n\n:::{note}\nThe support for protocols other than HTTPS is not available at this time.\n:::\n\n## Private server configuration\n\nNextflow is able to access repositories hosted on private BitBucket, GitHub, GitLab and Gitea server installations.\n\nIn order to use a private SCM installation you will need to set the server name and access credentials in your [SCM configuration file](#scm-configuration-file) .\n\nIf, for example, the host name of your private GitLab server is `gitlab.acme.org`, you will need to have in the `$HOME/.nextflow/scm` file a configuration like the following:\n\n```groovy\nproviders {\n    mygit {\n        server = 'http://gitlab.acme.org'\n        platform = 'gitlab'\n        user = 'your-user'\n        password = 'your-password'\n        token = 'your-api-token'\n    }\n}\n```\n\nThen you will be able to run/pull a project with Nextflow using the following command line:\n\n```bash\nnextflow run foo/bar -hub mygit\n```\n\nOr, in alternative, using the Git clone URL:\n\n```bash\nnextflow run http://gitlab.acme.org/foo/bar.git\n```\n\n:::{note}\nYou must also specify the server API endpoint URL if it differs from the server base URL. For example, for GitHub Enterprise V3, add `endpoint = 'https://git.your-domain.com/api/v3'`.\n:::\n\n:::{warning}\nWhen accessing a private SCM installation over `https` from a server that uses a custom SSL certificate, you may need to import the certificate into your local Java keystore. Read more [here](https://docs.oracle.com/javase/tutorial/security/toolsign/rstep2.html).\n:::\n\n## Local repository configuration\n\nNextflow is also able to handle repositories stored in a local or shared file system. The repository must be created as a [bare repository](https://mijingo.com/blog/what-is-a-bare-git-repository).\n\nHaving, for example. a bare repository store at path `/shared/projects/foo.git`, Nextflow is able to run it using the following syntax:\n\n```bash\nnextflow run file:/shared/projects/foo.git\n```\n\nSee [Git documentation](https://git-scm.com/book/en/v2/Git-on-the-Server-Getting-Git-on-a-Server) for more details about how create and manage bare repositories.\n\n## Publishing your pipeline\n\nIn order to publish your Nextflow pipeline to", "start_char_idx": 12839, "end_char_idx": 15801, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "38003815-e0f2-4dd1-9af3-0d93d869a3f2": {"__data__": {"id_": "38003815-e0f2-4dd1-9af3-0d93d869a3f2", "embedding": null, "metadata": {"file_path": "docs/sharing.md", "file_name": "sharing.md"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "aac7341d37e177f35e582c98b61ee22a8c002f39", "node_type": null, "metadata": {"file_path": "docs/sharing.md", "file_name": "sharing.md"}, "hash": "596c2beb2596af8ae546a165e699511a4bd6da5bb4d6827632cd34c7880048eb"}, "2": {"node_id": "5ff7a0d0-1e3e-4c78-8506-2bc7eb2a58f0", "node_type": null, "metadata": {"file_path": "docs/sharing.md", "file_name": "sharing.md"}, "hash": "878ff2ee7b0fa5a8dc34c8249afa261e96931d96a2a495f69f18535f9fa4ecb8"}, "3": {"node_id": "e28da2fd-46b1-4067-b0a6-664354fd3566", "node_type": null, "metadata": {"file_path": "docs/sharing.md", "file_name": "sharing.md"}, "hash": "4f653e63b4056984d488a95e81870641c95e1aafd516c5e3cf03d0ae75827bf2"}}, "hash": "781e1a2dcf3f92bf300356977b7ea1504da4a31950dc7e1d5a0c763287ef8389", "text": "Publishing your pipeline\n\nIn order to publish your Nextflow pipeline to GitHub (or any other supported platform) and allow other people to use it, you only need to create a GitHub repository containing all your project script and data files. If you don't know how to do it, follow this simple tutorial that explains how [create a GitHub repository](https://help.github.com/articles/create-a-repo).\n\nNextflow only requires that the main script in your pipeline project is called `main.nf`. A different name can be used by specifying the `manifest.mainScript` attribute in the `nextflow.config` file that must be included in your project. For example:\n\n```groovy\nmanifest.mainScript = 'my_very_long_script_name.nf'\n```\n\nTo learn more about this and other project metadata information, that can be defined in the Nextflow configuration file, read the {ref}`Manifest <config-manifest>` section on the Nextflow configuration page.\n\nOnce you have uploaded your pipeline project to GitHub other people can execute it simply using the project name or the repository URL.\n\nFor if your GitHub account name is `foo` and you have uploaded a project into a repository named `bar` the repository URL will be `http://github.com/foo/bar` and people will able to download and run it by using either the command:\n\n```bash\nnextflow run foo/bar\n```\n\nor\n\n```bash\nnextflow run http://github.com/foo/bar\n```\n\nSee the [Running a pipeline](#running-a-pipeline) section for more details on how to run Nextflow projects.\n\n## Manage dependencies\n\nComputational pipelines are rarely composed by a single script. In real world applications they depend on dozens of other components. These can be other scripts, databases, or applications compiled for a platform native binary format.\n\nExternal dependencies are the most common source of problems when sharing a piece of software, because the users need to have an identical set of tools and the same configuration to be able to use it. In many cases this has proven to be a painful and error prone process, that can severely limit the ability to reproduce computational results on a system other than the one on which it was originally developed.\n\nNextflow tackles this problem by integrating GitHub, BitBucket and GitLab sharing platforms and [Docker](http://www.docker.com) containers technology.\n\nThe use of a code management system is important to keep together all the dependencies of your pipeline project and allows you to track the changes of the source code in a consistent manner.\n\nMoreover to guarantee that a pipeline is reproducible it should be self-contained i.e. it should have ideally no dependencies on the hosting environment. By using Nextflow you can achieve this goal following these methods:\n\n### Third party scripts\n\nAny third party script that does not need to be compiled (Bash, Python, Perl, etc) can be included in the pipeline project repository, so that they are distributed with it.\n\nGrant the execute permission to these files and copy them into a folder named `bin/` in the root directory of your project repository. Nextflow will automatically add this folder to the `PATH` environment variable, and the scripts will automatically be accessible in your pipeline without the need to specify an absolute path to invoke them.\n\n### System environment\n\nAny environment variable that may be required by the tools in your pipeline can be defined in the `nextflow.config` file by using the `env` scope and including it in the root directory of your project. For example:\n\n```groovy\nenv {\n  DELTA = 'foo'\n  GAMMA = 'bar'\n}\n```\n\nSee the {ref}`config-page` page to learn more about the Nextflow configuration file.\n\n### Resource manager\n\nWhen using Nextflow you don't need to write the code to parallelize your pipeline for a specific grid engine/resource manager because the parallelization is defined implicitly and managed by the Nextflow runtime. The", "start_char_idx": 15762, "end_char_idx": 19659, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "e28da2fd-46b1-4067-b0a6-664354fd3566": {"__data__": {"id_": "e28da2fd-46b1-4067-b0a6-664354fd3566", "embedding": null, "metadata": {"file_path": "docs/sharing.md", "file_name": "sharing.md"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "aac7341d37e177f35e582c98b61ee22a8c002f39", "node_type": null, "metadata": {"file_path": "docs/sharing.md", "file_name": "sharing.md"}, "hash": "596c2beb2596af8ae546a165e699511a4bd6da5bb4d6827632cd34c7880048eb"}, "2": {"node_id": "38003815-e0f2-4dd1-9af3-0d93d869a3f2", "node_type": null, "metadata": {"file_path": "docs/sharing.md", "file_name": "sharing.md"}, "hash": "781e1a2dcf3f92bf300356977b7ea1504da4a31950dc7e1d5a0c763287ef8389"}}, "hash": "4f653e63b4056984d488a95e81870641c95e1aafd516c5e3cf03d0ae75827bf2", "text": "manager because the parallelization is defined implicitly and managed by the Nextflow runtime. The target execution environment is parametrized and defined in the configuration file, thus your code is free from this kind of dependency.\n\n### Bootstrap data\n\nWhenever your pipeline requires some files or dataset to carry out any initialization step, you can include this data in the pipeline repository itself and distribute them together.\n\nTo reference this data in your pipeline script in a portable manner (i.e. without the need to use a static absolute path) use the implicit variable `baseDir` which locates the base directory of your pipeline project.\n\nFor example, you can create a folder named `dataset/` in your repository root directory and copy there the required data file(s) you may need, then you can access this data in your script by writing:\n\n```groovy\nsequences = file(\"$baseDir/dataset/sequences.fa\")\nsequences.splitFasta {\n    println it\n}\n```\n\n### User inputs\n\nNextflow scripts can be easily parametrised to allow users to provide their own input data. Simply declare on the top of your script all the parameters it may require as shown below:\n\n```groovy\nparams.my_input = 'default input file'\nparams.my_output = 'default output path'\nparams.my_flag = false\n// ...\n```\n\nThe actual parameter values can be provided when launching the script execution on the command line by prefixed the parameter name with a double minus character i.e. `--`, for example:\n\n```bash\nnextflow run <your pipeline> --my_input /path/to/input/file --my_output /other/path --my_flag true\n```\n\n### Binary applications\n\nDocker allows you to ship any binary dependencies that you may have in your pipeline to a portable image that is downloaded on-demand and can be executed on any platform where a Docker engine is installed.\n\nIn order to use it with Nextflow, create a Docker image containing the tools needed by your pipeline and make it available in the [Docker registry](https://registry.hub.docker.com).\n\nThen declare in the `nextflow.config` file, that you will include in your project, the name of the Docker image you have created. For example:\n\n```groovy\nprocess.container = 'my-docker-image'\ndocker.enabled = true\n```\n\nIn this way when you launch the pipeline execution, the Docker image will be automatically downloaded and used to run your tasks.\n\nRead the {ref}`container-page` page to learn more on how to use containers with Nextflow.\n\nThis mix of technologies makes it possible to write self-contained and truly reproducible pipelines which require zero configuration and can be reproduced in any system having a Java VM and a Docker engine installed.\n\n[^id2]: BitBucket provides two types of version control system: Git and Mercurial. Nextflow supports only Git repositories.", "start_char_idx": 19623, "end_char_idx": 22408, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "3038151e-cec1-4531-a55a-943885881fcf": {"__data__": {"id_": "3038151e-cec1-4531-a55a-943885881fcf", "embedding": null, "metadata": {"file_path": "docs/spack.md", "file_name": "spack.md"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "636b0952f819caa55fb9bafa992266d6e9ba4045", "node_type": null, "metadata": {"file_path": "docs/spack.md", "file_name": "spack.md"}, "hash": "7d8a786d99536f543c14004a43e55b4d18938b2c16f1dc0cbdb63add4b67742e"}, "3": {"node_id": "cdc4fccb-2879-4e38-b6ce-c9eb068f92a4", "node_type": null, "metadata": {"file_path": "docs/spack.md", "file_name": "spack.md"}, "hash": "894adf8ef4bb1d51765246e1dcfc2f82ee2bb0af6bfaefd8baeb14bd046affde"}}, "hash": "24540141f71e88ed0dd3142833e76da2fef1af52dc75489cac2ddf8fe73c1367", "text": "(spack-page)=\n\n# Spack environments\n\n:::{versionadded} 23.02.0-edge\n:::\n\n[Spack](https://spack.io/) is a package manager for supercomputers, Linux, and macOS. It makes installing scientific software easy. Spack is not tied to a particular language; you can build a software stack in Python or R, link to libraries written in C, C++, or Fortran, and easily swap compilers or target specific CPU microarchitectures.\n\nNextflow has built-in support for Spack that allows the configuration of workflow dependencies using Spack recipes and environment files.\n\nThis allows Nextflow applications to build packages from source on the compute infrastructure in use, whilst taking advantage of the configuration flexibility provided by Nextflow. At shared compute facilities where Spack has been configured by the administrators, this may result in optimized builds without user intervention. With appropriate options, this also permits end users to customize binary optimizations by themselves.\n\n## Prerequisites\n\nThis feature requires the [Spack](https://spack.io) package manager to be installed on your system.\n\n## How it works\n\nNextflow automatically creates and activates the Spack environment(s) given the dependencies specified by each process.\n\nDependencies are specified by using the {ref}`process-spack` directive, providing either the names of the required Spack packages, the path of a Spack environment yaml file or the path of an existing Spack environment directory.\n\n:::{note}\nSpack always installs the software packages in its own directories, regardless of the Nextflow specifications. The Spack environment created by Nextflow only contains symbolic links pointing to the appropriate package locations, and therefore it is relatively small in size.\n:::\n\nYou can specify the directory where the Spack environment is stored using the `spack.cacheDir` configuration property (see the {ref}`configuration page <config-spack>` for details). When using a computing cluster, make sure to use a shared file system path accessible from all compute nodes.\n\n:::{warning}\nThe Spack environment feature is not supported by executors that use remote object storage as the work directory, e.g. AWS Batch.\n:::\n\n### Enabling Spack environment\n\nThe use of Spack recipes specified using the {ref}`process-spack` directive needs to be enabled explicitly by setting the option shown below in the pipeline configuration file (i.e. `nextflow.config`):\n\n```groovy\nspack.enabled = true\n```\n\nAlternatively, it can be specified by setting the variable `NXF_SPACK_ENABLED=true` in your environment or by using the `-with-spack` command line option.\n\n### Use Spack package names\n\nSpack package names can specified using the `spack` directive. Multiple package names can be specified by separating them with a blank space. For example:\n\n```groovy\nprocess foo {\n  spack 'bwa samtools py-multiqc'\n\n  '''\n  your_command --here\n  '''\n}\n```\n\nUsing the above definition, a Spack environment that includes BWA, Samtools, and MultiQC tools is created and activated when the process is executed.\n\nThe usual Spack package syntax and naming conventions can be used. The version of a package can be specified after the package name like so: `bwa@0.7.15`.\n\nSpack is able to infer the local CPU microarchitecture and optimize the build accordingly. If you really need to customize this option, you can use the {ref}`process-arch` directive.\n\nRead the Spack documentation for more details about [package specifications](https://spack.readthedocs.io/en/latest/basic_usage.html#specs-dependencies).\n\n### Use Spack environment files\n\nSpack environments can also be defined using one or more Spack environment files. A Spack environment file is a YAML file that lists the required packages and channels. For example:\n\n```yaml\nspack:\n  specs:\n  -", "start_char_idx": 0, "end_char_idx": 3805, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "cdc4fccb-2879-4e38-b6ce-c9eb068f92a4": {"__data__": {"id_": "cdc4fccb-2879-4e38-b6ce-c9eb068f92a4", "embedding": null, "metadata": {"file_path": "docs/spack.md", "file_name": "spack.md"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "636b0952f819caa55fb9bafa992266d6e9ba4045", "node_type": null, "metadata": {"file_path": "docs/spack.md", "file_name": "spack.md"}, "hash": "7d8a786d99536f543c14004a43e55b4d18938b2c16f1dc0cbdb63add4b67742e"}, "2": {"node_id": "3038151e-cec1-4531-a55a-943885881fcf", "node_type": null, "metadata": {"file_path": "docs/spack.md", "file_name": "spack.md"}, "hash": "24540141f71e88ed0dd3142833e76da2fef1af52dc75489cac2ddf8fe73c1367"}, "3": {"node_id": "55081324-bdaf-4f4f-83b1-ef63c409f632", "node_type": null, "metadata": {"file_path": "docs/spack.md", "file_name": "spack.md"}, "hash": "5d6db4b8450fd8fde2043329398f4ea110bbf0d09d7bbf9d795a38805b6b442e"}}, "hash": "894adf8ef4bb1d51765246e1dcfc2f82ee2bb0af6bfaefd8baeb14bd046affde", "text": "example:\n\n```yaml\nspack:\n  specs:\n  - star@2.5.4a\n  - bwa@0.7.15\n\n  concretizer:\n    unify: true\n```\n\nHere, the `concretizer` option is a sensible default for Spack environments.\n\n:::{note}\nWhen creating a Spack environment, Nextflow always enables the corresponding Spack view. This is required by Nextflow to locate executables at pipeline runtime.\n:::\n\nAs mentioned above, Spack is able to guess the target CPU microarchitecture and optimize the build accordingly. If you really need to customize this option, we advise to use the {ref}`process-arch` directive rather than the available options for the Spack environment file.\n\nRead the Spack documentation for more details about how to create [environment files](https://spack.readthedocs.io/en/latest/environments.html).\n\nThe path of an environment file can be specified using the `spack` directive:\n\n```groovy\nprocess foo {\n  spack '/some/path/my-env.yaml'\n\n  '''\n  your_command --here\n  '''\n}\n```\n\n:::{warning}\nThe environment file name **must** have a `.yaml` extension or else it won't be properly recognized.\n:::\n\n### Use existing Spack environments\n\nIf you already have a local Spack environment, you can use it in your workflow specifying the installation directory of such environment by using the `spack` directive:\n\n```groovy\nprocess foo {\n  spack '/path/to/an/existing/env/directory'\n\n  '''\n  your_command --here\n  '''\n}\n```\n\n## Best practices\n\n### Building Spack packages for Nextflow pipelines\n\nSpack builds most software package from their source codes, and it does this for a request package and for all its required dependencies. As a result, Spack builds can last for long, even several hours. This can represent an inconvenience, in that it can significantly lengthen the duration of Nextflow processes. Here we briefly discuss two strategies to mitigate this aspect, and render the usage of Spack more effective.\n\n1. Use a Spack yaml file, and pre-build the environment outside of Nextflow, prior to running the pipeline.\n   Building packages outside of the Nextflow pipeline will work since Spack always installs packages in its own directories,\n   and only creates symbolic links in the environment. This sequence of commands will do the trick in most cases:\n\n   ```bash\n   spack env create myenv /path/to/spack.yaml\n   spack env activate myenv\n   spack env view enable\n   spack concretize -f\n   spack install -y\n   spack env deactivate\n   ```\n\n2. Use the Nextflow stub functionality prior to running the pipeline for production.\n   Nextflow will run the stub pipeline, skipping process executions but still setting up the required software packages.\n   This option is useful if it is not possible to write a Spack yaml file for the environment.\n   The stub functionality is described in the {ref}`Stub <process-stub>` section of the Processes page.\n\n### Configuration file\n\nWhen the `spack` directive is used in any `process` definition within the pipeline script, Spack is required for the pipeline execution.\n\nSpecifying the Spack environments in a separate configuration {ref}`profile <config-profiles>` is therefore\nrecommended to allow the execution via a command line option and to enhance the pipeline portability. For example:\n\n```groovy\nprofiles {\n  spack {\n    process.spack = 'samtools'\n  }\n\n  docker {\n    process.container = 'biocontainers/samtools'\n    docker.enabled = true\n  }\n}\n```\n\nThe above configuration", "start_char_idx": 3773, "end_char_idx": 7173, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "55081324-bdaf-4f4f-83b1-ef63c409f632": {"__data__": {"id_": "55081324-bdaf-4f4f-83b1-ef63c409f632", "embedding": null, "metadata": {"file_path": "docs/spack.md", "file_name": "spack.md"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "636b0952f819caa55fb9bafa992266d6e9ba4045", "node_type": null, "metadata": {"file_path": "docs/spack.md", "file_name": "spack.md"}, "hash": "7d8a786d99536f543c14004a43e55b4d18938b2c16f1dc0cbdb63add4b67742e"}, "2": {"node_id": "cdc4fccb-2879-4e38-b6ce-c9eb068f92a4", "node_type": null, "metadata": {"file_path": "docs/spack.md", "file_name": "spack.md"}, "hash": "894adf8ef4bb1d51765246e1dcfc2f82ee2bb0af6bfaefd8baeb14bd046affde"}}, "hash": "5d6db4b8450fd8fde2043329398f4ea110bbf0d09d7bbf9d795a38805b6b442e", "text": " docker.enabled = true\n  }\n}\n```\n\nThe above configuration snippet allows the execution either with Spack or Docker by specifying `-profile spack` or\n`-profile docker` when running the pipeline script.\n\n## Advanced settings\n\nSpack advanced configuration settings are described in the {ref}`Spack <config-spack>` section on the Nextflow configuration page.", "start_char_idx": 7149, "end_char_idx": 7503, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "27774c1b-5464-4b2d-9ce3-98c622896d64": {"__data__": {"id_": "27774c1b-5464-4b2d-9ce3-98c622896d64", "embedding": null, "metadata": {"file_path": "docs/tracing.md", "file_name": "tracing.md"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "b53cbf19b410703f4d5108405c739c7cce41cf35", "node_type": null, "metadata": {"file_path": "docs/tracing.md", "file_name": "tracing.md"}, "hash": "e17e248f6b86a2071215f8daae1a979bf3b3f798d60d5f0b49145ed1730fd20b"}, "3": {"node_id": "f7a91cab-6829-4f8b-a8d1-ab23466363f1", "node_type": null, "metadata": {"file_path": "docs/tracing.md", "file_name": "tracing.md"}, "hash": "f23af7db56e7f14ec283fd2666808a2f1f83b894838e2713be67643cde81be66"}}, "hash": "3fb7357f9bd0fd1a585f472997715ff8f751442c795149a04608289c7ba044a6", "text": "(tracing-page)=\n\n# Tracing & visualisation\n\n(execution-log)=\n\n## Execution log\n\nThe `nextflow log` command shows information about executed pipelines in the current folder:\n\n```bash\nnextflow log <run name> [options]\n```\n\n:::{note}\nBoth the {ref}`execution report <execution-report>` and the {ref}`trace report <trace-report>` must be specified when the pipeline is first called. By contrast, the `log` option is useful after a pipeline has already run and is available for every executed pipeline.\n:::\n\nBy default, `log` prints the list of executed pipelines:\n\n```console\n$ nextflow log\nTIMESTAMP            RUN NAME         SESSION ID                            COMMAND\n2016-08-01 11:44:51  grave_poincare   18cbe2d3-d1b7-4030-8df4-ae6c42abaa9c  nextflow run hello\n2016-08-01 11:44:55  small_goldstine  18cbe2d3-d1b7-4030-8df4-ae6c42abaa9c  nextflow run hello -resume\n2016-08-01 11:45:09  goofy_kilby      0a1f1589-bd0e-4cfc-b688-34a03810735e  nextflow run rnatoy -with-docker\n```\n\nSpecifying a run name or session id prints tasks executed by that pipeline run:\n\n```console\n$ nextflow log goofy_kilby\n/Users/../work/0b/be0d1c4b6fd6c778d509caa3565b64\n/Users/../work/ec/3100e79e21c28a12ec2204304c1081\n/Users/../work/7d/eb4d4471d04cec3c69523aab599fd4\n/Users/../work/8f/d5a26b17b40374d37338ccfe967a30\n/Users/../work/94/dfdfb63d5816c9c65889ae34511b32\n```\n\n### Customizing fields\n\nBy default, only the task execution paths are printed. A custom list of fields to print can be provided via the `-f` (`-fields`) option. For example:\n\n```console\n$ nextflow log goofy_kilby -f hash,name,exit,status\n0b/be0d1c  buildIndex (ggal_1_48850000_49020000.Ggal71.500bpflank)  0  COMPLETED\nec/3100e7  mapping (ggal_gut)                                       0  COMPLETED\n7d/eb4d44  mapping (ggal_liver)                                     0  COMPLETED\n8f/d5a26b  makeTranscript (ggal_liver)                              0  COMPLETED\n94/dfdfb6  makeTranscript (ggal_gut)                                0  COMPLETED\n```\n\nThe fields accepted by the `-f` options are the ones in the {ref}`trace report<trace-fields>`, as well as: script, stdout, stderr, env. List available fields using the `-l` (`-list-fields`) option.\n\nThe `script` field is useful for examining script", "start_char_idx": 0, "end_char_idx": 2248, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "f7a91cab-6829-4f8b-a8d1-ab23466363f1": {"__data__": {"id_": "f7a91cab-6829-4f8b-a8d1-ab23466363f1", "embedding": null, "metadata": {"file_path": "docs/tracing.md", "file_name": "tracing.md"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "b53cbf19b410703f4d5108405c739c7cce41cf35", "node_type": null, "metadata": {"file_path": "docs/tracing.md", "file_name": "tracing.md"}, "hash": "e17e248f6b86a2071215f8daae1a979bf3b3f798d60d5f0b49145ed1730fd20b"}, "2": {"node_id": "27774c1b-5464-4b2d-9ce3-98c622896d64", "node_type": null, "metadata": {"file_path": "docs/tracing.md", "file_name": "tracing.md"}, "hash": "3fb7357f9bd0fd1a585f472997715ff8f751442c795149a04608289c7ba044a6"}, "3": {"node_id": "47b441db-a090-45d4-8a71-b5b5130d3c3d", "node_type": null, "metadata": {"file_path": "docs/tracing.md", "file_name": "tracing.md"}, "hash": "ed94e0f83100bcd325b097271d61c3a8b07c5a6609b6895b57e4b0813ccd9b0e"}}, "hash": "f23af7db56e7f14ec283fd2666808a2f1f83b894838e2713be67643cde81be66", "text": "option.\n\nThe `script` field is useful for examining script commands run in each task:\n\n```console\n$ nextflow log goofy_kilby -f name,status,script\nalign_genome      COMPLETED\n   bowtie --index /data/genome input.fastq > output\n...\n```\n\n### Templates\n\nThe `-t` option allows a template (string or file) to be specified. This makes it possible to create complex custom reports in any text-based format. For example, you could save this Markdown snippet to a file:\n\n```md\n## $name\n\nscript:\n\n    $script\n\nexist status: $exit\ntask status: $status\ntask folder: $folder\n```\n\nThen, the following command will output a markdown file containing the script, exit status and folder of all executed tasks:\n\n```bash\nnextflow log goofy_kilby -t my-template.md > execution-report.md\n```\n\n### Filtering\n\nThe `filter` option makes it possible to select which entries to include in the log report. Any valid groovy boolean expression on the log fields can be used to define the filter condition. For example:\n\n```bash\nnextflow log goofy_kilby -filter 'name =~ /foo.*/ && status == \"FAILED\"'\n```\n\n(execution-report)=\n\n## Execution report\n\nNextflow can create an HTML execution report: a single document which includes many useful metrics about a workflow execution. The report is organised in the three main sections: `Summary`, `Resources` and `Tasks` (see below for details).\n\nTo enable the creation of this report add the `-with-report` command line option when launching the pipeline execution. For example:\n\n```bash\nnextflow run <pipeline name> -with-report [file name]\n```\n\nThe report file name can be specified as an optional parameter following the report option.\n\n### Summary\n\nThe `Summary` section reports the execution status, the launch command, overall execution time and some other workflow metadata. You can see an example below:\n\n```{image} images/report-summary-min.png\n```\n\n### Resource Usage\n\nThe `Resources` section plots the distribution of resource usage for each workflow process using the interactive [plotly.js](https://plot.ly/javascript/) plotting library.\n\nPlots are shown for CPU, memory, job duration and disk I/O. They have two (or three) tabs with the raw values and a percentage representation showing what proportion of the requested resources were used. These plots are very helpful to check that task resources are used efficiently.\n\n```{image} images/report-resource-cpu.png\n```\n\nLearn more about how resource usage is computed in the {ref}`Metrics documentation <metrics-page>`.\n\n(execution-report-tasks)=\n\n### Tasks\n\nThe `Tasks` section lists all executed tasks, reporting for each of them the status, the actual command script, and many other metrics. You can see an example below:\n\n```{image} images/report-tasks-min.png\n```\n\n:::{note}\nNextflow collects these metrics through a background process for each job in the target environment. Make sure the following tools are available in the environment where tasks are executed: `awk`, `date`, `grep`, `ps`, `sed`, `tail`, `tee`. Moreover, some of these metrics are not reported when running on Mac OS X. See the note about that in the [Trace report](#trace-report) below.\n:::\n\n:::{warning}\nA common problem when using a third party container image is that it does not include one or more of the above utilities, resulting in an empty execution report.\n:::\n\nPlease read {ref}`Report scope <config-report>` section to learn more about the execution report configuration details.\n\n(trace-report)=\n\n## Trace report\n\nNextflow creates an execution tracing file that contains some useful information about each process executed in your pipeline script, including: submission", "start_char_idx": 2198, "end_char_idx": 5834, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "47b441db-a090-45d4-8a71-b5b5130d3c3d": {"__data__": {"id_": "47b441db-a090-45d4-8a71-b5b5130d3c3d", "embedding": null, "metadata": {"file_path": "docs/tracing.md", "file_name": "tracing.md"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "b53cbf19b410703f4d5108405c739c7cce41cf35", "node_type": null, "metadata": {"file_path": "docs/tracing.md", "file_name": "tracing.md"}, "hash": "e17e248f6b86a2071215f8daae1a979bf3b3f798d60d5f0b49145ed1730fd20b"}, "2": {"node_id": "f7a91cab-6829-4f8b-a8d1-ab23466363f1", "node_type": null, "metadata": {"file_path": "docs/tracing.md", "file_name": "tracing.md"}, "hash": "f23af7db56e7f14ec283fd2666808a2f1f83b894838e2713be67643cde81be66"}, "3": {"node_id": "7502534f-4fc0-47cf-a2e1-376d24608c28", "node_type": null, "metadata": {"file_path": "docs/tracing.md", "file_name": "tracing.md"}, "hash": "dab2277a5e396baa31b9d5d9eb2222c98fabd958ea579edccee04ae368a5495d"}}, "hash": "ed94e0f83100bcd325b097271d61c3a8b07c5a6609b6895b57e4b0813ccd9b0e", "text": "some useful information about each process executed in your pipeline script, including: submission time, start time, completion time, cpu and memory used.\n\nIn order to create the execution trace file add the `-with-trace` command line option when launching the pipeline execution. For example:\n\n```bash\nnextflow run <pipeline name> -with-trace\n```\n\nIt will create a file named `trace.txt` in the current directory. The content looks like the above example:\n\n| task_id | hash      | native_id | name           | status    | exit | submit                  | duration | walltime | %cpu   | rss      | vmem     | rchar    | wchar    |\n| ------- | --------- | --------- | -------------- | --------- | ---- | ----------------------- | -------- | -------- | ------ | -------- | -------- | -------- | -------- |\n| 19      | 45/ab752a | 2032      | blast (1)      | COMPLETED | 0    | 2014-10-23 16:33:16.288 | 1m       | 5s       | 0.0%   | 29.8 MB  | 354 MB   | 33.3 MB  | 0        |\n| 20      | 72/db873d | 2033      | blast (2)      | COMPLETED | 0    | 2014-10-23 16:34:17.211 | 30s      | 10s      | 35.7%  | 152.8 MB | 428.1 MB | 192.7 MB | 1 MB     |\n| 21      | 53/d13188 | 2034      | blast (3)      | COMPLETED | 0    | 2014-10-23 16:34:17.518 | 29s      | 20s      | 4.5%   | 289.5 MB | 381.6 MB | 33.3 MB  | 0        |\n| 22      | 26/f65116 | 2035      | blast (4)      | COMPLETED | 0    | 2014-10-23 16:34:18.459 | 30s      | 9s       | 6.0%   | 122.8 MB | 353.4 MB | 33.3 MB  | 0        |\n| 23      | 88/bc00e4 | 2036      | blast (5)      | COMPLETED | 0    | 2014-10-23 16:34:18.507 | 30s      | 19s      | 5.0%   | 195 MB   | 395.8 MB | 65.3 MB  | 121 KB   |\n| 24      | 74/2556e9 | 2037      | blast (6)      | COMPLETED | 0    | 2014-10-23 16:34:18.553 | 30s      | 12s      | 43.6%  | 140.7 MB | 432.2 MB | 192.7 MB | 182.7 MB |\n| 28      | b4/0f9613 | 2041      | exonerate (1)  | COMPLETED | 0    | 2014-10-23 16:38:19.657 | 1m 30s   | 1m 11s   | 94.3%  | 611.6 MB | 693.8 MB | 961.2 GB | 6.1 GB   |\n| 32      | af/7f2f57 | 2044      | exonerate (4)  | COMPLETED | 0    | 2014-10-23 16:46:50.902 | 1m 1s ", "start_char_idx": 5800, "end_char_idx": 7919, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "7502534f-4fc0-47cf-a2e1-376d24608c28": {"__data__": {"id_": "7502534f-4fc0-47cf-a2e1-376d24608c28", "embedding": null, "metadata": {"file_path": "docs/tracing.md", "file_name": "tracing.md"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "b53cbf19b410703f4d5108405c739c7cce41cf35", "node_type": null, "metadata": {"file_path": "docs/tracing.md", "file_name": "tracing.md"}, "hash": "e17e248f6b86a2071215f8daae1a979bf3b3f798d60d5f0b49145ed1730fd20b"}, "2": {"node_id": "47b441db-a090-45d4-8a71-b5b5130d3c3d", "node_type": null, "metadata": {"file_path": "docs/tracing.md", "file_name": "tracing.md"}, "hash": "ed94e0f83100bcd325b097271d61c3a8b07c5a6609b6895b57e4b0813ccd9b0e"}, "3": {"node_id": "b6a9a829-f63e-40f5-a777-385c8e1ac15f", "node_type": null, "metadata": {"file_path": "docs/tracing.md", "file_name": "tracing.md"}, "hash": "94483832c4d78e1ad98354913495ad1f09536d7ab6eb7fe826fa789788ca0e62"}}, "hash": "dab2277a5e396baa31b9d5d9eb2222c98fabd958ea579edccee04ae368a5495d", "text": "| 2014-10-23 16:46:50.902 | 1m 1s    | 38s      | 36.6%  | 115.8 MB | 167.8 MB | 364 GB   | 5.1 GB   |\n| 33      | 37/ab1fcc | 2045      | exonerate (5)  | COMPLETED | 0    | 2014-10-23 16:47:51.625 | 30s      | 12s      | 59.6%  | 696 MB   | 734.6 MB | 354.3 GB | 420.4 MB |\n| 31      | d7/eabe51 | 2042      | exonerate (3)  | COMPLETED | 0    | 2014-10-23 16:45:50.846 | 3m 1s    | 2m 6s    | 130.1% | 703.3 MB | 760.9 MB | 1.1 TB   | 28.6 GB  |\n| 36      | c4/d6cc15 | 2048      | exonerate (6)  | COMPLETED | 0    | 2014-10-23 16:48:48.718 | 3m 1s    | 2m 43s   | 116.6% | 682.1 MB | 743.6 MB | 868.5 GB | 42 GB    |\n| 30      | 4f/1ad1f0 | 2043      | exonerate (2)  | COMPLETED | 0    | 2014-10-23 16:45:50.961 | 10m 2s   | 9m 16s   | 95.5%  | 706.2 MB | 764 MB   | 1.6 TB   | 172.4 GB |\n| 52      | 72/41d0c6 | 2055      | similarity (1) | COMPLETED | 0    | 2014-10-23 17:13:23.543 | 30s      | 352ms    | 0.0%   | 35.6 MB  | 58.3 MB  | 199.3 MB | 7.9 MB   |\n| 57      | 9b/111b5e | 2058      | similarity (6) | COMPLETED | 0    | 2014-10-23 17:13:23.655 | 30s      | 488ms    | 0.0%   | 108.2 MB | 158 MB   | 317.1 MB | 9.8 MB   |\n| 53      | 3e/bca30f | 2061      | similarity (2) | COMPLETED | 0    | 2014-10-23 17:13:23.770 | 30s      | 238ms    | 0.0%   | 6.7 MB   | 29.6 MB  | 190 MB   | 91.2 MB  |\n| 54      | 8b/d45b47 | 2062      | similarity (3) | COMPLETED | 0    | 2014-10-23 17:13:23.808 | 30s      | 442ms    | 0.0%   | 108.1 MB | 158 MB   | 832 MB   | 565.6 MB |\n| 55      | 51/ac19c6 | 2064      | similarity (4) | COMPLETED | 0    | 2014-10-23 17:13:23.873 | 30s      | 6s       | 0.0%   | 112.7 MB | 162.8 MB | 4.9 GB   | 3.9 GB   |\n| 56      | c3/ec5f4a | 2066      | similarity (5) | COMPLETED | 0    | 2014-10-23 17:13:23.948 | 30s      | 616ms    |", "start_char_idx": 7978, "end_char_idx": 9757, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "b6a9a829-f63e-40f5-a777-385c8e1ac15f": {"__data__": {"id_": "b6a9a829-f63e-40f5-a777-385c8e1ac15f", "embedding": null, "metadata": {"file_path": "docs/tracing.md", "file_name": "tracing.md"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "b53cbf19b410703f4d5108405c739c7cce41cf35", "node_type": null, "metadata": {"file_path": "docs/tracing.md", "file_name": "tracing.md"}, "hash": "e17e248f6b86a2071215f8daae1a979bf3b3f798d60d5f0b49145ed1730fd20b"}, "2": {"node_id": "7502534f-4fc0-47cf-a2e1-376d24608c28", "node_type": null, "metadata": {"file_path": "docs/tracing.md", "file_name": "tracing.md"}, "hash": "dab2277a5e396baa31b9d5d9eb2222c98fabd958ea579edccee04ae368a5495d"}, "3": {"node_id": "631c1fc8-e333-4113-b6de-541d0f6699a6", "node_type": null, "metadata": {"file_path": "docs/tracing.md", "file_name": "tracing.md"}, "hash": "7a556aa2343dd89ba69fbfee67be849c5c1709c1f5976398909bdb04f3a2ac68"}}, "hash": "94483832c4d78e1ad98354913495ad1f09536d7ab6eb7fe826fa789788ca0e62", "text": "| 30s      | 616ms    | 0.0%   | 10.4 MB  | 34.6 MB  | 238 MB   | 8.4 MB   |\n| 98      | de/d6c0a6 | 2099      | matrix (1)     | COMPLETED | 0    | 2014-10-23 17:14:27.139 | 30s      | 1s       | 0.0%   | 4.8 MB   | 42 MB    | 240.6 MB | 79 KB    |\n\n(trace-fields)=\n\nThe following table shows the fields that can be included in the execution report:\n\n`task_id`\n: Task ID.\n\n`hash`\n: Task hash code.\n\n`native_id`\n: Task ID given by the underlying execution system e.g. POSIX process PID when executed locally, job ID when executed by a grid engine, etc.\n\n`process`\n: Nextflow process name.\n\n`tag`\n: User provided identifier associated this task.\n\n`name`\n: Task name.\n\n`status`\n: Task status. Possible values are: `NEW`, `SUBMITTED`, `RUNNING`, `COMPLETED`, `FAILED`, and `ABORTED`.\n\n`exit`\n: POSIX process exit status.\n\n`module`\n: Environment module used to run the task.\n\n`container`\n: Docker image name used to execute the task.\n\n`cpus`\n: The cpus number request for the task execution.\n\n`time`\n: The time request for the task execution\n\n`disk`\n: The disk space request for the task execution.\n\n`memory`\n: The memory request for the task execution.\n\n`attempt`\n: Attempt at which the task completed.\n\n`submit`\n: Timestamp when the task has been submitted.\n\n`start`\n: Timestamp when the task execution has started.\n\n`complete`\n: Timestamp when task execution has completed.\n\n`duration`\n: Time elapsed to complete since the submission.\n\n`realtime`\n: Task execution time i.e. delta between completion and start timestamp.\n\n`queue`\n: The queue that the executor attempted to run the process on.\n\n`%cpu`\n: Percentage of CPU used by the process.\n\n`%mem`\n: Percentage of memory used by the process.\n\n`rss`\n: Real memory (resident set) size of the process. Equivalent to `ps -o rss` .\n\n`vmem`\n: Virtual memory size of the process. Equivalent to `ps -o vsize` .\n\n`peak_rss`\n: Peak of real memory. This data is read from field `VmHWM` in `/proc/$pid/status` file.\n\n`peak_vmem`\n: Peak of virtual memory. This data is read from field `VmPeak` in `/proc/$pid/status` file.\n\n`rchar`\n: Number of bytes the process read, using any read-like system call from files, pipes, tty, etc. This data is read from file `/proc/$pid/io`.\n\n`wchar`\n: Number of bytes the process wrote, using any write-like system call. This data is read from file `/proc/$pid/io`.\n\n`syscr`\n: Number of read-like system call invocations that the process performed. This data is read from file `/proc/$pid/io`.\n\n`syscw`\n: Number of write-like system call invocations that the process performed. This data is read from file `/proc/$pid/io`.\n\n`read_bytes`\n: Number of bytes the process directly read from disk. This data is read from file `/proc/$pid/io`.\n\n`write_bytes`\n: Number of bytes the process originally dirtied in the page-cache (assuming they will go to disk later). This data is read from file `/proc/$pid/io`.\n\n`vol_ctxt`\n: Number of voluntary context", "start_char_idx": 9775, "end_char_idx": 12689, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "631c1fc8-e333-4113-b6de-541d0f6699a6": {"__data__": {"id_": "631c1fc8-e333-4113-b6de-541d0f6699a6", "embedding": null, "metadata": {"file_path": "docs/tracing.md", "file_name": "tracing.md"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "b53cbf19b410703f4d5108405c739c7cce41cf35", "node_type": null, "metadata": {"file_path": "docs/tracing.md", "file_name": "tracing.md"}, "hash": "e17e248f6b86a2071215f8daae1a979bf3b3f798d60d5f0b49145ed1730fd20b"}, "2": {"node_id": "b6a9a829-f63e-40f5-a777-385c8e1ac15f", "node_type": null, "metadata": {"file_path": "docs/tracing.md", "file_name": "tracing.md"}, "hash": "94483832c4d78e1ad98354913495ad1f09536d7ab6eb7fe826fa789788ca0e62"}, "3": {"node_id": "eb83dd1a-d5c1-42b6-901b-d0d6c201a0e4", "node_type": null, "metadata": {"file_path": "docs/tracing.md", "file_name": "tracing.md"}, "hash": "55cc86fe58ce8b17c8692cb408e57252abce3a27a7b596f2145355136f78e4a0"}}, "hash": "7a556aa2343dd89ba69fbfee67be849c5c1709c1f5976398909bdb04f3a2ac68", "text": "Number of voluntary context switches.\n\n`inv_ctxt`\n: Number of involuntary context switches.\n\n`env`\n: The variables defined in task execution environment.\n\n`workdir`\n: The directory path where the task was executed.\n\n`script`\n: The task command script.\n\n`scratch`\n: The value of the process `scratch` directive.\n\n`error_action`\n: The action applied on errof task failure.\n\n`hostname`\n: :::{versionadded} 22.05.0-edge\n  :::\n: The host on which the task was executed. Supported only for the Kubernetes executor yet. Activate with `k8s.fetchNodeName = true` in the Nextflow config file.\n\n:::{note}\nThese metrics provide an estimation of the resources used by running tasks. They are not an alternative to low-level performance analysis tools, and they may not be completely accurate, especially for very short-lived tasks (running for less than a few seconds).\n:::\n\nTrace report layout and other configuration settings can be specified by using the `nextflow.config` configuration file.\n\nPlease read {ref}`Trace scope <config-trace>` section to learn more about it.\n\n(timeline-report)=\n\n## Timeline report\n\nNextflow can render an HTML timeline for all processes executed in your pipeline. An example of the timeline report is shown below:\n\n```{image} images/timeline-min.png\n```\n\nEach bar represents a process run in the pipeline execution. The bar length represents the task duration time (wall-time). The colored area in each bar represents the real execution time. The grey area to the *left* of the colored area represents the task scheduling wait time. The grey area to the *right* of the colored area represents the task termination time (clean-up and file un-staging). The numbers on the x-axis represent the time in absolute units e.g. minutes, hours, etc.\n\nEach bar displays two numbers: the task duration time and the virtual memory size peak.\n\nAs each process can spawn many tasks, colors are used to identify those tasks belonging to the same process.\n\nTo enable the creation of the timeline report add the `-with-timeline` command line option when launching the pipeline execution. For example:\n\n```bash\nnextflow run <pipeline name> -with-timeline [file name]\n```\n\nThe report file name can be specified as an optional parameter following the timeline option.\n\n(dag-visualisation)=\n\n## DAG visualisation\n\nA Nextflow pipeline is implicitly modelled by a direct acyclic graph (DAG). The vertices in the graph represent the pipeline's processes and operators, while the edges represent the data connections (i.e. channels) between them.\n\nThe pipeline execution DAG can be outputted by adding the `-with-dag` option to the run command line. It creates a file named `dag.dot` containing a textual representation of the pipeline execution graph in the [DOT format](http://www.graphviz.org/content/dot-language).\n\nThe execution DAG can be rendered in a different format by specifying an output file name which has an extension corresponding to the required format. For example:\n\n```bash\nnextflow run <script-name> -with-dag flowchart.png\n```\n\nList of supported file formats:\n\n| Extension | File format                     |\n| --------- | ------------------------------- |\n| dot       | Graphviz DOT file               |\n| html      | HTML file                       |\n| mmd       | Mermaid diagram                 |\n| pdf       | PDF file (\\*)  ", "start_char_idx": 12677, "end_char_idx": 16023, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "eb83dd1a-d5c1-42b6-901b-d0d6c201a0e4": {"__data__": {"id_": "eb83dd1a-d5c1-42b6-901b-d0d6c201a0e4", "embedding": null, "metadata": {"file_path": "docs/tracing.md", "file_name": "tracing.md"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "b53cbf19b410703f4d5108405c739c7cce41cf35", "node_type": null, "metadata": {"file_path": "docs/tracing.md", "file_name": "tracing.md"}, "hash": "e17e248f6b86a2071215f8daae1a979bf3b3f798d60d5f0b49145ed1730fd20b"}, "2": {"node_id": "631c1fc8-e333-4113-b6de-541d0f6699a6", "node_type": null, "metadata": {"file_path": "docs/tracing.md", "file_name": "tracing.md"}, "hash": "7a556aa2343dd89ba69fbfee67be849c5c1709c1f5976398909bdb04f3a2ac68"}, "3": {"node_id": "2cdbca24-3ddc-4512-b8fb-6a4096afc86a", "node_type": null, "metadata": {"file_path": "docs/tracing.md", "file_name": "tracing.md"}, "hash": "ce56f6586639cf91984f7591d1c7d17df7f615d1162066b3060ec6f42d4c7d4e"}}, "hash": "55cc86fe58ce8b17c8692cb408e57252abce3a27a7b596f2145355136f78e4a0", "text": "   |\n| pdf       | PDF file (\\*)                   |\n| png       | PNG file (\\*)                   |\n| svg       | SVG file (\\*)                   |\n| gexf      | Graph Exchange XML file (Gephi) |\n\n:::{note}\nFile formats marked with \"\\*\" require the [Graphviz](http://www.graphviz.org) tool to be installed.\n:::\n\nThe DAG produced by Nextflow for the [Unistrap](https://github.com/cbcrg/unistrap/) pipeline:\n\n```{image} images/dag.png\n```\n\n### Mermaid diagram\n\n:::{versionadded} 22.04.0\n:::\n\nNextflow can render the DAG as a [Mermaid](https://mermaid-js.github.io/) diagram. Mermaid diagrams are particularly useful because they can be embedded in [GitHub Flavored Markdown](https://github.blog/2022-02-14-include-diagrams-markdown-files-mermaid/) without having to render them yourself. You can customize the diagram with CSS, and you can even add links! Visit the [Mermaid documentation](https://mermaid-js.github.io/mermaid/#/flowchart?id=styling-and-classes) for details.\n\nHere is the Mermaid diagram produced by Nextflow for the above example:\n\n```mermaid\nflowchart TD\n    p0((Channel.fromPath))\n    p1([ifEmpty])\n    p2[get_shuffle_replicates]\n    p3[get_msa_replicates]\n    p4[get_msa_trees]\n    p5([collectFile])\n    p6([first])\n    p7[get_stable_msa_trees]\n    p8(( ))\n    p9[get_seqboot_replicates]\n    p10[get_replicate_trees]\n    p11([collectFile])\n    p12([max])\n    p13[get_shootstrap_tree]\n    p14(( ))\n    p0 --> p1\n    p1 -->|file_names| p2\n    p2 -->|shuffle_replicates| p3\n    p3 -->|msa_replicates| p4\n    p3 -->|msa_replicates2| p9\n    p4 -->|msa_trees| p7\n    p4 -->|msa_trees2| p5\n    p5 --> p6\n    p6 --> p7\n    p7 -->|stable_trees| p8\n    p7 -->|most_stable_tree| p12\n    p9 -->|replicates| p10\n    p10 -->|trees| p11\n    p11 --> p13\n    p12 --> p13\n    p13 -->|shootstrap_tree| p14\n```\n\nAnd the final image produced with the [Mermaid Live Editor](https://mermaid-js.github.io/mermaid-live-editor/edit) (using the `default` theme):\n\n```{image} images/dag-mermaid.png\n```\n\n(weblog-service)=\n\n## Weblog via HTTP\n\nNextflow can send detailed workflow execution metadata and runtime statistics to a HTTP endpoint. To enable this feature, use the `-with-weblog` as shown below:\n\n```bash\nnextflow run <pipeline name> -with-weblog [url]\n```\n\nWorkflow events are sent as HTTP POST requests to the given URL. The message consists of the following JSON structure:\n\n```json\n{\n  \"runName\":", "start_char_idx": 16030, "end_char_idx": 18430, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "2cdbca24-3ddc-4512-b8fb-6a4096afc86a": {"__data__": {"id_": "2cdbca24-3ddc-4512-b8fb-6a4096afc86a", "embedding": null, "metadata": {"file_path": "docs/tracing.md", "file_name": "tracing.md"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "b53cbf19b410703f4d5108405c739c7cce41cf35", "node_type": null, "metadata": {"file_path": "docs/tracing.md", "file_name": "tracing.md"}, "hash": "e17e248f6b86a2071215f8daae1a979bf3b3f798d60d5f0b49145ed1730fd20b"}, "2": {"node_id": "eb83dd1a-d5c1-42b6-901b-d0d6c201a0e4", "node_type": null, "metadata": {"file_path": "docs/tracing.md", "file_name": "tracing.md"}, "hash": "55cc86fe58ce8b17c8692cb408e57252abce3a27a7b596f2145355136f78e4a0"}, "3": {"node_id": "46a61c50-ef38-4639-a4f2-f123b1869a18", "node_type": null, "metadata": {"file_path": "docs/tracing.md", "file_name": "tracing.md"}, "hash": "a861f8b20c72114855bbd71b82511381a20251093324b669b1dc02afc1b26c2f"}}, "hash": "ce56f6586639cf91984f7591d1c7d17df7f615d1162066b3060ec6f42d4c7d4e", "text": "the following JSON structure:\n\n```json\n{\n  \"runName\": \"<run name>\",\n  \"runId\": \"<uuid>\",\n  \"event\": \"<started|process_submitted|process_started|process_completed|error|completed>\",\n  \"utcTime\": \"<UTC timestamp>\",\n  \"trace\": {  },\n  \"metadata\": {  }\n}\n```\n\nThe JSON object contains the following attributes:\n\n`runName`\n: The workflow execution run name.\n\n`runId`\n: The workflow execution unique ID.\n\n`event`\n: The workflow execution event. One of `started`, `process_submitted`, `process_started`, `process_completed`, `error`, `completed`.\n\n`utcTime`\n: The UTC timestamp in ISO 8601 format.\n\n`trace`\n: *Included only for the following events: `process_submitted`, `process_started`, `process_completed`, `error`*\n: The task runtime information as described in the {ref}`trace fields<trace-fields>` section.\n: The set of included fields is determined by the `trace.fields` setting in the Nextflow configuration file. See the {ref}`Trace configuration<config-trace>` and [Trace report](#trace-report) sections to learn more.\n\n`metadata`\n: *Included only for the following events: `started`, `completed`*\n: The workflow metadata including the {ref}`config manifest<config-manifest>`. For a list of all fields, have a look at the bottom message examples.\n\n### Example `started` event\n\nWhen a workflow execution is started, a message like the following is posted to the specified end-point. Be aware that the properties in the parameter scope will look different for your workflow. Here is an example output from the `nf-core/hlatyping` pipeline with the weblog feature enabled:\n\n```json\n{\n  \"runName\": \"friendly_pesquet\",\n  \"runId\": \"170aa09c-105f-49d0-99b4-8eb6a146e4a7\",\n  \"event\": \"started\",\n  \"utcTime\": \"2018-10-07T11:42:08Z\",\n  \"metadata\": {\n    \"params\": {\n      \"container\": \"nfcore/hlatyping:1.1.4\",\n      \"help\": false,\n      \"outdir\": \"results\",\n      \"bam\": true,\n      \"singleEnd\": false,\n      \"single-end\": false,\n      \"reads\": \"data/test*{1,2}.fq.gz\",\n      \"seqtype\": \"dna\",\n      \"solver\": \"glpk\",\n      \"igenomes_base\": \"./iGenomes\",\n      \"multiqc_config\": \"/Users/sven1103/.nextflow/assets/nf-core/hlatyping/conf/multiqc_config.yaml\",\n      \"clusterOptions\": false,\n      \"cluster-options\": false,\n      \"enumerations\": 1,\n      \"beta\": 0.009,\n      \"prefix\": \"hla_run\",\n      \"base_index\": \"/Users/sven1103/.nextflow/assets/nf-core/hlatyping/data/indices/yara/hla_reference_\",\n      \"index\": \"/Users/sven1103/.nextflow/assets/nf-core/hlatyping/data/indices/yara/hla_reference_dna\",\n      \"custom_config_version\": \"master\",\n      \"custom_config_base\": \"https://raw.githubusercontent.com/nf-core/configs/master\"\n    },\n    \"workflow\": {\n", "start_char_idx": 18401, "end_char_idx": 21055, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "46a61c50-ef38-4639-a4f2-f123b1869a18": {"__data__": {"id_": "46a61c50-ef38-4639-a4f2-f123b1869a18", "embedding": null, "metadata": {"file_path": "docs/tracing.md", "file_name": "tracing.md"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "b53cbf19b410703f4d5108405c739c7cce41cf35", "node_type": null, "metadata": {"file_path": "docs/tracing.md", "file_name": "tracing.md"}, "hash": "e17e248f6b86a2071215f8daae1a979bf3b3f798d60d5f0b49145ed1730fd20b"}, "2": {"node_id": "2cdbca24-3ddc-4512-b8fb-6a4096afc86a", "node_type": null, "metadata": {"file_path": "docs/tracing.md", "file_name": "tracing.md"}, "hash": "ce56f6586639cf91984f7591d1c7d17df7f615d1162066b3060ec6f42d4c7d4e"}, "3": {"node_id": "4ec64556-b6c0-4ffa-a53f-54b5ec9d9d4e", "node_type": null, "metadata": {"file_path": "docs/tracing.md", "file_name": "tracing.md"}, "hash": "8a313c97b7a19f8ed983a1bf5e97ff6b9ba0847044a052002b6a24581bd4ac77"}}, "hash": "a861f8b20c72114855bbd71b82511381a20251093324b669b1dc02afc1b26c2f", "text": "   },\n    \"workflow\": {\n      \"start\": \"2019-03-25T12:09:52Z\",\n      \"projectDir\": \"/Users/sven1103/.nextflow/assets/nf-core/hlatyping\",\n      \"manifest\": {\n        \"nextflowVersion\": \">=18.10.1\",\n        \"defaultBranch\": \"master\",\n        \"version\": \"1.1.4\",\n        \"homePage\": \"https://github.com/nf-core/hlatyping\",\n        \"gitmodules\": null,\n        \"description\": \"Precision HLA typing from next-generation sequencing data.\",\n        \"name\": \"nf-core/hlatyping\",\n        \"mainScript\": \"main.nf\",\n        \"author\": null\n      },\n      \"complete\": null,\n      \"profile\": \"docker,test\",\n      \"homeDir\": \"/Users/sven1103\",\n      \"workDir\": \"/Users/sven1103/git/nextflow/work\",\n      \"container\": \"nfcore/hlatyping:1.1.4\",\n      \"commitId\": \"4bcced898ee23600bd8c249ff085f8f88db90e7c\",\n      \"errorMessage\": null,\n      \"repository\": \"https://github.com/nf-core/hlatyping.git\",\n      \"containerEngine\": \"docker\",\n      \"scriptFile\": \"/Users/sven1103/.nextflow/assets/nf-core/hlatyping/main.nf\",\n      \"userName\": \"sven1103\",\n      \"launchDir\": \"/Users/sven1103/git/nextflow\",\n      \"runName\": \"shrivelled_cantor\",\n      \"configFiles\": [\n        \"/Users/sven1103/.nextflow/assets/nf-core/hlatyping/nextflow.config\"\n      ],\n      \"sessionId\": \"7f344978-999c-480d-8439-741bc7520f6a\",\n      \"errorReport\": null,\n      \"scriptId\": \"2902f5aa7f297f2dccd6baebac7730a2\",\n      \"revision\": \"master\",\n      \"exitStatus\": null,\n      \"commandLine\": \"./launch.sh run nf-core/hlatyping -profile docker,test -with-weblog 'http://localhost:4567'\",\n      \"nextflow\": {\n        \"version\": \"19.03.0-edge\",\n        \"build\": 5137,\n        \"timestamp\": \"2019-03-28T14:46:55Z\"\n      },\n    },\n    \"stats\": {\n      \"computeTimeFmt\": \"(a few seconds)\",\n      \"cachedCount\": 0,\n      \"cachedDuration\": 0,\n      \"failedDuration\": 0,\n      \"succeedDuration\": 0,\n      \"failedCount\": 0,\n      \"cachedPct\": 0.0,\n      \"cachedCountFmt\": \"0\",\n      \"succeedCountFmt\": \"0\",\n      \"failedPct\": 0.0,\n      \"failedCountFmt\":", "start_char_idx": 21088, "end_char_idx": 23079, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "4ec64556-b6c0-4ffa-a53f-54b5ec9d9d4e": {"__data__": {"id_": "4ec64556-b6c0-4ffa-a53f-54b5ec9d9d4e", "embedding": null, "metadata": {"file_path": "docs/tracing.md", "file_name": "tracing.md"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "b53cbf19b410703f4d5108405c739c7cce41cf35", "node_type": null, "metadata": {"file_path": "docs/tracing.md", "file_name": "tracing.md"}, "hash": "e17e248f6b86a2071215f8daae1a979bf3b3f798d60d5f0b49145ed1730fd20b"}, "2": {"node_id": "46a61c50-ef38-4639-a4f2-f123b1869a18", "node_type": null, "metadata": {"file_path": "docs/tracing.md", "file_name": "tracing.md"}, "hash": "a861f8b20c72114855bbd71b82511381a20251093324b669b1dc02afc1b26c2f"}}, "hash": "8a313c97b7a19f8ed983a1bf5e97ff6b9ba0847044a052002b6a24581bd4ac77", "text": "0.0,\n      \"failedCountFmt\": \"0\",\n      \"ignoredCountFmt\": \"0\",\n      \"ignoredCount\": 0,\n      \"succeedPct\": 0.0,\n      \"succeedCount\": 0,\n      \"ignoredPct\": 0.0\n    },\n    \"resume\": false,\n    \"success\": false,\n    \"scriptName\": \"main.nf\",\n    \"duration\": null\n  }\n}\n```\n\n### Example `completed` event\n\nWhen a task is completed, a message like the following is posted to the specified end-point:\n\n```json\n{\n  \"runName\": \"friendly_pesquet\",\n  \"runId\": \"170aa09c-105f-49d0-99b4-8eb6a146e4a7\",\n  \"event\": \"process_completed\",\n  \"utcTime\": \"2018-10-07T11:45:30Z\",\n  \"trace\": {\n    \"task_id\": 2,\n    \"status\": \"COMPLETED\",\n    \"hash\": \"a1/0024fd\",\n    \"name\": \"make_ot_config\",\n    \"exit\": 0,\n    \"submit\": 1538912529498,\n    \"start\": 1538912529629,\n    \"process\": \"make_ot_config\",\n    \"tag\": null,\n    \"module\": [\n\n    ],\n    \"container\": \"nfcore/hlatyping:1.1.1\",\n    \"attempt\": 1,\n    \"script\": \"\\n    configbuilder --max-cpus 2 --solver glpk > config.ini\\n    \",\n    \"scratch\": null,\n    \"workdir\": \"/home/sven1103/git/hlatyping-workflow/work/a1/0024fd028375e2b601aaed44d112e3\",\n    \"queue\": null,\n    \"cpus\": 1,\n    \"memory\": 7516192768,\n    \"disk\": null,\n    \"time\": 7200000,\n    \"env\": \"PATH=/home/sven1103/git/hlatyping-workflow/bin:$PATH\\n\",\n    \"error_action\": null,\n    \"complete\": 1538912730599,\n    \"duration\": 201101,\n    \"realtime\": 69,\n    \"%cpu\": 0.0,\n    \"%mem\": 0.1,\n    \"vmem\": 54259712,\n    \"rss\": 10469376,\n    \"peak_vmem\": 20185088,\n    \"peak_rss\": 574972928,\n    \"rchar\": 7597,\n    \"wchar\": 162,\n    \"syscr\": 16,\n    \"syscw\": 4083712,\n    \"read_bytes\": 4096,\n    \"write_bytes\": 0,\n    \"native_id\": 27185\n  }\n}\n```", "start_char_idx": 23067, "end_char_idx": 24702, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "c034246f-6e6f-4396-adcf-07d1f0518ec5": {"__data__": {"id_": "c034246f-6e6f-4396-adcf-07d1f0518ec5", "embedding": null, "metadata": {"file_path": "docs/wave.md", "file_name": "wave.md"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "a33bb9aa3007e487161c4bbb30fe17f2a39bf993", "node_type": null, "metadata": {"file_path": "docs/wave.md", "file_name": "wave.md"}, "hash": "e048fe608e959d9fb9f2fe881f6619f59eab3b88ccf723b9d18d6544d0030cd1"}, "3": {"node_id": "079822ac-1560-4bec-bf7c-5ca2b446fb6d", "node_type": null, "metadata": {"file_path": "docs/wave.md", "file_name": "wave.md"}, "hash": "39009fc9ce7e8a4d6b1a434adcab315e4f947590be066cd7dc2497f0062028d3"}}, "hash": "1382f5d6a2670deaa5251b477074b6b80bb66bd455780c1521178e1b04b69cc0", "text": "(wave-page)=\n\n# Wave containers\n\n:::{versionadded} 22.10.0\n:::\n\n[Wave](https://seqera.io/wave/) is a container provisioning service integrated with Nextflow. With Wave, you can build, upload, and manage the container images required by your data analysis workflows automatically and on-demand during pipeline execution.\n\n## Getting started\n\n### Nextflow installation\n\nIf you have already installed Nextflow, update to the latest version using this command:\n\n```bash\nnextflow -self-update\n```\n\nIf you don't have Nextflow already installed, install it with the command below:\n\n```bash\ncurl get.nextflow.io | bash\n```\n\n### Wave configuration\n\nWave can be used in any Nextflow pipeline by adding the following snippet to your `nextflow.config` file:\n\n```groovy\nwave {\n  enabled = true\n}\n\ntower {\n  accessToken = '<your access token>'\n}\n```\n\n:::{note}\nThe Tower access token is not mandatory, but it is recommended in order to access private container repositories and pull public containers without being affected by service rate limits. Credentials should be made available to Wave using the [credentials manager](https://help.tower.nf/latest/credentials/overview) in Tower.\n:::\n\n## Use cases\n\n### Authenticate private repositories\n\nWave allows the use of private repositories in your Nextflow pipelines. The repository access keys must be provided in the form of [Nextflow Tower credentials](https://help.tower.nf/22.2/credentials/overview/).\n\nOnce the credentials have been created, simply specify your [Tower account access token](https://help.tower.nf/22.2/api/overview/#authentication) in your pipeline configuration file. If the credentials were created in a Tower organization workspace, specify the workspace ID as well in the config file as shown below:\n\n```groovy\ntower {\n  accessToken = '<your access token>'\n  workspaceId = '<your workspace id>'\n}\n```\n\n### Build module containers\n\nWave can build and provision container images on-demand for your Nextflow pipelines.\n\nTo enable this feature, add the Dockerfile of the container to be built in the {ref}`module directory <dsl2-module-directory>` where the pipeline process is defined. When Wave is enabled, it automatically uses the Dockerfile to build the required container, upload to the registry, and it uses the container to carry out the tasks defined in the module.\n\n:::{tip}\nMake sure the process does not declare a `container` directive, otherwise it will take precedence over the Dockerfile definition.\n:::\n\nIf a process uses a `container` directive and you still want to build the container using the Dockerfile provided in the module directory, add the following setting to the pipeline config file:\n\n```groovy\nwave.strategy = ['dockerfile','container']\n```\n\nThis setting instructs Wave to prioritize the module Dockerfile over process `container` directives.\n\n:::{warning}\nWhen building containers, Wave currently does not support `ADD`, `COPY`, or any other Dockerfile commands that access files in the host file system.\n:::\n\n### Build Conda based containers\n\nWave allows the provisioning of containers based on the {ref}`process-conda` directive used by the processes in your pipeline. This is a quick alternative to building Conda packages in the local computer. Moreover, this enables the use of Conda packages in your pipeline when deploying in cloud-native platforms such as AWS Batch and Kubernetes, which do not allow the (easy) use of the Conda package manager.\n\nWith Wave enabled in your pipeline, simply define the `conda` requirements in the pipeline processes, provided the same process does not also specify a `container` directive or a Dockerfile.\n\nIn the latter case, add the", "start_char_idx": 0, "end_char_idx": 3662, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "079822ac-1560-4bec-bf7c-5ca2b446fb6d": {"__data__": {"id_": "079822ac-1560-4bec-bf7c-5ca2b446fb6d", "embedding": null, "metadata": {"file_path": "docs/wave.md", "file_name": "wave.md"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "a33bb9aa3007e487161c4bbb30fe17f2a39bf993", "node_type": null, "metadata": {"file_path": "docs/wave.md", "file_name": "wave.md"}, "hash": "e048fe608e959d9fb9f2fe881f6619f59eab3b88ccf723b9d18d6544d0030cd1"}, "2": {"node_id": "c034246f-6e6f-4396-adcf-07d1f0518ec5", "node_type": null, "metadata": {"file_path": "docs/wave.md", "file_name": "wave.md"}, "hash": "1382f5d6a2670deaa5251b477074b6b80bb66bd455780c1521178e1b04b69cc0"}, "3": {"node_id": "0f353ea5-010f-45b3-ad99-c0f6cc2ef073", "node_type": null, "metadata": {"file_path": "docs/wave.md", "file_name": "wave.md"}, "hash": "895c076933d069cb9d41c8562b64d4c45439471265db6be097fbebd8e58d6379"}}, "hash": "39009fc9ce7e8a4d6b1a434adcab315e4f947590be066cd7dc2497f0062028d3", "text": "directive or a Dockerfile.\n\nIn the latter case, add the following setting to your pipeline configuration:\n\n```groovy\nwave.strategy = ['conda']\n```\n\nThe above setting instructs Wave to use the `conda` directive to provision the pipeline containers and ignore the `container` directive and any Dockerfile(s).\n\n### Build Spack based containers\n\n:::{warning}\nSpack based Wave containers are currently in beta testing. Functionality is still sub-optimal, due to long build times that may result in backend time-out and subsequent task failure.\n:::\n\nWave allows the provisioning of containers based on the {ref}`process-spack` directive used by the processes in your\npipeline. This is an alternative to building Spack packages in the local computer.\nMoreover, this enables to run optimised builds with almost no user intervention.\n\nHaving Wave enabled in your pipeline, there's nothing else to do other than define the `spack` requirements in\nthe pipeline processes provided the same process does not also specify a `container` or `conda` directive or a Dockerfile.\n\nIn the latter case, add the following setting to your pipeline configuration:\n\n```groovy\nwave.strategy = ['spack']\n```\n\nThe above setting instructs Wave to only use the `spack` directive to provision the pipeline containers, ignoring the use of\nthe `container` directive and any Dockerfile(s).\n\nIn order to request the build of containers that are optimised for a specific CPU microarchitecture, the latter can be specified by means of the {ref}`process-arch` directive. The architecture must always be specified for processes that run on an ARM system. Otherwise, by default, Wave will build containers for the generic `x86_64` architecture family.\n\n:::{note}\nIf using a Spack YAML file to provide the required packages, you should avoid editing the following sections, which are already configured by the Wave plugin: `packages`, `config`, `view` and `concretizer` (your edits may be ignored), and `compilers` (your edits will be considered, and may interfere with the setup by the Wave plugin).\n:::\n\n### Push to a private repository\n\nContainers built by Wave are uploaded to the Wave default repository hosted on AWS ECR at `195996028523.dkr.ecr.eu-west-1.amazonaws.com/wave/build`. The images in this repository are automatically deleted 1 week after the date of their push.\n\nIf you want to store Wave containers in your own container repository use the following settings in the Nextflow configuration file:\n\n```groovy\nwave.build.repository = 'example.com/your/build-repo'\nwave.build.cacheRepository = 'example.com/your/cache-repo'\n```\n\nThe first repository is used to store the built container images. The second one is used to store the individual image layers for caching purposes.\n\nThe repository access keys must be provided as Tower credentials (see\n[Authenticate private repositories](#authenticate-private-repositories) above).\n\n### Run pipelines using Fusion file system\n\nWave containers allows you to run your containerised workflow with the {ref}`fusion-page`.\n\nThis enables the use of an object storage bucket such as AWS S3 or Google Cloud Storage as your pipeline work directory, simplifying and speeding up many operations on local, AWS Batch, Google Batch or Kubernetes executions.\n\nSee the {ref}`Fusion documentation <fusion-page>` for more details.\n\n## Advanced settings\n\nThe following configuration options are available:\n\n`wave.enabled`\n: Enable/disable the execution of Wave containers.\n\n`wave.endpoint`\n: The Wave service endpoint (default: `https://wave.seqera.io`).\n\n`wave.build.repository`\n: The container repository", "start_char_idx": 3616, "end_char_idx": 7223, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "0f353ea5-010f-45b3-ad99-c0f6cc2ef073": {"__data__": {"id_": "0f353ea5-010f-45b3-ad99-c0f6cc2ef073", "embedding": null, "metadata": {"file_path": "docs/wave.md", "file_name": "wave.md"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "a33bb9aa3007e487161c4bbb30fe17f2a39bf993", "node_type": null, "metadata": {"file_path": "docs/wave.md", "file_name": "wave.md"}, "hash": "e048fe608e959d9fb9f2fe881f6619f59eab3b88ccf723b9d18d6544d0030cd1"}, "2": {"node_id": "079822ac-1560-4bec-bf7c-5ca2b446fb6d", "node_type": null, "metadata": {"file_path": "docs/wave.md", "file_name": "wave.md"}, "hash": "39009fc9ce7e8a4d6b1a434adcab315e4f947590be066cd7dc2497f0062028d3"}}, "hash": "895c076933d069cb9d41c8562b64d4c45439471265db6be097fbebd8e58d6379", "text": "The container repository where images built by Wave are uploaded (note: the corresponding credentials must be provided in your Nextflow Tower account).\n\n`wave.build.cacheRepository`\n: The container repository used to cache image layers built by the Wave service (note: the corresponding credentials must be provided in your Nextflow Tower account).\n\n`wave.build.conda.basePackages`\n: One or more Conda packages to be always added in the resulting container e.g. `conda-forge::procps-ng`.\n\n`wave.build.conda.commands`\n: One or more commands to be added to the Dockerfile used to build a Conda based image.\n\n`wave.build.conda.mambaImage`\n: The Mamba container image is used to build Conda based container. This is expected to be [micromamba-docker](https://github.com/mamba-org/micromamba-docker) image.\n\n`wave.build.spack.basePackages`\n: :::{versionadded} 22.06.0-edge\n:::\n: One or more Spack packages to be always added in the resulting container.\n\n`wave.build.spack.commands`\n: :::{versionadded} 22.06.0-edge\n:::\n: One or more commands to be added to the Dockerfile used to build a Spack based image.\n\n`wave.httpClient.connectTime`\n: :::{versionadded} 22.06.0-edge\n:::\n: Sets the connection timeout duration for the HTTP client connecting to the Wave service (default: `30s`).\n\n`wave.strategy`\n: The strategy to be used when resolving ambiguous Wave container requirements (default: `'container,dockerfile,conda,spack'`).\n\n`wave.report.enabled` (preview)\n: Enable the reporting of the Wave containers used during the pipeline execution (default: `false`, requires version `23.06.0-edge` or later).\n\n`wave.report.file` (preview)\n: The name of the containers report file (default: `containers-<timestamp>.config` requires version `23.06.0-edge` or later).\n\n`wave.retryPolicy.delay`\n: :::{versionadded} 22.06.0-edge\n  :::\n: The initial delay when a failing HTTP request is retried (default: `150ms`). \n\n`wave.retryPolicy.maxDelay`\n: :::{versionadded} 22.06.0-edge\n  :::\n: The max delay when a failing HTTP request is retried (default: `90 seconds`).\n\n`wave.retryPolicy.maxAttempts`\n: :::{versionadded} 22.06.0-edge\n  :::\n: The max number of attempts a failing HTTP request is retried (default: `5`).\n\n`wave.retryPolicy.jitter`\n: :::{versionadded} 22.06.0-edge\n  :::\n: Sets the jitterFactor to randomly vary retry delays by (default: `0.25`).", "start_char_idx": 7246, "end_char_idx": 9585, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}}, "docstore/ref_doc_info": {"f0b98c510df971d3ce3cba2df5454a8ddd109fc5": {"node_ids": ["1ec77f04-7b50-4419-a1e5-63eef72a8579"], "metadata": {"file_path": "docs/README.md", "file_name": "README.md"}}, "bfcabb23c961bc8f4b16cb62d6345edfce1c6ba8": {"node_ids": ["05ce8a6b-3f28-4366-9bbf-8361455a8b27", "c7abbe2c-8cd8-42d1-9600-64ff0faf3063"], "metadata": {"file_path": "docs/amazons3.md", "file_name": "amazons3.md"}}, "edc18535956f4832534c0eb7f12118f393382241": {"node_ids": ["1b1c0d36-3430-4f38-b3c1-3c02cd7c555a", "a0f026f5-95a0-4489-9a27-e8fb6aa0b87d", "79916c62-fc44-45c7-8fe0-3a1f992df87d", "445a9234-9058-4838-91cf-f5dc12a42ee6", "ef3ef38f-542a-401b-aa20-fbc728ae3b62", "ec2dc4a0-8008-4498-8774-2afa4893a22e", "9d86b46f-6ff1-41d4-a772-df4fe3b270c6", "7a52b72f-a01d-4b63-943f-c796a7798925"], "metadata": {"file_path": "docs/aws.md", "file_name": "aws.md"}}, "aebf7bb9c1405f04479ebe20a4c05eecada7e126": {"node_ids": ["e364f60e-08ed-418e-921c-ed51d94b3b24", "a764c2d7-5329-45ca-92ba-1ba9e99490c7", "81ecf67e-0411-4eba-8cab-8c6732b5d388", "36792f6e-b573-4812-aeb9-341790196e85", "ab3cd4e3-2b68-4e33-9bd2-8075e6b99f51", "68f5f974-7162-47a5-b198-ba401306bee4"], "metadata": {"file_path": "docs/azure.md", "file_name": "azure.md"}}, "6f7b64b74e52d83f0c64de07cde0e3d49b26acaa": {"node_ids": ["865f459a-485e-43fb-ad14-e3b09686f266", "32e4722a-009e-4f49-86d1-f4e68f38a76c"], "metadata": {"file_path": "docs/basic.md", "file_name": "basic.md"}}, "fcab491dca76f6cbed6abef8c60c3955d641fd52": {"node_ids": ["7bdafbdc-652c-4fad-acc4-dbe3db44623f", "cf99a2be-2c1c-489c-a1fa-852c9982b88a", "944c48e5-f036-4200-a31e-71d697017d5d", "1ede687e-b6b9-4190-9182-2111f6a1c8e6", "a2f1a300-5ed5-49e7-ba30-3e09b09849f0", "487e6953-b868-421d-a102-4c50e24c5e39"], "metadata": {"file_path": "docs/channel.md", "file_name": "channel.md"}}, "164ca94a0451118caab80ef9f9fe59d9b70e1ac7": {"node_ids": ["125170b8-9579-4228-aaef-bf6a18ad2b84", "2298a7b1-911b-490c-a2ed-1877652579ee", "a4f6a240-67cc-4018-9ba1-33ac586fd589", "c25a9759-eb01-484a-87af-7787accef6c9", "d3327c84-8696-4709-a1cb-f9cf4f353e60", "bb05c387-9ce5-459f-b950-31dfa2df0a26", "21f20236-84b5-40a0-a78f-956cefb72524", "c1000cdd-019c-41b7-8646-4c2dcb2f6b62", "17cb78f4-7d8d-4113-8e88-cde6b42ad9b9", "075a46ac-b470-4883-8720-389a85aa84ec", "0a92b93f-276b-4593-acb2-06b141803419", "0013b21a-8efc-4460-8593-1d48294c47df"], "metadata": {"file_path": "docs/cli.md", "file_name": "cli.md"}}, "b4f05092efb2fb768e7b064ec7f3c0bbe80f2688": {"node_ids": ["82bce502-5b2d-40bf-a61b-69dec53a6a8b", "b9abe666-6658-45ca-bf79-1dcc42eeef00"], "metadata": {"file_path": "docs/conda.md", "file_name": "conda.md"}}, "5f43dd876f21e8b1a4c175d44180bc9a99b46e3d": {"node_ids": ["5902745f-43f5-4e02-8ffe-4c32bb240d27", "13e2b4fd-a55e-40f4-aea0-0a6a21ce5adc", "4416d839-f94b-45e7-9dbf-c1516926babf", "4c5bfd13-0e96-4acb-b7b9-27afecb12995", "eb6668ea-4b84-4d25-a38b-351a718b8eed", "1966337f-81ce-468d-80b3-1714fea6e49d", "82b36099-7ef0-4cb3-afe6-6d391f4b5c27", "ebafea1a-b940-4cbb-b1d4-d2350ad77905", "599dbdfb-3049-4a7c-afa1-88d0372625ab", "40da5828-2dba-4cbb-acda-83c02bdd8b63", "32b1f477-b3f8-4542-9321-b617259b3a13", "ec8cbbc1-3390-4794-a203-10cce7f1113f", "0a39cbf1-a298-4f1e-9d18-bb82cf940edc", "fa74fa51-e7eb-41d5-afab-6c96c0e0033c", "885ba2dc-e05b-4055-b45a-841a7adf9bc9", "6066287c-e78e-46b4-9747-13d1ac9291ca", "fd81bc34-c92f-414c-9813-a0ce486af0bd", "89329e42-8078-42e9-85d4-002d0aa6cbbd", "c1a3b427-4784-48b4-ae4d-655dd4e3ea08", "018f7e7c-995c-4587-bc2b-7c4ad1d6a62b", "b416f012-a8c7-44f1-b5ed-1cf906e8a631"], "metadata": {"file_path": "docs/config.md", "file_name": "config.md"}}, "5f43db1a3196c695750e930895ca2c989594a5d9": {"node_ids": ["7832a54c-1260-4b79-a2fc-2633e6c02f48", "cb545bda-e589-4c66-9fc5-ad49f436e370", "c04ef79d-dd51-402e-9087-9c4534b341ed", "51cebf8b-6391-4eb6-950f-9178e8a441ec", "6cfda5e0-7219-423e-9372-b61d44ed1b87", "354d5665-b2a8-4bc4-b831-ea2e21a34727", "94e0c2a4-9d84-4aaf-b1e9-d35b8bf5285b", "f81d8bc5-ff42-4de1-b8eb-508b8112b90c", "6bece696-695d-4d03-8972-baf9d18a4319"], "metadata": {"file_path": "docs/container.md", "file_name": "container.md"}}, "036c5d8ecf903500763d02d18d3f5317bfed1923": {"node_ids": ["e84dfd76-1081-4a09-bd5b-24de0f2ecb54", "a8016ff5-8f66-42f7-b4da-fefc410bbf11", "0a5284a0-9f00-401d-bfd8-f3a1e54ba28c", "94b62fd8-02be-4783-92dd-78ea1b7d99d4", "6920ea8f-5c14-44f2-9ea4-addbc435c4ba", "dd9c496f-c281-4cec-8c23-233a046ad400", "16e76ad7-03e5-433f-a8f3-a891665536a3"], "metadata": {"file_path": "docs/dsl2.md", "file_name": "dsl2.md"}}, "4adc9c0d92b6708aee44a652137dbf24403c7944": {"node_ids": ["6629031a-a105-4430-8302-a16a24a7e7fc", "6d3586ab-8145-4b14-9ed8-6e3cd8b5596f", "205e4873-6821-4621-a7c1-5741886f1e91", "8788ca26-5277-4dd0-bacc-8a14aad5cfc5", "6b4a08a1-e6e7-4bab-8e55-e27203b74bbd", "7e0361c3-0921-4dbb-bab8-bebc722c3a68", "e0bba44c-21fb-4c6b-89a2-17bed896ac35"], "metadata": {"file_path": "docs/executor.md", "file_name": "executor.md"}}, "6e16ac103b09cb71be3f33bbb68b2ef6bc7d3fcc": {"node_ids": ["5acbedd4-f8d3-4b50-b12b-1929fefd31dd", "f474ca00-0583-43aa-8266-ce97fc931ce4"], "metadata": {"file_path": "docs/flux.md", "file_name": "flux.md"}}, "1aeaeda99d3d4d605e1e5a50c24221a24ec14545": {"node_ids": ["f26eb4d3-0dcf-4a8f-8cac-9e45f11fb3fc", "d003939d-e855-4f7c-aea9-3657ed918f27", "00f70469-9b34-4893-9a87-8f010cefe2a3"], "metadata": {"file_path": "docs/fusion.md", "file_name": "fusion.md"}}, "d2ef3a5aebad2315dfabfe610a95a2148843b2f3": {"node_ids": ["c1bf8ab0-3b4a-4c6e-a4ae-386430078a30", "50ec52c6-eb13-47d9-9789-198df925f274", "392de5e3-6ee2-443c-a63d-0fd4b3e16985"], "metadata": {"file_path": "docs/getstarted.md", "file_name": "getstarted.md"}}, "36a88465987820e159f5c5c85e7ef67bf6fcee5f": {"node_ids": ["5a0f079e-c8f8-453c-b868-f7678a5c89ea", "90f94734-6c75-4bc2-b66f-8bedcaf69a26", "8fdc6edc-7354-446d-b226-25a7cfe06b39", "6aeffd42-ab0c-41c4-acbb-694beac9a0a4", "b0a9a076-eaed-4258-86a0-51da739d8798", "5be61f5e-83c2-4b85-a269-3abf9837eb1e"], "metadata": {"file_path": "docs/google.md", "file_name": "google.md"}}, "4c3aa6bae911907618067c75cd83a8585070dfb1": {"node_ids": ["3ac5d80d-d57e-495b-92b2-963f3a83d847", "021840f5-53ac-49dc-8804-813553c72162", "0fd1bdd5-9695-4177-ae1a-0c5adef838f8", "3ddc18b6-7593-4e54-b150-f9249eb2f9e0", "5d4dc916-6a40-44c4-94d4-799b499573b0", "21d436f7-2402-45d8-b5af-8b5822c28dac", "f0809c93-14fc-44f7-b228-d78dc4a9028c"], "metadata": {"file_path": "docs/ignite.md", "file_name": "ignite.md"}}, "95ce5773add72664c480be8dc558573dfd0daa1a": {"node_ids": ["bf38b9e5-0243-436e-9683-94696aca428e"], "metadata": {"file_path": "docs/index.md", "file_name": "index.md"}}, "06c954d55f081965a46e87f087da2d13a4d78a54": {"node_ids": ["5679e811-ced1-405d-b111-8832c2bd191b", "e325d2b6-1b9c-4e4f-899a-931e94194617", "f9108746-9470-4159-abf0-6f1dc870148f"], "metadata": {"file_path": "docs/kubernetes.md", "file_name": "kubernetes.md"}}, "5396077a51c81e98f78ee259c51b9d568b48e656": {"node_ids": ["650167e0-b75b-423e-9c86-95d1e6628dc8", "55490239-e8fc-406c-bff6-89cbb86ce742", "146d6f8a-443f-473b-a5a2-637b3ce26e86"], "metadata": {"file_path": "docs/mail.md", "file_name": "mail.md"}}, "7d353751852345c3464fab7b1c2160fd7e745d75": {"node_ids": ["7b7d0930-3f63-44f4-acdc-cb59b02848d8", "ba656205-9446-431c-84c9-e5e336774d5a"], "metadata": {"file_path": "docs/metadata.md", "file_name": "metadata.md"}}, "694d68cd05211c977fd232cf1e8166579eef2d90": {"node_ids": ["76ac2da5-c3f4-448b-a853-1215816a7da8", "36a2215c-9a6d-45ac-bfa1-dd0349e75cf5", "78eeaf75-8b78-460e-b074-7f868bbbe16f", "24085bab-197a-40a6-92bd-31637ca55de8"], "metadata": {"file_path": "docs/metrics.md", "file_name": "metrics.md"}}, "636d82cbe736f0e4b87074e2c4bac77becae2a28": {"node_ids": ["e279fcd0-f738-464e-bfc1-64bf088b3bc1", "51b774b9-2723-42b6-88e8-a121bc275146", "1fd51f4e-62cd-4a67-a4cb-96f822030abe", "aa7b7f87-d852-426b-bd8e-7fbbe178a655", "e260f3c4-051e-4076-b03c-37a78b8189b2", "91eaade7-7478-4723-8fb6-3e34868a4859", "c71dbe1d-3141-4b8c-a3a9-ebdf501354c5", "d88f337e-736e-4b55-bc11-3bdbd99a2641", "5c02ba1a-6514-48fe-bffb-8d9085d8d849", "510939b7-eda6-4953-bfe8-678dffdafac7", "3e7c21df-7e8b-473c-90f7-fed1cb0d2511", "c2785032-514c-4f06-b894-c367e44a06cd", "86a518cd-a160-4b6e-a600-3e8e04e7bbf6", "251985eb-6011-43d3-ad55-53a3d4c7ee30", "60b68069-0cc2-495f-a9f5-add32930ef46", "3df84d5b-5623-4ffb-bc05-0bbdf371eefd", "65ce763f-c431-435e-9657-0b6f2d5fc9c4", "df5ea1f3-e42e-4bb5-a8b3-e3a101ead26b", "8b5ef574-ebaa-4afd-8777-78f7b986a5a1", "342261b5-48ee-4425-8897-2a96bd90006c", "6bf31471-1b03-4690-a43e-c8c71d8bd884"], "metadata": {"file_path": "docs/operator.md", "file_name": "operator.md"}}, "5f68cc26ac6f6f085c287888a51130f9e6d399a1": {"node_ids": ["bba317d1-c5eb-4944-9ba2-0912a09d1720", "849314f2-9d75-4761-a629-218cd4f2bc8f"], "metadata": {"file_path": "docs/plugins.md", "file_name": "plugins.md"}}, "db7a594acaaabbb6b84352a6719c7045b36a31c3": {"node_ids": ["b1f0978e-8242-448a-85b3-6bb61f702256", "987e8618-c2ad-403d-8e4a-e478f0ab5260", "d245941a-e0af-4e45-a04b-e23ff5fbfdef", "2ea9be27-a1af-461b-ab04-71f4ac6fca73", "21251270-23dc-4a6f-b29f-b9c13a48c226", "964776da-1750-475e-9ff5-efe227f3cb04", "44945d41-5a12-491a-924a-b97e239af1ba", "1ed85396-1fe1-4c6b-ace0-dab134705495", "e23489cd-f252-48fb-ace5-bfce5c2f3251", "8e6ad538-2e89-48a3-854c-f7a8abe351ee", "3a4bc64a-71be-4851-aa46-f9163e424db4", "2670d1e3-f2c4-4e6a-a234-545c9ca4f909", "c907ebdd-ec57-495c-b258-3bbae9ff432f", "7dd06df2-2281-4ee7-bffa-637628a09c0c", "6b96c19c-e8a7-4e6f-bbd4-e7691ddf6117", "91d12dfc-dd9f-4c13-abf6-eeacb0925fa4", "023b4e6a-1f76-4f8b-b548-863c6dc27d3b", "767b9b4f-6546-4376-8122-1aec03371b68", "0ed1fe50-3b49-4022-b72f-f684715aa191", "6da8aa39-9c5d-4a3b-8a63-9157b7c1a241", "adf110de-6ee7-40eb-a6e4-656d82d04f05", "3ef9a297-63d2-4050-bfb4-3b8a46d1aea4", "cad50b16-721e-4045-949f-2ec998dc9b3b", "1958a5c9-7550-484c-96a3-942056c58345", "28409c52-9258-4335-941d-ac167bcaa839", "a3f69744-aecd-4d85-86aa-dc85a3654336", "565938cc-1933-4079-bdfc-35ef73169e62", "92a3ac49-2d11-4c91-8adb-2f2a3de43477"], "metadata": {"file_path": "docs/process.md", "file_name": "process.md"}}, "3778126c3f8d6c114a4804ff3e62747973115392": {"node_ids": ["c5c76d6d-dc87-42f6-a4f4-f60200c7089a", "5f38ced3-f532-4e1e-8e2e-a157da62bd55", "191609f6-dcb7-46b3-8759-31889a56726b", "21988fc6-30c1-4e66-8fd0-3bdaa70ee9ad", "5bdedd96-7a5b-4418-8c1d-ad10f33d2b70", "bf8d2480-0ae0-4cf0-adf6-7abf6215d9aa", "9f9fdf29-987f-484d-9f71-984d46de8d01", "0e1ff258-fcaf-4962-92af-fbd7a8475a7a", "21bca994-6699-410b-a1a9-b732721bd33b", "92288b6a-332b-4c21-804a-68e1301d538f", "35041c69-67b4-46b1-9093-01f260631494", "0cecd8ba-77b1-4051-98a7-01627b961b02", "d0141c92-fc27-4d55-b4d1-94076acfdce8", "baad20ff-c3fb-424a-9810-140d2ab4696a"], "metadata": {"file_path": "docs/script.md", "file_name": "script.md"}}, "1f6253a69f7dc9d6db384a333c1e40582b2d4cb7": {"node_ids": ["95baefbe-e992-4876-82a3-a2e7dff9aee6"], "metadata": {"file_path": "docs/secrets.md", "file_name": "secrets.md"}}, "aac7341d37e177f35e582c98b61ee22a8c002f39": {"node_ids": ["c7c3fb4b-734c-4a9f-8a50-77f6c805f858", "32cd74a7-d777-4148-91f3-59212a5e7db2", "f24222d1-22a0-4213-8821-0bea3b01faca", "162daf80-f111-47e5-9363-d502d3e827c4", "5ff7a0d0-1e3e-4c78-8506-2bc7eb2a58f0", "38003815-e0f2-4dd1-9af3-0d93d869a3f2", "e28da2fd-46b1-4067-b0a6-664354fd3566"], "metadata": {"file_path": "docs/sharing.md", "file_name": "sharing.md"}}, "636b0952f819caa55fb9bafa992266d6e9ba4045": {"node_ids": ["3038151e-cec1-4531-a55a-943885881fcf", "cdc4fccb-2879-4e38-b6ce-c9eb068f92a4", "55081324-bdaf-4f4f-83b1-ef63c409f632"], "metadata": {"file_path": "docs/spack.md", "file_name": "spack.md"}}, "b53cbf19b410703f4d5108405c739c7cce41cf35": {"node_ids": ["27774c1b-5464-4b2d-9ce3-98c622896d64", "f7a91cab-6829-4f8b-a8d1-ab23466363f1", "47b441db-a090-45d4-8a71-b5b5130d3c3d", "7502534f-4fc0-47cf-a2e1-376d24608c28", "b6a9a829-f63e-40f5-a777-385c8e1ac15f", "631c1fc8-e333-4113-b6de-541d0f6699a6", "eb83dd1a-d5c1-42b6-901b-d0d6c201a0e4", "2cdbca24-3ddc-4512-b8fb-6a4096afc86a", "46a61c50-ef38-4639-a4f2-f123b1869a18", "4ec64556-b6c0-4ffa-a53f-54b5ec9d9d4e"], "metadata": {"file_path": "docs/tracing.md", "file_name": "tracing.md"}}, "a33bb9aa3007e487161c4bbb30fe17f2a39bf993": {"node_ids": ["c034246f-6e6f-4396-adcf-07d1f0518ec5", "079822ac-1560-4bec-bf7c-5ca2b446fb6d", "0f353ea5-010f-45b3-ad99-c0f6cc2ef073"], "metadata": {"file_path": "docs/wave.md", "file_name": "wave.md"}}}}